{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0804970286331712,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0003601656762110571,
      "grad_norm": 0.3649095594882965,
      "learning_rate": 1.5e-06,
      "loss": 2.061,
      "step": 1
    },
    {
      "epoch": 0.0007203313524221142,
      "grad_norm": 0.37731099128723145,
      "learning_rate": 3e-06,
      "loss": 2.0264,
      "step": 2
    },
    {
      "epoch": 0.0010804970286331713,
      "grad_norm": 0.40116384625434875,
      "learning_rate": 4.5e-06,
      "loss": 2.1205,
      "step": 3
    },
    {
      "epoch": 0.0014406627048442284,
      "grad_norm": 0.3743530809879303,
      "learning_rate": 6e-06,
      "loss": 2.046,
      "step": 4
    },
    {
      "epoch": 0.0018008283810552854,
      "grad_norm": 0.358709454536438,
      "learning_rate": 7.5e-06,
      "loss": 2.1339,
      "step": 5
    },
    {
      "epoch": 0.0021609940572663426,
      "grad_norm": 0.3888291120529175,
      "learning_rate": 9e-06,
      "loss": 2.0598,
      "step": 6
    },
    {
      "epoch": 0.0025211597334773997,
      "grad_norm": 0.42136967182159424,
      "learning_rate": 1.05e-05,
      "loss": 1.9509,
      "step": 7
    },
    {
      "epoch": 0.002881325409688457,
      "grad_norm": 0.37163087725639343,
      "learning_rate": 1.2e-05,
      "loss": 2.1327,
      "step": 8
    },
    {
      "epoch": 0.0032414910858995136,
      "grad_norm": 0.541587769985199,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 2.1342,
      "step": 9
    },
    {
      "epoch": 0.003601656762110571,
      "grad_norm": 0.4380086362361908,
      "learning_rate": 1.5e-05,
      "loss": 2.0422,
      "step": 10
    },
    {
      "epoch": 0.0039618224383216275,
      "grad_norm": 0.4659181833267212,
      "learning_rate": 1.65e-05,
      "loss": 2.0736,
      "step": 11
    },
    {
      "epoch": 0.004321988114532685,
      "grad_norm": 0.4081474244594574,
      "learning_rate": 1.8e-05,
      "loss": 2.0692,
      "step": 12
    },
    {
      "epoch": 0.004682153790743742,
      "grad_norm": 0.40035635232925415,
      "learning_rate": 1.95e-05,
      "loss": 2.1902,
      "step": 13
    },
    {
      "epoch": 0.0050423194669547994,
      "grad_norm": 0.41765886545181274,
      "learning_rate": 2.1e-05,
      "loss": 2.0487,
      "step": 14
    },
    {
      "epoch": 0.005402485143165856,
      "grad_norm": 0.4297017753124237,
      "learning_rate": 2.25e-05,
      "loss": 2.0314,
      "step": 15
    },
    {
      "epoch": 0.005762650819376914,
      "grad_norm": 0.43443018198013306,
      "learning_rate": 2.4e-05,
      "loss": 2.0377,
      "step": 16
    },
    {
      "epoch": 0.0061228164955879705,
      "grad_norm": 0.4012235999107361,
      "learning_rate": 2.55e-05,
      "loss": 2.0642,
      "step": 17
    },
    {
      "epoch": 0.006482982171799027,
      "grad_norm": 0.4035767614841461,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.9319,
      "step": 18
    },
    {
      "epoch": 0.006843147848010085,
      "grad_norm": 0.3544952869415283,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.9764,
      "step": 19
    },
    {
      "epoch": 0.007203313524221142,
      "grad_norm": 0.39815008640289307,
      "learning_rate": 3e-05,
      "loss": 1.9811,
      "step": 20
    },
    {
      "epoch": 0.007563479200432199,
      "grad_norm": 0.3446319103240967,
      "learning_rate": 2.9996389022628793e-05,
      "loss": 2.0553,
      "step": 21
    },
    {
      "epoch": 0.007923644876643255,
      "grad_norm": 0.38462477922439575,
      "learning_rate": 2.9992778045257582e-05,
      "loss": 2.0595,
      "step": 22
    },
    {
      "epoch": 0.008283810552854314,
      "grad_norm": 0.340946227312088,
      "learning_rate": 2.9989167067886374e-05,
      "loss": 2.0637,
      "step": 23
    },
    {
      "epoch": 0.00864397622906537,
      "grad_norm": 0.32039687037467957,
      "learning_rate": 2.9985556090515167e-05,
      "loss": 1.987,
      "step": 24
    },
    {
      "epoch": 0.009004141905276427,
      "grad_norm": 0.33457908034324646,
      "learning_rate": 2.998194511314396e-05,
      "loss": 1.8985,
      "step": 25
    },
    {
      "epoch": 0.009364307581487484,
      "grad_norm": 0.3745743930339813,
      "learning_rate": 2.997833413577275e-05,
      "loss": 1.8984,
      "step": 26
    },
    {
      "epoch": 0.009724473257698542,
      "grad_norm": 0.3560616374015808,
      "learning_rate": 2.997472315840154e-05,
      "loss": 1.8837,
      "step": 27
    },
    {
      "epoch": 0.010084638933909599,
      "grad_norm": 0.3184840977191925,
      "learning_rate": 2.9971112181030332e-05,
      "loss": 1.9814,
      "step": 28
    },
    {
      "epoch": 0.010444804610120656,
      "grad_norm": 0.40996861457824707,
      "learning_rate": 2.9967501203659125e-05,
      "loss": 1.9261,
      "step": 29
    },
    {
      "epoch": 0.010804970286331712,
      "grad_norm": 0.34092193841934204,
      "learning_rate": 2.9963890226287914e-05,
      "loss": 1.811,
      "step": 30
    },
    {
      "epoch": 0.011165135962542769,
      "grad_norm": 0.31445541977882385,
      "learning_rate": 2.996027924891671e-05,
      "loss": 1.959,
      "step": 31
    },
    {
      "epoch": 0.011525301638753828,
      "grad_norm": 0.2976394295692444,
      "learning_rate": 2.9956668271545498e-05,
      "loss": 1.8619,
      "step": 32
    },
    {
      "epoch": 0.011885467314964884,
      "grad_norm": 0.2743781507015228,
      "learning_rate": 2.995305729417429e-05,
      "loss": 1.8834,
      "step": 33
    },
    {
      "epoch": 0.012245632991175941,
      "grad_norm": 0.28469130396842957,
      "learning_rate": 2.9949446316803083e-05,
      "loss": 1.8138,
      "step": 34
    },
    {
      "epoch": 0.012605798667386998,
      "grad_norm": 0.29503127932548523,
      "learning_rate": 2.994583533943187e-05,
      "loss": 1.8906,
      "step": 35
    },
    {
      "epoch": 0.012965964343598054,
      "grad_norm": 0.2861257493495941,
      "learning_rate": 2.9942224362060664e-05,
      "loss": 1.8727,
      "step": 36
    },
    {
      "epoch": 0.013326130019809113,
      "grad_norm": 0.2823062241077423,
      "learning_rate": 2.993861338468946e-05,
      "loss": 1.9134,
      "step": 37
    },
    {
      "epoch": 0.01368629569602017,
      "grad_norm": 0.27708423137664795,
      "learning_rate": 2.993500240731825e-05,
      "loss": 1.771,
      "step": 38
    },
    {
      "epoch": 0.014046461372231226,
      "grad_norm": 0.27495652437210083,
      "learning_rate": 2.993139142994704e-05,
      "loss": 1.8741,
      "step": 39
    },
    {
      "epoch": 0.014406627048442283,
      "grad_norm": 0.2578408122062683,
      "learning_rate": 2.992778045257583e-05,
      "loss": 1.9018,
      "step": 40
    },
    {
      "epoch": 0.01476679272465334,
      "grad_norm": 0.2847082018852234,
      "learning_rate": 2.9924169475204622e-05,
      "loss": 1.8397,
      "step": 41
    },
    {
      "epoch": 0.015126958400864398,
      "grad_norm": 0.28604376316070557,
      "learning_rate": 2.9920558497833414e-05,
      "loss": 1.873,
      "step": 42
    },
    {
      "epoch": 0.015487124077075455,
      "grad_norm": 0.281523734331131,
      "learning_rate": 2.9916947520462203e-05,
      "loss": 1.7674,
      "step": 43
    },
    {
      "epoch": 0.01584728975328651,
      "grad_norm": 0.2862682342529297,
      "learning_rate": 2.9913336543091e-05,
      "loss": 1.8114,
      "step": 44
    },
    {
      "epoch": 0.01620745542949757,
      "grad_norm": 0.2895442545413971,
      "learning_rate": 2.990972556571979e-05,
      "loss": 1.6724,
      "step": 45
    },
    {
      "epoch": 0.016567621105708627,
      "grad_norm": 0.3294444978237152,
      "learning_rate": 2.990611458834858e-05,
      "loss": 1.8141,
      "step": 46
    },
    {
      "epoch": 0.016927786781919682,
      "grad_norm": 0.2790137231349945,
      "learning_rate": 2.9902503610977372e-05,
      "loss": 1.9574,
      "step": 47
    },
    {
      "epoch": 0.01728795245813074,
      "grad_norm": 0.29136669635772705,
      "learning_rate": 2.989889263360616e-05,
      "loss": 1.7476,
      "step": 48
    },
    {
      "epoch": 0.0176481181343418,
      "grad_norm": 0.3044491708278656,
      "learning_rate": 2.9895281656234954e-05,
      "loss": 1.6547,
      "step": 49
    },
    {
      "epoch": 0.018008283810552854,
      "grad_norm": 0.3822215795516968,
      "learning_rate": 2.9891670678863746e-05,
      "loss": 1.639,
      "step": 50
    },
    {
      "epoch": 0.018368449486763912,
      "grad_norm": 0.3392201364040375,
      "learning_rate": 2.9888059701492538e-05,
      "loss": 1.6329,
      "step": 51
    },
    {
      "epoch": 0.018728615162974967,
      "grad_norm": 0.3356004059314728,
      "learning_rate": 2.988444872412133e-05,
      "loss": 1.7782,
      "step": 52
    },
    {
      "epoch": 0.019088780839186026,
      "grad_norm": 0.289699524641037,
      "learning_rate": 2.9880837746750123e-05,
      "loss": 1.6559,
      "step": 53
    },
    {
      "epoch": 0.019448946515397084,
      "grad_norm": 0.2796614468097687,
      "learning_rate": 2.9877226769378912e-05,
      "loss": 1.7241,
      "step": 54
    },
    {
      "epoch": 0.01980911219160814,
      "grad_norm": 0.2726266086101532,
      "learning_rate": 2.9873615792007704e-05,
      "loss": 1.754,
      "step": 55
    },
    {
      "epoch": 0.020169277867819198,
      "grad_norm": 0.28746625781059265,
      "learning_rate": 2.9870004814636493e-05,
      "loss": 1.7603,
      "step": 56
    },
    {
      "epoch": 0.020529443544030253,
      "grad_norm": 0.2760368585586548,
      "learning_rate": 2.9866393837265285e-05,
      "loss": 1.7534,
      "step": 57
    },
    {
      "epoch": 0.02088960922024131,
      "grad_norm": 0.292365163564682,
      "learning_rate": 2.986278285989408e-05,
      "loss": 1.6023,
      "step": 58
    },
    {
      "epoch": 0.02124977489645237,
      "grad_norm": 0.28867122530937195,
      "learning_rate": 2.985917188252287e-05,
      "loss": 1.5937,
      "step": 59
    },
    {
      "epoch": 0.021609940572663425,
      "grad_norm": 0.2777148187160492,
      "learning_rate": 2.9855560905151662e-05,
      "loss": 1.6087,
      "step": 60
    },
    {
      "epoch": 0.021970106248874483,
      "grad_norm": 0.27771317958831787,
      "learning_rate": 2.9851949927780454e-05,
      "loss": 1.5625,
      "step": 61
    },
    {
      "epoch": 0.022330271925085538,
      "grad_norm": 0.2993263006210327,
      "learning_rate": 2.9848338950409243e-05,
      "loss": 1.5239,
      "step": 62
    },
    {
      "epoch": 0.022690437601296597,
      "grad_norm": 0.28414130210876465,
      "learning_rate": 2.9844727973038036e-05,
      "loss": 1.5834,
      "step": 63
    },
    {
      "epoch": 0.023050603277507655,
      "grad_norm": 0.3021251857280731,
      "learning_rate": 2.9841116995666828e-05,
      "loss": 1.7481,
      "step": 64
    },
    {
      "epoch": 0.02341076895371871,
      "grad_norm": 0.31067705154418945,
      "learning_rate": 2.983750601829562e-05,
      "loss": 1.5591,
      "step": 65
    },
    {
      "epoch": 0.02377093462992977,
      "grad_norm": 0.2900265157222748,
      "learning_rate": 2.9833895040924413e-05,
      "loss": 1.6087,
      "step": 66
    },
    {
      "epoch": 0.024131100306140824,
      "grad_norm": 0.28841307759284973,
      "learning_rate": 2.98302840635532e-05,
      "loss": 1.6167,
      "step": 67
    },
    {
      "epoch": 0.024491265982351882,
      "grad_norm": 0.2579374313354492,
      "learning_rate": 2.9826673086181994e-05,
      "loss": 1.5523,
      "step": 68
    },
    {
      "epoch": 0.02485143165856294,
      "grad_norm": 0.2895168364048004,
      "learning_rate": 2.9823062108810786e-05,
      "loss": 1.4483,
      "step": 69
    },
    {
      "epoch": 0.025211597334773996,
      "grad_norm": 0.2607572376728058,
      "learning_rate": 2.9819451131439575e-05,
      "loss": 1.6095,
      "step": 70
    },
    {
      "epoch": 0.025571763010985054,
      "grad_norm": 0.2519356310367584,
      "learning_rate": 2.981584015406837e-05,
      "loss": 1.5278,
      "step": 71
    },
    {
      "epoch": 0.02593192868719611,
      "grad_norm": 0.28542524576187134,
      "learning_rate": 2.981222917669716e-05,
      "loss": 1.5137,
      "step": 72
    },
    {
      "epoch": 0.026292094363407167,
      "grad_norm": 0.28215527534484863,
      "learning_rate": 2.9808618199325952e-05,
      "loss": 1.4536,
      "step": 73
    },
    {
      "epoch": 0.026652260039618226,
      "grad_norm": 0.24936003983020782,
      "learning_rate": 2.9805007221954744e-05,
      "loss": 1.5647,
      "step": 74
    },
    {
      "epoch": 0.02701242571582928,
      "grad_norm": 0.23680970072746277,
      "learning_rate": 2.9801396244583533e-05,
      "loss": 1.4998,
      "step": 75
    },
    {
      "epoch": 0.02737259139204034,
      "grad_norm": 0.23720234632492065,
      "learning_rate": 2.9797785267212325e-05,
      "loss": 1.4765,
      "step": 76
    },
    {
      "epoch": 0.027732757068251394,
      "grad_norm": 0.2661297023296356,
      "learning_rate": 2.9794174289841118e-05,
      "loss": 1.7029,
      "step": 77
    },
    {
      "epoch": 0.028092922744462453,
      "grad_norm": 0.2407240867614746,
      "learning_rate": 2.979056331246991e-05,
      "loss": 1.6247,
      "step": 78
    },
    {
      "epoch": 0.02845308842067351,
      "grad_norm": 0.22877804934978485,
      "learning_rate": 2.9786952335098702e-05,
      "loss": 1.5405,
      "step": 79
    },
    {
      "epoch": 0.028813254096884566,
      "grad_norm": 0.25848388671875,
      "learning_rate": 2.978334135772749e-05,
      "loss": 1.506,
      "step": 80
    },
    {
      "epoch": 0.029173419773095625,
      "grad_norm": 0.22595524787902832,
      "learning_rate": 2.9779730380356283e-05,
      "loss": 1.5171,
      "step": 81
    },
    {
      "epoch": 0.02953358544930668,
      "grad_norm": 0.22789041697978973,
      "learning_rate": 2.9776119402985076e-05,
      "loss": 1.5177,
      "step": 82
    },
    {
      "epoch": 0.029893751125517738,
      "grad_norm": 0.23644651472568512,
      "learning_rate": 2.9772508425613865e-05,
      "loss": 1.4952,
      "step": 83
    },
    {
      "epoch": 0.030253916801728797,
      "grad_norm": 0.2032473087310791,
      "learning_rate": 2.9768897448242657e-05,
      "loss": 1.4919,
      "step": 84
    },
    {
      "epoch": 0.03061408247793985,
      "grad_norm": 0.2119610756635666,
      "learning_rate": 2.9765286470871453e-05,
      "loss": 1.56,
      "step": 85
    },
    {
      "epoch": 0.03097424815415091,
      "grad_norm": 0.21250185370445251,
      "learning_rate": 2.976167549350024e-05,
      "loss": 1.4887,
      "step": 86
    },
    {
      "epoch": 0.031334413830361965,
      "grad_norm": 0.21701325476169586,
      "learning_rate": 2.9758064516129034e-05,
      "loss": 1.6451,
      "step": 87
    },
    {
      "epoch": 0.03169457950657302,
      "grad_norm": 0.22702528536319733,
      "learning_rate": 2.9754453538757823e-05,
      "loss": 1.5323,
      "step": 88
    },
    {
      "epoch": 0.03205474518278408,
      "grad_norm": 0.19738775491714478,
      "learning_rate": 2.9750842561386615e-05,
      "loss": 1.5951,
      "step": 89
    },
    {
      "epoch": 0.03241491085899514,
      "grad_norm": 0.22341622412204742,
      "learning_rate": 2.9747231584015407e-05,
      "loss": 1.603,
      "step": 90
    },
    {
      "epoch": 0.03277507653520619,
      "grad_norm": 0.2233441025018692,
      "learning_rate": 2.97436206066442e-05,
      "loss": 1.3557,
      "step": 91
    },
    {
      "epoch": 0.033135242211417254,
      "grad_norm": 0.20859824120998383,
      "learning_rate": 2.9740009629272992e-05,
      "loss": 1.3608,
      "step": 92
    },
    {
      "epoch": 0.03349540788762831,
      "grad_norm": 0.20526087284088135,
      "learning_rate": 2.9736398651901784e-05,
      "loss": 1.4677,
      "step": 93
    },
    {
      "epoch": 0.033855573563839364,
      "grad_norm": 0.20021383464336395,
      "learning_rate": 2.9732787674530573e-05,
      "loss": 1.6187,
      "step": 94
    },
    {
      "epoch": 0.034215739240050426,
      "grad_norm": 0.20436890423297882,
      "learning_rate": 2.9729176697159365e-05,
      "loss": 1.4428,
      "step": 95
    },
    {
      "epoch": 0.03457590491626148,
      "grad_norm": 0.2053392231464386,
      "learning_rate": 2.9725565719788154e-05,
      "loss": 1.5916,
      "step": 96
    },
    {
      "epoch": 0.034936070592472536,
      "grad_norm": 0.20374621450901031,
      "learning_rate": 2.9721954742416947e-05,
      "loss": 1.398,
      "step": 97
    },
    {
      "epoch": 0.0352962362686836,
      "grad_norm": 0.20106831192970276,
      "learning_rate": 2.9718343765045742e-05,
      "loss": 1.48,
      "step": 98
    },
    {
      "epoch": 0.03565640194489465,
      "grad_norm": 0.20145520567893982,
      "learning_rate": 2.971473278767453e-05,
      "loss": 1.47,
      "step": 99
    },
    {
      "epoch": 0.03601656762110571,
      "grad_norm": 0.2268989086151123,
      "learning_rate": 2.9711121810303323e-05,
      "loss": 1.5482,
      "step": 100
    },
    {
      "epoch": 0.03637673329731676,
      "grad_norm": 0.20101900398731232,
      "learning_rate": 2.9707510832932116e-05,
      "loss": 1.7439,
      "step": 101
    },
    {
      "epoch": 0.036736898973527825,
      "grad_norm": 0.20871619880199432,
      "learning_rate": 2.9703899855560905e-05,
      "loss": 1.4864,
      "step": 102
    },
    {
      "epoch": 0.03709706464973888,
      "grad_norm": 0.21375049650669098,
      "learning_rate": 2.9700288878189697e-05,
      "loss": 1.5213,
      "step": 103
    },
    {
      "epoch": 0.037457230325949935,
      "grad_norm": 0.20847058296203613,
      "learning_rate": 2.9696677900818486e-05,
      "loss": 1.4796,
      "step": 104
    },
    {
      "epoch": 0.037817396002161,
      "grad_norm": 0.24388909339904785,
      "learning_rate": 2.969306692344728e-05,
      "loss": 1.4915,
      "step": 105
    },
    {
      "epoch": 0.03817756167837205,
      "grad_norm": 0.20978856086730957,
      "learning_rate": 2.9689455946076074e-05,
      "loss": 1.3798,
      "step": 106
    },
    {
      "epoch": 0.03853772735458311,
      "grad_norm": 0.19594550132751465,
      "learning_rate": 2.9685844968704863e-05,
      "loss": 1.495,
      "step": 107
    },
    {
      "epoch": 0.03889789303079417,
      "grad_norm": 0.22586490213871002,
      "learning_rate": 2.9682233991333655e-05,
      "loss": 1.453,
      "step": 108
    },
    {
      "epoch": 0.039258058707005224,
      "grad_norm": 0.23094120621681213,
      "learning_rate": 2.9678623013962447e-05,
      "loss": 1.4145,
      "step": 109
    },
    {
      "epoch": 0.03961822438321628,
      "grad_norm": 0.22449523210525513,
      "learning_rate": 2.9675012036591236e-05,
      "loss": 1.462,
      "step": 110
    },
    {
      "epoch": 0.039978390059427334,
      "grad_norm": 0.2037985473871231,
      "learning_rate": 2.967140105922003e-05,
      "loss": 1.3895,
      "step": 111
    },
    {
      "epoch": 0.040338555735638396,
      "grad_norm": 0.20139877498149872,
      "learning_rate": 2.966779008184882e-05,
      "loss": 1.349,
      "step": 112
    },
    {
      "epoch": 0.04069872141184945,
      "grad_norm": 0.19590400159358978,
      "learning_rate": 2.9664179104477613e-05,
      "loss": 1.4269,
      "step": 113
    },
    {
      "epoch": 0.041058887088060506,
      "grad_norm": 0.2015218436717987,
      "learning_rate": 2.9660568127106405e-05,
      "loss": 1.3738,
      "step": 114
    },
    {
      "epoch": 0.04141905276427157,
      "grad_norm": 0.2017108052968979,
      "learning_rate": 2.9656957149735194e-05,
      "loss": 1.4792,
      "step": 115
    },
    {
      "epoch": 0.04177921844048262,
      "grad_norm": 0.19829556345939636,
      "learning_rate": 2.9653346172363987e-05,
      "loss": 1.5687,
      "step": 116
    },
    {
      "epoch": 0.04213938411669368,
      "grad_norm": 0.19471846520900726,
      "learning_rate": 2.964973519499278e-05,
      "loss": 1.3989,
      "step": 117
    },
    {
      "epoch": 0.04249954979290474,
      "grad_norm": 0.20315013825893402,
      "learning_rate": 2.964612421762157e-05,
      "loss": 1.433,
      "step": 118
    },
    {
      "epoch": 0.042859715469115794,
      "grad_norm": 0.21097739040851593,
      "learning_rate": 2.9642513240250364e-05,
      "loss": 1.531,
      "step": 119
    },
    {
      "epoch": 0.04321988114532685,
      "grad_norm": 0.19176140427589417,
      "learning_rate": 2.9638902262879152e-05,
      "loss": 1.5244,
      "step": 120
    },
    {
      "epoch": 0.043580046821537904,
      "grad_norm": 0.21034075319766998,
      "learning_rate": 2.9635291285507945e-05,
      "loss": 1.4865,
      "step": 121
    },
    {
      "epoch": 0.043940212497748966,
      "grad_norm": 0.19070254266262054,
      "learning_rate": 2.9631680308136737e-05,
      "loss": 1.4697,
      "step": 122
    },
    {
      "epoch": 0.04430037817396002,
      "grad_norm": 0.19759799540042877,
      "learning_rate": 2.9628069330765526e-05,
      "loss": 1.3539,
      "step": 123
    },
    {
      "epoch": 0.044660543850171076,
      "grad_norm": 0.19518281519412994,
      "learning_rate": 2.9624458353394318e-05,
      "loss": 1.4209,
      "step": 124
    },
    {
      "epoch": 0.04502070952638214,
      "grad_norm": 0.1948355883359909,
      "learning_rate": 2.9620847376023114e-05,
      "loss": 1.4002,
      "step": 125
    },
    {
      "epoch": 0.04538087520259319,
      "grad_norm": 0.18381986021995544,
      "learning_rate": 2.9617236398651903e-05,
      "loss": 1.4649,
      "step": 126
    },
    {
      "epoch": 0.04574104087880425,
      "grad_norm": 0.18894343078136444,
      "learning_rate": 2.9613625421280695e-05,
      "loss": 1.5003,
      "step": 127
    },
    {
      "epoch": 0.04610120655501531,
      "grad_norm": 0.1924382597208023,
      "learning_rate": 2.9610014443909484e-05,
      "loss": 1.5817,
      "step": 128
    },
    {
      "epoch": 0.046461372231226365,
      "grad_norm": 0.18838000297546387,
      "learning_rate": 2.9606403466538276e-05,
      "loss": 1.5575,
      "step": 129
    },
    {
      "epoch": 0.04682153790743742,
      "grad_norm": 0.19919735193252563,
      "learning_rate": 2.960279248916707e-05,
      "loss": 1.586,
      "step": 130
    },
    {
      "epoch": 0.047181703583648475,
      "grad_norm": 0.19013351202011108,
      "learning_rate": 2.9599181511795858e-05,
      "loss": 1.2808,
      "step": 131
    },
    {
      "epoch": 0.04754186925985954,
      "grad_norm": 0.18617959320545197,
      "learning_rate": 2.9595570534424653e-05,
      "loss": 1.3477,
      "step": 132
    },
    {
      "epoch": 0.04790203493607059,
      "grad_norm": 0.19791485369205475,
      "learning_rate": 2.9591959557053446e-05,
      "loss": 1.4904,
      "step": 133
    },
    {
      "epoch": 0.04826220061228165,
      "grad_norm": 0.18982745707035065,
      "learning_rate": 2.9588348579682234e-05,
      "loss": 1.5146,
      "step": 134
    },
    {
      "epoch": 0.04862236628849271,
      "grad_norm": 0.180281862616539,
      "learning_rate": 2.9584737602311027e-05,
      "loss": 1.5233,
      "step": 135
    },
    {
      "epoch": 0.048982531964703764,
      "grad_norm": 0.1829923391342163,
      "learning_rate": 2.9581126624939816e-05,
      "loss": 1.3372,
      "step": 136
    },
    {
      "epoch": 0.04934269764091482,
      "grad_norm": 0.19257934391498566,
      "learning_rate": 2.9577515647568608e-05,
      "loss": 1.4122,
      "step": 137
    },
    {
      "epoch": 0.04970286331712588,
      "grad_norm": 0.19983375072479248,
      "learning_rate": 2.95739046701974e-05,
      "loss": 1.4437,
      "step": 138
    },
    {
      "epoch": 0.050063028993336936,
      "grad_norm": 0.19179606437683105,
      "learning_rate": 2.9570293692826193e-05,
      "loss": 1.4805,
      "step": 139
    },
    {
      "epoch": 0.05042319466954799,
      "grad_norm": 0.17893359065055847,
      "learning_rate": 2.9566682715454985e-05,
      "loss": 1.3952,
      "step": 140
    },
    {
      "epoch": 0.050783360345759046,
      "grad_norm": 0.2017267942428589,
      "learning_rate": 2.9563071738083777e-05,
      "loss": 1.3802,
      "step": 141
    },
    {
      "epoch": 0.05114352602197011,
      "grad_norm": 0.20433509349822998,
      "learning_rate": 2.9559460760712566e-05,
      "loss": 1.3564,
      "step": 142
    },
    {
      "epoch": 0.05150369169818116,
      "grad_norm": 0.20002135634422302,
      "learning_rate": 2.955584978334136e-05,
      "loss": 1.3958,
      "step": 143
    },
    {
      "epoch": 0.05186385737439222,
      "grad_norm": 0.18815334141254425,
      "learning_rate": 2.9552238805970147e-05,
      "loss": 1.3367,
      "step": 144
    },
    {
      "epoch": 0.05222402305060328,
      "grad_norm": 0.200324147939682,
      "learning_rate": 2.9548627828598943e-05,
      "loss": 1.4668,
      "step": 145
    },
    {
      "epoch": 0.052584188726814335,
      "grad_norm": 0.17745444178581238,
      "learning_rate": 2.9545016851227735e-05,
      "loss": 1.4462,
      "step": 146
    },
    {
      "epoch": 0.05294435440302539,
      "grad_norm": 0.21197175979614258,
      "learning_rate": 2.9541405873856524e-05,
      "loss": 1.4747,
      "step": 147
    },
    {
      "epoch": 0.05330452007923645,
      "grad_norm": 0.1845823973417282,
      "learning_rate": 2.9537794896485316e-05,
      "loss": 1.3712,
      "step": 148
    },
    {
      "epoch": 0.05366468575544751,
      "grad_norm": 0.18001818656921387,
      "learning_rate": 2.953418391911411e-05,
      "loss": 1.3815,
      "step": 149
    },
    {
      "epoch": 0.05402485143165856,
      "grad_norm": 0.2067052274942398,
      "learning_rate": 2.9530572941742898e-05,
      "loss": 1.4416,
      "step": 150
    },
    {
      "epoch": 0.05438501710786962,
      "grad_norm": 0.1842821091413498,
      "learning_rate": 2.952696196437169e-05,
      "loss": 1.5635,
      "step": 151
    },
    {
      "epoch": 0.05474518278408068,
      "grad_norm": 0.18826791644096375,
      "learning_rate": 2.9523350987000482e-05,
      "loss": 1.4654,
      "step": 152
    },
    {
      "epoch": 0.055105348460291734,
      "grad_norm": 0.17712664604187012,
      "learning_rate": 2.9519740009629275e-05,
      "loss": 1.3403,
      "step": 153
    },
    {
      "epoch": 0.05546551413650279,
      "grad_norm": 0.18622149527072906,
      "learning_rate": 2.9516129032258067e-05,
      "loss": 1.4401,
      "step": 154
    },
    {
      "epoch": 0.05582567981271385,
      "grad_norm": 0.1990392506122589,
      "learning_rate": 2.9512518054886856e-05,
      "loss": 1.5213,
      "step": 155
    },
    {
      "epoch": 0.056185845488924906,
      "grad_norm": 0.1830984205007553,
      "learning_rate": 2.9508907077515648e-05,
      "loss": 1.4824,
      "step": 156
    },
    {
      "epoch": 0.05654601116513596,
      "grad_norm": 0.1809527575969696,
      "learning_rate": 2.950529610014444e-05,
      "loss": 1.3934,
      "step": 157
    },
    {
      "epoch": 0.05690617684134702,
      "grad_norm": 0.19260360300540924,
      "learning_rate": 2.950168512277323e-05,
      "loss": 1.4423,
      "step": 158
    },
    {
      "epoch": 0.05726634251755808,
      "grad_norm": 0.19104433059692383,
      "learning_rate": 2.9498074145402025e-05,
      "loss": 1.3917,
      "step": 159
    },
    {
      "epoch": 0.05762650819376913,
      "grad_norm": 0.19608184695243835,
      "learning_rate": 2.9494463168030814e-05,
      "loss": 1.3973,
      "step": 160
    },
    {
      "epoch": 0.05798667386998019,
      "grad_norm": 0.19173943996429443,
      "learning_rate": 2.9490852190659606e-05,
      "loss": 1.4298,
      "step": 161
    },
    {
      "epoch": 0.05834683954619125,
      "grad_norm": 0.18511782586574554,
      "learning_rate": 2.94872412132884e-05,
      "loss": 1.3397,
      "step": 162
    },
    {
      "epoch": 0.058707005222402305,
      "grad_norm": 0.1851251721382141,
      "learning_rate": 2.9483630235917187e-05,
      "loss": 1.5103,
      "step": 163
    },
    {
      "epoch": 0.05906717089861336,
      "grad_norm": 0.18437612056732178,
      "learning_rate": 2.948001925854598e-05,
      "loss": 1.3573,
      "step": 164
    },
    {
      "epoch": 0.05942733657482442,
      "grad_norm": 0.18905702233314514,
      "learning_rate": 2.9476408281174775e-05,
      "loss": 1.5024,
      "step": 165
    },
    {
      "epoch": 0.059787502251035476,
      "grad_norm": 0.18073490262031555,
      "learning_rate": 2.9472797303803564e-05,
      "loss": 1.5288,
      "step": 166
    },
    {
      "epoch": 0.06014766792724653,
      "grad_norm": 0.19182296097278595,
      "learning_rate": 2.9469186326432357e-05,
      "loss": 1.4106,
      "step": 167
    },
    {
      "epoch": 0.06050783360345759,
      "grad_norm": 0.20119619369506836,
      "learning_rate": 2.9465575349061145e-05,
      "loss": 1.3514,
      "step": 168
    },
    {
      "epoch": 0.06086799927966865,
      "grad_norm": 0.1822376698255539,
      "learning_rate": 2.9461964371689938e-05,
      "loss": 1.4567,
      "step": 169
    },
    {
      "epoch": 0.0612281649558797,
      "grad_norm": 0.18643079698085785,
      "learning_rate": 2.945835339431873e-05,
      "loss": 1.4256,
      "step": 170
    },
    {
      "epoch": 0.06158833063209076,
      "grad_norm": 0.20308901369571686,
      "learning_rate": 2.945474241694752e-05,
      "loss": 1.4286,
      "step": 171
    },
    {
      "epoch": 0.06194849630830182,
      "grad_norm": 0.18466806411743164,
      "learning_rate": 2.9451131439576315e-05,
      "loss": 1.4099,
      "step": 172
    },
    {
      "epoch": 0.062308661984512875,
      "grad_norm": 0.19794075191020966,
      "learning_rate": 2.9447520462205107e-05,
      "loss": 1.533,
      "step": 173
    },
    {
      "epoch": 0.06266882766072393,
      "grad_norm": 0.20073522627353668,
      "learning_rate": 2.9443909484833896e-05,
      "loss": 1.3694,
      "step": 174
    },
    {
      "epoch": 0.06302899333693499,
      "grad_norm": 0.19706247746944427,
      "learning_rate": 2.9440298507462688e-05,
      "loss": 1.4278,
      "step": 175
    },
    {
      "epoch": 0.06338915901314604,
      "grad_norm": 0.18581700325012207,
      "learning_rate": 2.9436687530091477e-05,
      "loss": 1.4416,
      "step": 176
    },
    {
      "epoch": 0.06374932468935711,
      "grad_norm": 0.20702750980854034,
      "learning_rate": 2.943307655272027e-05,
      "loss": 1.446,
      "step": 177
    },
    {
      "epoch": 0.06410949036556816,
      "grad_norm": 0.197096586227417,
      "learning_rate": 2.942946557534906e-05,
      "loss": 1.4901,
      "step": 178
    },
    {
      "epoch": 0.06446965604177922,
      "grad_norm": 0.19005578756332397,
      "learning_rate": 2.9425854597977854e-05,
      "loss": 1.4076,
      "step": 179
    },
    {
      "epoch": 0.06482982171799027,
      "grad_norm": 0.18684080243110657,
      "learning_rate": 2.9422243620606646e-05,
      "loss": 1.3887,
      "step": 180
    },
    {
      "epoch": 0.06518998739420133,
      "grad_norm": 0.191757470369339,
      "learning_rate": 2.941863264323544e-05,
      "loss": 1.5041,
      "step": 181
    },
    {
      "epoch": 0.06555015307041238,
      "grad_norm": 0.20178364217281342,
      "learning_rate": 2.9415021665864227e-05,
      "loss": 1.4375,
      "step": 182
    },
    {
      "epoch": 0.06591031874662345,
      "grad_norm": 0.18982061743736267,
      "learning_rate": 2.941141068849302e-05,
      "loss": 1.4311,
      "step": 183
    },
    {
      "epoch": 0.06627048442283451,
      "grad_norm": 0.20788541436195374,
      "learning_rate": 2.940779971112181e-05,
      "loss": 1.3928,
      "step": 184
    },
    {
      "epoch": 0.06663065009904556,
      "grad_norm": 0.18675442039966583,
      "learning_rate": 2.94041887337506e-05,
      "loss": 1.4728,
      "step": 185
    },
    {
      "epoch": 0.06699081577525662,
      "grad_norm": 0.19132216274738312,
      "learning_rate": 2.9400577756379397e-05,
      "loss": 1.4196,
      "step": 186
    },
    {
      "epoch": 0.06735098145146767,
      "grad_norm": 0.20405253767967224,
      "learning_rate": 2.9396966779008186e-05,
      "loss": 1.43,
      "step": 187
    },
    {
      "epoch": 0.06771114712767873,
      "grad_norm": 0.20731964707374573,
      "learning_rate": 2.9393355801636978e-05,
      "loss": 1.4935,
      "step": 188
    },
    {
      "epoch": 0.06807131280388978,
      "grad_norm": 0.20007005333900452,
      "learning_rate": 2.938974482426577e-05,
      "loss": 1.3475,
      "step": 189
    },
    {
      "epoch": 0.06843147848010085,
      "grad_norm": 0.206342414021492,
      "learning_rate": 2.938613384689456e-05,
      "loss": 1.4102,
      "step": 190
    },
    {
      "epoch": 0.0687916441563119,
      "grad_norm": 0.19380620121955872,
      "learning_rate": 2.938252286952335e-05,
      "loss": 1.3835,
      "step": 191
    },
    {
      "epoch": 0.06915180983252296,
      "grad_norm": 0.20670530200004578,
      "learning_rate": 2.9378911892152144e-05,
      "loss": 1.4717,
      "step": 192
    },
    {
      "epoch": 0.06951197550873402,
      "grad_norm": 0.19167135655879974,
      "learning_rate": 2.9375300914780936e-05,
      "loss": 1.4653,
      "step": 193
    },
    {
      "epoch": 0.06987214118494507,
      "grad_norm": 0.19237881898880005,
      "learning_rate": 2.9371689937409728e-05,
      "loss": 1.3917,
      "step": 194
    },
    {
      "epoch": 0.07023230686115613,
      "grad_norm": 0.20252062380313873,
      "learning_rate": 2.9368078960038517e-05,
      "loss": 1.4053,
      "step": 195
    },
    {
      "epoch": 0.0705924725373672,
      "grad_norm": 0.19304117560386658,
      "learning_rate": 2.936446798266731e-05,
      "loss": 1.367,
      "step": 196
    },
    {
      "epoch": 0.07095263821357825,
      "grad_norm": 0.20529165863990784,
      "learning_rate": 2.9360857005296102e-05,
      "loss": 1.4367,
      "step": 197
    },
    {
      "epoch": 0.0713128038897893,
      "grad_norm": 0.20438291132450104,
      "learning_rate": 2.935724602792489e-05,
      "loss": 1.532,
      "step": 198
    },
    {
      "epoch": 0.07167296956600036,
      "grad_norm": 0.19183601438999176,
      "learning_rate": 2.9353635050553686e-05,
      "loss": 1.3324,
      "step": 199
    },
    {
      "epoch": 0.07203313524221142,
      "grad_norm": 0.1914803683757782,
      "learning_rate": 2.9350024073182475e-05,
      "loss": 1.3678,
      "step": 200
    },
    {
      "epoch": 0.07239330091842247,
      "grad_norm": 0.1936551332473755,
      "learning_rate": 2.9346413095811268e-05,
      "loss": 1.3986,
      "step": 201
    },
    {
      "epoch": 0.07275346659463353,
      "grad_norm": 0.20026059448719025,
      "learning_rate": 2.934280211844006e-05,
      "loss": 1.3421,
      "step": 202
    },
    {
      "epoch": 0.0731136322708446,
      "grad_norm": 0.1878579705953598,
      "learning_rate": 2.933919114106885e-05,
      "loss": 1.3527,
      "step": 203
    },
    {
      "epoch": 0.07347379794705565,
      "grad_norm": 0.2049810290336609,
      "learning_rate": 2.933558016369764e-05,
      "loss": 1.5394,
      "step": 204
    },
    {
      "epoch": 0.0738339636232667,
      "grad_norm": 0.19305504858493805,
      "learning_rate": 2.9331969186326433e-05,
      "loss": 1.4722,
      "step": 205
    },
    {
      "epoch": 0.07419412929947776,
      "grad_norm": 0.19251607358455658,
      "learning_rate": 2.9328358208955226e-05,
      "loss": 1.3499,
      "step": 206
    },
    {
      "epoch": 0.07455429497568881,
      "grad_norm": 0.20395797491073608,
      "learning_rate": 2.9324747231584018e-05,
      "loss": 1.5912,
      "step": 207
    },
    {
      "epoch": 0.07491446065189987,
      "grad_norm": 0.18488971889019012,
      "learning_rate": 2.9321136254212807e-05,
      "loss": 1.3115,
      "step": 208
    },
    {
      "epoch": 0.07527462632811092,
      "grad_norm": 0.19896459579467773,
      "learning_rate": 2.93175252768416e-05,
      "loss": 1.3288,
      "step": 209
    },
    {
      "epoch": 0.075634792004322,
      "grad_norm": 0.20384755730628967,
      "learning_rate": 2.931391429947039e-05,
      "loss": 1.4036,
      "step": 210
    },
    {
      "epoch": 0.07599495768053305,
      "grad_norm": 0.19265273213386536,
      "learning_rate": 2.931030332209918e-05,
      "loss": 1.2827,
      "step": 211
    },
    {
      "epoch": 0.0763551233567441,
      "grad_norm": 0.20232273638248444,
      "learning_rate": 2.9306692344727973e-05,
      "loss": 1.4829,
      "step": 212
    },
    {
      "epoch": 0.07671528903295516,
      "grad_norm": 0.1959133893251419,
      "learning_rate": 2.9303081367356765e-05,
      "loss": 1.4157,
      "step": 213
    },
    {
      "epoch": 0.07707545470916621,
      "grad_norm": 0.19469614326953888,
      "learning_rate": 2.9299470389985557e-05,
      "loss": 1.4291,
      "step": 214
    },
    {
      "epoch": 0.07743562038537727,
      "grad_norm": 0.20305177569389343,
      "learning_rate": 2.929585941261435e-05,
      "loss": 1.45,
      "step": 215
    },
    {
      "epoch": 0.07779578606158834,
      "grad_norm": 0.19006547331809998,
      "learning_rate": 2.929224843524314e-05,
      "loss": 1.35,
      "step": 216
    },
    {
      "epoch": 0.07815595173779939,
      "grad_norm": 0.19678820669651031,
      "learning_rate": 2.928863745787193e-05,
      "loss": 1.3657,
      "step": 217
    },
    {
      "epoch": 0.07851611741401045,
      "grad_norm": 0.19474224746227264,
      "learning_rate": 2.9285026480500723e-05,
      "loss": 1.3675,
      "step": 218
    },
    {
      "epoch": 0.0788762830902215,
      "grad_norm": 0.19666458666324615,
      "learning_rate": 2.9281415503129515e-05,
      "loss": 1.3506,
      "step": 219
    },
    {
      "epoch": 0.07923644876643256,
      "grad_norm": 0.20244617760181427,
      "learning_rate": 2.9277804525758308e-05,
      "loss": 1.3394,
      "step": 220
    },
    {
      "epoch": 0.07959661444264361,
      "grad_norm": 0.2090068906545639,
      "learning_rate": 2.9274193548387097e-05,
      "loss": 1.2774,
      "step": 221
    },
    {
      "epoch": 0.07995678011885467,
      "grad_norm": 0.19914290308952332,
      "learning_rate": 2.927058257101589e-05,
      "loss": 1.4218,
      "step": 222
    },
    {
      "epoch": 0.08031694579506574,
      "grad_norm": 0.2019362449645996,
      "learning_rate": 2.926697159364468e-05,
      "loss": 1.4258,
      "step": 223
    },
    {
      "epoch": 0.08067711147127679,
      "grad_norm": 0.20058073103427887,
      "learning_rate": 2.926336061627347e-05,
      "loss": 1.3906,
      "step": 224
    },
    {
      "epoch": 0.08103727714748785,
      "grad_norm": 0.19866898655891418,
      "learning_rate": 2.9259749638902262e-05,
      "loss": 1.3676,
      "step": 225
    },
    {
      "epoch": 0.0813974428236989,
      "grad_norm": 0.19343040883541107,
      "learning_rate": 2.9256138661531058e-05,
      "loss": 1.4114,
      "step": 226
    },
    {
      "epoch": 0.08175760849990996,
      "grad_norm": 0.21314916014671326,
      "learning_rate": 2.9252527684159847e-05,
      "loss": 1.4276,
      "step": 227
    },
    {
      "epoch": 0.08211777417612101,
      "grad_norm": 0.19783227145671844,
      "learning_rate": 2.924891670678864e-05,
      "loss": 1.4436,
      "step": 228
    },
    {
      "epoch": 0.08247793985233207,
      "grad_norm": 0.21717797219753265,
      "learning_rate": 2.9245305729417428e-05,
      "loss": 1.4528,
      "step": 229
    },
    {
      "epoch": 0.08283810552854314,
      "grad_norm": 0.2198711782693863,
      "learning_rate": 2.924169475204622e-05,
      "loss": 1.412,
      "step": 230
    },
    {
      "epoch": 0.08319827120475419,
      "grad_norm": 0.20149360597133636,
      "learning_rate": 2.9238083774675013e-05,
      "loss": 1.3052,
      "step": 231
    },
    {
      "epoch": 0.08355843688096525,
      "grad_norm": 0.19675356149673462,
      "learning_rate": 2.92344727973038e-05,
      "loss": 1.274,
      "step": 232
    },
    {
      "epoch": 0.0839186025571763,
      "grad_norm": 0.20662164688110352,
      "learning_rate": 2.9230861819932597e-05,
      "loss": 1.3463,
      "step": 233
    },
    {
      "epoch": 0.08427876823338736,
      "grad_norm": 0.1964236944913864,
      "learning_rate": 2.922725084256139e-05,
      "loss": 1.3749,
      "step": 234
    },
    {
      "epoch": 0.08463893390959841,
      "grad_norm": 0.21260260045528412,
      "learning_rate": 2.922363986519018e-05,
      "loss": 1.3857,
      "step": 235
    },
    {
      "epoch": 0.08499909958580948,
      "grad_norm": 0.20414502918720245,
      "learning_rate": 2.922002888781897e-05,
      "loss": 1.3621,
      "step": 236
    },
    {
      "epoch": 0.08535926526202053,
      "grad_norm": 0.20397444069385529,
      "learning_rate": 2.921641791044776e-05,
      "loss": 1.454,
      "step": 237
    },
    {
      "epoch": 0.08571943093823159,
      "grad_norm": 0.21632489562034607,
      "learning_rate": 2.9212806933076552e-05,
      "loss": 1.4725,
      "step": 238
    },
    {
      "epoch": 0.08607959661444264,
      "grad_norm": 0.20790140330791473,
      "learning_rate": 2.9209195955705344e-05,
      "loss": 1.2638,
      "step": 239
    },
    {
      "epoch": 0.0864397622906537,
      "grad_norm": 0.19501993060112,
      "learning_rate": 2.9205584978334137e-05,
      "loss": 1.4125,
      "step": 240
    },
    {
      "epoch": 0.08679992796686475,
      "grad_norm": 0.21522562205791473,
      "learning_rate": 2.920197400096293e-05,
      "loss": 1.3811,
      "step": 241
    },
    {
      "epoch": 0.08716009364307581,
      "grad_norm": 0.19850996136665344,
      "learning_rate": 2.919836302359172e-05,
      "loss": 1.3344,
      "step": 242
    },
    {
      "epoch": 0.08752025931928688,
      "grad_norm": 0.20163895189762115,
      "learning_rate": 2.919475204622051e-05,
      "loss": 1.3166,
      "step": 243
    },
    {
      "epoch": 0.08788042499549793,
      "grad_norm": 0.20849673449993134,
      "learning_rate": 2.9191141068849302e-05,
      "loss": 1.3391,
      "step": 244
    },
    {
      "epoch": 0.08824059067170899,
      "grad_norm": 0.20998847484588623,
      "learning_rate": 2.918753009147809e-05,
      "loss": 1.2703,
      "step": 245
    },
    {
      "epoch": 0.08860075634792004,
      "grad_norm": 0.20428785681724548,
      "learning_rate": 2.9183919114106887e-05,
      "loss": 1.344,
      "step": 246
    },
    {
      "epoch": 0.0889609220241311,
      "grad_norm": 0.20991180837154388,
      "learning_rate": 2.918030813673568e-05,
      "loss": 1.4569,
      "step": 247
    },
    {
      "epoch": 0.08932108770034215,
      "grad_norm": 0.20801907777786255,
      "learning_rate": 2.9176697159364468e-05,
      "loss": 1.3875,
      "step": 248
    },
    {
      "epoch": 0.08968125337655321,
      "grad_norm": 0.20362688601016998,
      "learning_rate": 2.917308618199326e-05,
      "loss": 1.4055,
      "step": 249
    },
    {
      "epoch": 0.09004141905276428,
      "grad_norm": 0.20513083040714264,
      "learning_rate": 2.9169475204622053e-05,
      "loss": 1.3999,
      "step": 250
    },
    {
      "epoch": 0.09040158472897533,
      "grad_norm": 0.20876877009868622,
      "learning_rate": 2.916586422725084e-05,
      "loss": 1.3408,
      "step": 251
    },
    {
      "epoch": 0.09076175040518639,
      "grad_norm": 0.19570396840572357,
      "learning_rate": 2.9162253249879634e-05,
      "loss": 1.3365,
      "step": 252
    },
    {
      "epoch": 0.09112191608139744,
      "grad_norm": 0.21018388867378235,
      "learning_rate": 2.9158642272508426e-05,
      "loss": 1.3702,
      "step": 253
    },
    {
      "epoch": 0.0914820817576085,
      "grad_norm": 0.20319487154483795,
      "learning_rate": 2.915503129513722e-05,
      "loss": 1.4054,
      "step": 254
    },
    {
      "epoch": 0.09184224743381955,
      "grad_norm": 0.2057499885559082,
      "learning_rate": 2.915142031776601e-05,
      "loss": 1.4227,
      "step": 255
    },
    {
      "epoch": 0.09220241311003062,
      "grad_norm": 0.20354628562927246,
      "learning_rate": 2.91478093403948e-05,
      "loss": 1.4416,
      "step": 256
    },
    {
      "epoch": 0.09256257878624168,
      "grad_norm": 0.21318651735782623,
      "learning_rate": 2.9144198363023592e-05,
      "loss": 1.3747,
      "step": 257
    },
    {
      "epoch": 0.09292274446245273,
      "grad_norm": 0.22165781259536743,
      "learning_rate": 2.9140587385652384e-05,
      "loss": 1.3902,
      "step": 258
    },
    {
      "epoch": 0.09328291013866379,
      "grad_norm": 0.2158079594373703,
      "learning_rate": 2.9136976408281173e-05,
      "loss": 1.4147,
      "step": 259
    },
    {
      "epoch": 0.09364307581487484,
      "grad_norm": 0.21651074290275574,
      "learning_rate": 2.913336543090997e-05,
      "loss": 1.2786,
      "step": 260
    },
    {
      "epoch": 0.0940032414910859,
      "grad_norm": 0.20792531967163086,
      "learning_rate": 2.9129754453538758e-05,
      "loss": 1.3532,
      "step": 261
    },
    {
      "epoch": 0.09436340716729695,
      "grad_norm": 0.20644894242286682,
      "learning_rate": 2.912614347616755e-05,
      "loss": 1.393,
      "step": 262
    },
    {
      "epoch": 0.09472357284350802,
      "grad_norm": 0.19992060959339142,
      "learning_rate": 2.9122532498796342e-05,
      "loss": 1.299,
      "step": 263
    },
    {
      "epoch": 0.09508373851971907,
      "grad_norm": 0.21508225798606873,
      "learning_rate": 2.911892152142513e-05,
      "loss": 1.319,
      "step": 264
    },
    {
      "epoch": 0.09544390419593013,
      "grad_norm": 0.21250642836093903,
      "learning_rate": 2.9115310544053924e-05,
      "loss": 1.3283,
      "step": 265
    },
    {
      "epoch": 0.09580406987214118,
      "grad_norm": 0.2057727426290512,
      "learning_rate": 2.9111699566682716e-05,
      "loss": 1.3425,
      "step": 266
    },
    {
      "epoch": 0.09616423554835224,
      "grad_norm": 0.20768551528453827,
      "learning_rate": 2.9108088589311508e-05,
      "loss": 1.3907,
      "step": 267
    },
    {
      "epoch": 0.0965244012245633,
      "grad_norm": 0.22078706324100494,
      "learning_rate": 2.91044776119403e-05,
      "loss": 1.4294,
      "step": 268
    },
    {
      "epoch": 0.09688456690077435,
      "grad_norm": 0.21853283047676086,
      "learning_rate": 2.910086663456909e-05,
      "loss": 1.3671,
      "step": 269
    },
    {
      "epoch": 0.09724473257698542,
      "grad_norm": 0.23177598416805267,
      "learning_rate": 2.9097255657197882e-05,
      "loss": 1.4661,
      "step": 270
    },
    {
      "epoch": 0.09760489825319647,
      "grad_norm": 0.21365799009799957,
      "learning_rate": 2.9093644679826674e-05,
      "loss": 1.395,
      "step": 271
    },
    {
      "epoch": 0.09796506392940753,
      "grad_norm": 0.20982883870601654,
      "learning_rate": 2.9090033702455463e-05,
      "loss": 1.2833,
      "step": 272
    },
    {
      "epoch": 0.09832522960561858,
      "grad_norm": 0.2294354885816574,
      "learning_rate": 2.908642272508426e-05,
      "loss": 1.528,
      "step": 273
    },
    {
      "epoch": 0.09868539528182964,
      "grad_norm": 0.21723322570323944,
      "learning_rate": 2.908281174771305e-05,
      "loss": 1.3059,
      "step": 274
    },
    {
      "epoch": 0.0990455609580407,
      "grad_norm": 0.20565177500247955,
      "learning_rate": 2.907920077034184e-05,
      "loss": 1.2831,
      "step": 275
    },
    {
      "epoch": 0.09940572663425176,
      "grad_norm": 0.21318672597408295,
      "learning_rate": 2.9075589792970632e-05,
      "loss": 1.3229,
      "step": 276
    },
    {
      "epoch": 0.09976589231046282,
      "grad_norm": 0.2215481549501419,
      "learning_rate": 2.907197881559942e-05,
      "loss": 1.3037,
      "step": 277
    },
    {
      "epoch": 0.10012605798667387,
      "grad_norm": 0.2039259672164917,
      "learning_rate": 2.9068367838228213e-05,
      "loss": 1.3802,
      "step": 278
    },
    {
      "epoch": 0.10048622366288493,
      "grad_norm": 0.233512744307518,
      "learning_rate": 2.9064756860857006e-05,
      "loss": 1.3053,
      "step": 279
    },
    {
      "epoch": 0.10084638933909598,
      "grad_norm": 0.2122342735528946,
      "learning_rate": 2.9061145883485798e-05,
      "loss": 1.4527,
      "step": 280
    },
    {
      "epoch": 0.10120655501530704,
      "grad_norm": 0.21205353736877441,
      "learning_rate": 2.905753490611459e-05,
      "loss": 1.4406,
      "step": 281
    },
    {
      "epoch": 0.10156672069151809,
      "grad_norm": 0.22758004069328308,
      "learning_rate": 2.9053923928743383e-05,
      "loss": 1.4404,
      "step": 282
    },
    {
      "epoch": 0.10192688636772916,
      "grad_norm": 0.21105360984802246,
      "learning_rate": 2.905031295137217e-05,
      "loss": 1.3656,
      "step": 283
    },
    {
      "epoch": 0.10228705204394022,
      "grad_norm": 0.2233448028564453,
      "learning_rate": 2.9046701974000964e-05,
      "loss": 1.3905,
      "step": 284
    },
    {
      "epoch": 0.10264721772015127,
      "grad_norm": 0.23074114322662354,
      "learning_rate": 2.9043090996629753e-05,
      "loss": 1.311,
      "step": 285
    },
    {
      "epoch": 0.10300738339636233,
      "grad_norm": 0.23665404319763184,
      "learning_rate": 2.9039480019258545e-05,
      "loss": 1.4422,
      "step": 286
    },
    {
      "epoch": 0.10336754907257338,
      "grad_norm": 0.2092353105545044,
      "learning_rate": 2.903586904188734e-05,
      "loss": 1.4021,
      "step": 287
    },
    {
      "epoch": 0.10372771474878444,
      "grad_norm": 0.2116660177707672,
      "learning_rate": 2.903225806451613e-05,
      "loss": 1.4458,
      "step": 288
    },
    {
      "epoch": 0.1040878804249955,
      "grad_norm": 0.21690016984939575,
      "learning_rate": 2.9028647087144922e-05,
      "loss": 1.2792,
      "step": 289
    },
    {
      "epoch": 0.10444804610120656,
      "grad_norm": 0.23138253390789032,
      "learning_rate": 2.9025036109773714e-05,
      "loss": 1.3149,
      "step": 290
    },
    {
      "epoch": 0.10480821177741761,
      "grad_norm": 0.21653808653354645,
      "learning_rate": 2.9021425132402503e-05,
      "loss": 1.3994,
      "step": 291
    },
    {
      "epoch": 0.10516837745362867,
      "grad_norm": 0.2196579873561859,
      "learning_rate": 2.9017814155031295e-05,
      "loss": 1.4258,
      "step": 292
    },
    {
      "epoch": 0.10552854312983972,
      "grad_norm": 0.23126919567584991,
      "learning_rate": 2.9014203177660084e-05,
      "loss": 1.4075,
      "step": 293
    },
    {
      "epoch": 0.10588870880605078,
      "grad_norm": 0.2222573459148407,
      "learning_rate": 2.901059220028888e-05,
      "loss": 1.2608,
      "step": 294
    },
    {
      "epoch": 0.10624887448226183,
      "grad_norm": 0.2410454899072647,
      "learning_rate": 2.9006981222917672e-05,
      "loss": 1.5833,
      "step": 295
    },
    {
      "epoch": 0.1066090401584729,
      "grad_norm": 0.21649621427059174,
      "learning_rate": 2.900337024554646e-05,
      "loss": 1.3056,
      "step": 296
    },
    {
      "epoch": 0.10696920583468396,
      "grad_norm": 0.23082800209522247,
      "learning_rate": 2.8999759268175253e-05,
      "loss": 1.3516,
      "step": 297
    },
    {
      "epoch": 0.10732937151089501,
      "grad_norm": 0.22338125109672546,
      "learning_rate": 2.8996148290804046e-05,
      "loss": 1.392,
      "step": 298
    },
    {
      "epoch": 0.10768953718710607,
      "grad_norm": 0.22990868985652924,
      "learning_rate": 2.8992537313432835e-05,
      "loss": 1.2753,
      "step": 299
    },
    {
      "epoch": 0.10804970286331712,
      "grad_norm": 0.22812223434448242,
      "learning_rate": 2.898892633606163e-05,
      "loss": 1.3805,
      "step": 300
    },
    {
      "epoch": 0.10840986853952818,
      "grad_norm": 0.22860153019428253,
      "learning_rate": 2.898531535869042e-05,
      "loss": 1.3996,
      "step": 301
    },
    {
      "epoch": 0.10877003421573923,
      "grad_norm": 0.2226143181324005,
      "learning_rate": 2.898170438131921e-05,
      "loss": 1.3674,
      "step": 302
    },
    {
      "epoch": 0.1091301998919503,
      "grad_norm": 0.2273710072040558,
      "learning_rate": 2.8978093403948004e-05,
      "loss": 1.5104,
      "step": 303
    },
    {
      "epoch": 0.10949036556816136,
      "grad_norm": 0.22202646732330322,
      "learning_rate": 2.8974482426576793e-05,
      "loss": 1.3726,
      "step": 304
    },
    {
      "epoch": 0.10985053124437241,
      "grad_norm": 0.2265104055404663,
      "learning_rate": 2.8970871449205585e-05,
      "loss": 1.3798,
      "step": 305
    },
    {
      "epoch": 0.11021069692058347,
      "grad_norm": 0.2166312038898468,
      "learning_rate": 2.8967260471834377e-05,
      "loss": 1.3696,
      "step": 306
    },
    {
      "epoch": 0.11057086259679452,
      "grad_norm": 0.21997547149658203,
      "learning_rate": 2.896364949446317e-05,
      "loss": 1.2494,
      "step": 307
    },
    {
      "epoch": 0.11093102827300558,
      "grad_norm": 0.2218310832977295,
      "learning_rate": 2.8960038517091962e-05,
      "loss": 1.2564,
      "step": 308
    },
    {
      "epoch": 0.11129119394921665,
      "grad_norm": 0.22420644760131836,
      "learning_rate": 2.895642753972075e-05,
      "loss": 1.3309,
      "step": 309
    },
    {
      "epoch": 0.1116513596254277,
      "grad_norm": 0.22174197435379028,
      "learning_rate": 2.8952816562349543e-05,
      "loss": 1.3669,
      "step": 310
    },
    {
      "epoch": 0.11201152530163876,
      "grad_norm": 0.21888749301433563,
      "learning_rate": 2.8949205584978335e-05,
      "loss": 1.3705,
      "step": 311
    },
    {
      "epoch": 0.11237169097784981,
      "grad_norm": 0.2113817036151886,
      "learning_rate": 2.8945594607607124e-05,
      "loss": 1.2984,
      "step": 312
    },
    {
      "epoch": 0.11273185665406087,
      "grad_norm": 0.239140123128891,
      "learning_rate": 2.8941983630235917e-05,
      "loss": 1.4079,
      "step": 313
    },
    {
      "epoch": 0.11309202233027192,
      "grad_norm": 0.22153572738170624,
      "learning_rate": 2.8938372652864712e-05,
      "loss": 1.3825,
      "step": 314
    },
    {
      "epoch": 0.11345218800648298,
      "grad_norm": 0.2285381704568863,
      "learning_rate": 2.89347616754935e-05,
      "loss": 1.4243,
      "step": 315
    },
    {
      "epoch": 0.11381235368269405,
      "grad_norm": 0.21294943988323212,
      "learning_rate": 2.8931150698122294e-05,
      "loss": 1.2946,
      "step": 316
    },
    {
      "epoch": 0.1141725193589051,
      "grad_norm": 0.22422540187835693,
      "learning_rate": 2.8927539720751082e-05,
      "loss": 1.3265,
      "step": 317
    },
    {
      "epoch": 0.11453268503511616,
      "grad_norm": 0.2370932251214981,
      "learning_rate": 2.8923928743379875e-05,
      "loss": 1.4006,
      "step": 318
    },
    {
      "epoch": 0.11489285071132721,
      "grad_norm": 0.23471127450466156,
      "learning_rate": 2.8920317766008667e-05,
      "loss": 1.4693,
      "step": 319
    },
    {
      "epoch": 0.11525301638753827,
      "grad_norm": 0.23289278149604797,
      "learning_rate": 2.8916706788637456e-05,
      "loss": 1.3562,
      "step": 320
    },
    {
      "epoch": 0.11561318206374932,
      "grad_norm": 0.2208426147699356,
      "learning_rate": 2.891309581126625e-05,
      "loss": 1.277,
      "step": 321
    },
    {
      "epoch": 0.11597334773996038,
      "grad_norm": 0.2255161553621292,
      "learning_rate": 2.8909484833895044e-05,
      "loss": 1.4884,
      "step": 322
    },
    {
      "epoch": 0.11633351341617144,
      "grad_norm": 0.22470396757125854,
      "learning_rate": 2.8905873856523833e-05,
      "loss": 1.2972,
      "step": 323
    },
    {
      "epoch": 0.1166936790923825,
      "grad_norm": 0.22466929256916046,
      "learning_rate": 2.8902262879152625e-05,
      "loss": 1.3821,
      "step": 324
    },
    {
      "epoch": 0.11705384476859355,
      "grad_norm": 0.22911620140075684,
      "learning_rate": 2.8898651901781414e-05,
      "loss": 1.3063,
      "step": 325
    },
    {
      "epoch": 0.11741401044480461,
      "grad_norm": 0.22246409952640533,
      "learning_rate": 2.8895040924410206e-05,
      "loss": 1.3604,
      "step": 326
    },
    {
      "epoch": 0.11777417612101566,
      "grad_norm": 0.21587605774402618,
      "learning_rate": 2.8891429947039002e-05,
      "loss": 1.3253,
      "step": 327
    },
    {
      "epoch": 0.11813434179722672,
      "grad_norm": 0.23472774028778076,
      "learning_rate": 2.888781896966779e-05,
      "loss": 1.3913,
      "step": 328
    },
    {
      "epoch": 0.11849450747343779,
      "grad_norm": 0.2273298054933548,
      "learning_rate": 2.8884207992296583e-05,
      "loss": 1.2797,
      "step": 329
    },
    {
      "epoch": 0.11885467314964884,
      "grad_norm": 0.21769560873508453,
      "learning_rate": 2.8880597014925376e-05,
      "loss": 1.3624,
      "step": 330
    },
    {
      "epoch": 0.1192148388258599,
      "grad_norm": 0.23625770211219788,
      "learning_rate": 2.8876986037554164e-05,
      "loss": 1.411,
      "step": 331
    },
    {
      "epoch": 0.11957500450207095,
      "grad_norm": 0.2285350114107132,
      "learning_rate": 2.8873375060182957e-05,
      "loss": 1.3757,
      "step": 332
    },
    {
      "epoch": 0.11993517017828201,
      "grad_norm": 0.21959835290908813,
      "learning_rate": 2.8869764082811746e-05,
      "loss": 1.2292,
      "step": 333
    },
    {
      "epoch": 0.12029533585449306,
      "grad_norm": 0.2332044243812561,
      "learning_rate": 2.886615310544054e-05,
      "loss": 1.397,
      "step": 334
    },
    {
      "epoch": 0.12065550153070412,
      "grad_norm": 0.2428867667913437,
      "learning_rate": 2.8862542128069334e-05,
      "loss": 1.4207,
      "step": 335
    },
    {
      "epoch": 0.12101566720691519,
      "grad_norm": 0.21320383250713348,
      "learning_rate": 2.8858931150698123e-05,
      "loss": 1.3294,
      "step": 336
    },
    {
      "epoch": 0.12137583288312624,
      "grad_norm": 0.23297454416751862,
      "learning_rate": 2.8855320173326915e-05,
      "loss": 1.3981,
      "step": 337
    },
    {
      "epoch": 0.1217359985593373,
      "grad_norm": 0.2372557371854782,
      "learning_rate": 2.8851709195955707e-05,
      "loss": 1.3624,
      "step": 338
    },
    {
      "epoch": 0.12209616423554835,
      "grad_norm": 0.22728052735328674,
      "learning_rate": 2.8848098218584496e-05,
      "loss": 1.3696,
      "step": 339
    },
    {
      "epoch": 0.1224563299117594,
      "grad_norm": 0.2173340767621994,
      "learning_rate": 2.884448724121329e-05,
      "loss": 1.3275,
      "step": 340
    },
    {
      "epoch": 0.12281649558797046,
      "grad_norm": 0.21954648196697235,
      "learning_rate": 2.884087626384208e-05,
      "loss": 1.3098,
      "step": 341
    },
    {
      "epoch": 0.12317666126418152,
      "grad_norm": 0.23730050027370453,
      "learning_rate": 2.8837265286470873e-05,
      "loss": 1.4658,
      "step": 342
    },
    {
      "epoch": 0.12353682694039259,
      "grad_norm": 0.2258806824684143,
      "learning_rate": 2.8833654309099665e-05,
      "loss": 1.4106,
      "step": 343
    },
    {
      "epoch": 0.12389699261660364,
      "grad_norm": 0.22681516408920288,
      "learning_rate": 2.8830043331728454e-05,
      "loss": 1.3533,
      "step": 344
    },
    {
      "epoch": 0.1242571582928147,
      "grad_norm": 0.22295163571834564,
      "learning_rate": 2.8826432354357246e-05,
      "loss": 1.3244,
      "step": 345
    },
    {
      "epoch": 0.12461732396902575,
      "grad_norm": 0.2345491647720337,
      "learning_rate": 2.882282137698604e-05,
      "loss": 1.3644,
      "step": 346
    },
    {
      "epoch": 0.1249774896452368,
      "grad_norm": 0.22796232998371124,
      "learning_rate": 2.8819210399614828e-05,
      "loss": 1.3238,
      "step": 347
    },
    {
      "epoch": 0.12533765532144786,
      "grad_norm": 0.24308635294437408,
      "learning_rate": 2.8815599422243623e-05,
      "loss": 1.3454,
      "step": 348
    },
    {
      "epoch": 0.12569782099765892,
      "grad_norm": 0.23906579613685608,
      "learning_rate": 2.8811988444872412e-05,
      "loss": 1.4832,
      "step": 349
    },
    {
      "epoch": 0.12605798667386997,
      "grad_norm": 0.21956582367420197,
      "learning_rate": 2.8808377467501204e-05,
      "loss": 1.3374,
      "step": 350
    },
    {
      "epoch": 0.12641815235008103,
      "grad_norm": 0.24250797927379608,
      "learning_rate": 2.8804766490129997e-05,
      "loss": 1.4206,
      "step": 351
    },
    {
      "epoch": 0.12677831802629208,
      "grad_norm": 0.2421501725912094,
      "learning_rate": 2.8801155512758786e-05,
      "loss": 1.4407,
      "step": 352
    },
    {
      "epoch": 0.12713848370250316,
      "grad_norm": 0.2312784641981125,
      "learning_rate": 2.8797544535387578e-05,
      "loss": 1.3405,
      "step": 353
    },
    {
      "epoch": 0.12749864937871422,
      "grad_norm": 0.2343374788761139,
      "learning_rate": 2.8793933558016374e-05,
      "loss": 1.3096,
      "step": 354
    },
    {
      "epoch": 0.12785881505492527,
      "grad_norm": 0.23616039752960205,
      "learning_rate": 2.8790322580645163e-05,
      "loss": 1.3991,
      "step": 355
    },
    {
      "epoch": 0.12821898073113633,
      "grad_norm": 0.24416041374206543,
      "learning_rate": 2.8786711603273955e-05,
      "loss": 1.4441,
      "step": 356
    },
    {
      "epoch": 0.12857914640734738,
      "grad_norm": 0.23603743314743042,
      "learning_rate": 2.8783100625902744e-05,
      "loss": 1.4197,
      "step": 357
    },
    {
      "epoch": 0.12893931208355844,
      "grad_norm": 0.24049702286720276,
      "learning_rate": 2.8779489648531536e-05,
      "loss": 1.4222,
      "step": 358
    },
    {
      "epoch": 0.1292994777597695,
      "grad_norm": 0.24555370211601257,
      "learning_rate": 2.877587867116033e-05,
      "loss": 1.4218,
      "step": 359
    },
    {
      "epoch": 0.12965964343598055,
      "grad_norm": 0.22216707468032837,
      "learning_rate": 2.8772267693789117e-05,
      "loss": 1.2819,
      "step": 360
    },
    {
      "epoch": 0.1300198091121916,
      "grad_norm": 0.23450502753257751,
      "learning_rate": 2.8768656716417913e-05,
      "loss": 1.3046,
      "step": 361
    },
    {
      "epoch": 0.13037997478840266,
      "grad_norm": 0.23462122678756714,
      "learning_rate": 2.8765045739046705e-05,
      "loss": 1.3215,
      "step": 362
    },
    {
      "epoch": 0.1307401404646137,
      "grad_norm": 0.23328612744808197,
      "learning_rate": 2.8761434761675494e-05,
      "loss": 1.341,
      "step": 363
    },
    {
      "epoch": 0.13110030614082477,
      "grad_norm": 0.23133815824985504,
      "learning_rate": 2.8757823784304286e-05,
      "loss": 1.3757,
      "step": 364
    },
    {
      "epoch": 0.13146047181703582,
      "grad_norm": 0.22604043781757355,
      "learning_rate": 2.8754212806933075e-05,
      "loss": 1.445,
      "step": 365
    },
    {
      "epoch": 0.1318206374932469,
      "grad_norm": 0.2337178885936737,
      "learning_rate": 2.8750601829561868e-05,
      "loss": 1.2548,
      "step": 366
    },
    {
      "epoch": 0.13218080316945796,
      "grad_norm": 0.23905770480632782,
      "learning_rate": 2.874699085219066e-05,
      "loss": 1.3741,
      "step": 367
    },
    {
      "epoch": 0.13254096884566902,
      "grad_norm": 0.23626989126205444,
      "learning_rate": 2.8743379874819452e-05,
      "loss": 1.4126,
      "step": 368
    },
    {
      "epoch": 0.13290113452188007,
      "grad_norm": 0.23062895238399506,
      "learning_rate": 2.8739768897448245e-05,
      "loss": 1.2366,
      "step": 369
    },
    {
      "epoch": 0.13326130019809113,
      "grad_norm": 0.2303735315799713,
      "learning_rate": 2.8736157920077037e-05,
      "loss": 1.308,
      "step": 370
    },
    {
      "epoch": 0.13362146587430218,
      "grad_norm": 0.23234136402606964,
      "learning_rate": 2.8732546942705826e-05,
      "loss": 1.3691,
      "step": 371
    },
    {
      "epoch": 0.13398163155051324,
      "grad_norm": 0.22999511659145355,
      "learning_rate": 2.8728935965334618e-05,
      "loss": 1.2582,
      "step": 372
    },
    {
      "epoch": 0.1343417972267243,
      "grad_norm": 0.2337975949048996,
      "learning_rate": 2.8725324987963407e-05,
      "loss": 1.3284,
      "step": 373
    },
    {
      "epoch": 0.13470196290293535,
      "grad_norm": 0.24474196135997772,
      "learning_rate": 2.87217140105922e-05,
      "loss": 1.3736,
      "step": 374
    },
    {
      "epoch": 0.1350621285791464,
      "grad_norm": 0.24122856557369232,
      "learning_rate": 2.8718103033220995e-05,
      "loss": 1.5024,
      "step": 375
    },
    {
      "epoch": 0.13542229425535746,
      "grad_norm": 0.2435533106327057,
      "learning_rate": 2.8714492055849784e-05,
      "loss": 1.4024,
      "step": 376
    },
    {
      "epoch": 0.1357824599315685,
      "grad_norm": 0.23782195150852203,
      "learning_rate": 2.8710881078478576e-05,
      "loss": 1.3817,
      "step": 377
    },
    {
      "epoch": 0.13614262560777957,
      "grad_norm": 0.2345922589302063,
      "learning_rate": 2.870727010110737e-05,
      "loss": 1.2913,
      "step": 378
    },
    {
      "epoch": 0.13650279128399065,
      "grad_norm": 0.24055905640125275,
      "learning_rate": 2.8703659123736157e-05,
      "loss": 1.2336,
      "step": 379
    },
    {
      "epoch": 0.1368629569602017,
      "grad_norm": 0.2348117083311081,
      "learning_rate": 2.870004814636495e-05,
      "loss": 1.3051,
      "step": 380
    },
    {
      "epoch": 0.13722312263641276,
      "grad_norm": 0.2538188397884369,
      "learning_rate": 2.8696437168993742e-05,
      "loss": 1.4153,
      "step": 381
    },
    {
      "epoch": 0.1375832883126238,
      "grad_norm": 0.23780590295791626,
      "learning_rate": 2.8692826191622534e-05,
      "loss": 1.3663,
      "step": 382
    },
    {
      "epoch": 0.13794345398883487,
      "grad_norm": 0.23112432658672333,
      "learning_rate": 2.8689215214251327e-05,
      "loss": 1.2916,
      "step": 383
    },
    {
      "epoch": 0.13830361966504592,
      "grad_norm": 0.2440546154975891,
      "learning_rate": 2.8685604236880115e-05,
      "loss": 1.4387,
      "step": 384
    },
    {
      "epoch": 0.13866378534125698,
      "grad_norm": 0.22636453807353973,
      "learning_rate": 2.8681993259508908e-05,
      "loss": 1.2728,
      "step": 385
    },
    {
      "epoch": 0.13902395101746803,
      "grad_norm": 0.2389863282442093,
      "learning_rate": 2.86783822821377e-05,
      "loss": 1.2578,
      "step": 386
    },
    {
      "epoch": 0.1393841166936791,
      "grad_norm": 0.23972301185131073,
      "learning_rate": 2.867477130476649e-05,
      "loss": 1.3435,
      "step": 387
    },
    {
      "epoch": 0.13974428236989014,
      "grad_norm": 0.258516788482666,
      "learning_rate": 2.8671160327395285e-05,
      "loss": 1.455,
      "step": 388
    },
    {
      "epoch": 0.1401044480461012,
      "grad_norm": 0.2252066731452942,
      "learning_rate": 2.8667549350024074e-05,
      "loss": 1.3092,
      "step": 389
    },
    {
      "epoch": 0.14046461372231225,
      "grad_norm": 0.25225746631622314,
      "learning_rate": 2.8663938372652866e-05,
      "loss": 1.4095,
      "step": 390
    },
    {
      "epoch": 0.1408247793985233,
      "grad_norm": 0.23667548596858978,
      "learning_rate": 2.8660327395281658e-05,
      "loss": 1.4687,
      "step": 391
    },
    {
      "epoch": 0.1411849450747344,
      "grad_norm": 0.23528188467025757,
      "learning_rate": 2.8656716417910447e-05,
      "loss": 1.3574,
      "step": 392
    },
    {
      "epoch": 0.14154511075094545,
      "grad_norm": 0.2528766095638275,
      "learning_rate": 2.865310544053924e-05,
      "loss": 1.3534,
      "step": 393
    },
    {
      "epoch": 0.1419052764271565,
      "grad_norm": 0.24444344639778137,
      "learning_rate": 2.864949446316803e-05,
      "loss": 1.4907,
      "step": 394
    },
    {
      "epoch": 0.14226544210336756,
      "grad_norm": 0.24068200588226318,
      "learning_rate": 2.8645883485796824e-05,
      "loss": 1.352,
      "step": 395
    },
    {
      "epoch": 0.1426256077795786,
      "grad_norm": 0.24550771713256836,
      "learning_rate": 2.8642272508425616e-05,
      "loss": 1.3299,
      "step": 396
    },
    {
      "epoch": 0.14298577345578967,
      "grad_norm": 0.2538597285747528,
      "learning_rate": 2.8638661531054405e-05,
      "loss": 1.3203,
      "step": 397
    },
    {
      "epoch": 0.14334593913200072,
      "grad_norm": 0.24831660091876984,
      "learning_rate": 2.8635050553683197e-05,
      "loss": 1.3141,
      "step": 398
    },
    {
      "epoch": 0.14370610480821178,
      "grad_norm": 0.24730919301509857,
      "learning_rate": 2.863143957631199e-05,
      "loss": 1.3165,
      "step": 399
    },
    {
      "epoch": 0.14406627048442283,
      "grad_norm": 0.246711865067482,
      "learning_rate": 2.862782859894078e-05,
      "loss": 1.3511,
      "step": 400
    },
    {
      "epoch": 0.1444264361606339,
      "grad_norm": 0.24079518020153046,
      "learning_rate": 2.862421762156957e-05,
      "loss": 1.3376,
      "step": 401
    },
    {
      "epoch": 0.14478660183684494,
      "grad_norm": 0.25202158093452454,
      "learning_rate": 2.8620606644198363e-05,
      "loss": 1.539,
      "step": 402
    },
    {
      "epoch": 0.145146767513056,
      "grad_norm": 0.2449522167444229,
      "learning_rate": 2.8616995666827156e-05,
      "loss": 1.3274,
      "step": 403
    },
    {
      "epoch": 0.14550693318926705,
      "grad_norm": 0.24639731645584106,
      "learning_rate": 2.8613384689455948e-05,
      "loss": 1.381,
      "step": 404
    },
    {
      "epoch": 0.1458670988654781,
      "grad_norm": 0.255276620388031,
      "learning_rate": 2.8609773712084737e-05,
      "loss": 1.4104,
      "step": 405
    },
    {
      "epoch": 0.1462272645416892,
      "grad_norm": 0.2535833716392517,
      "learning_rate": 2.860616273471353e-05,
      "loss": 1.4811,
      "step": 406
    },
    {
      "epoch": 0.14658743021790024,
      "grad_norm": 0.23267094790935516,
      "learning_rate": 2.860255175734232e-05,
      "loss": 1.3504,
      "step": 407
    },
    {
      "epoch": 0.1469475958941113,
      "grad_norm": 0.25488731265068054,
      "learning_rate": 2.8598940779971114e-05,
      "loss": 1.2934,
      "step": 408
    },
    {
      "epoch": 0.14730776157032235,
      "grad_norm": 0.24155108630657196,
      "learning_rate": 2.8595329802599906e-05,
      "loss": 1.3465,
      "step": 409
    },
    {
      "epoch": 0.1476679272465334,
      "grad_norm": 0.26184555888175964,
      "learning_rate": 2.8591718825228695e-05,
      "loss": 1.4896,
      "step": 410
    },
    {
      "epoch": 0.14802809292274446,
      "grad_norm": 0.25232604146003723,
      "learning_rate": 2.8588107847857487e-05,
      "loss": 1.3723,
      "step": 411
    },
    {
      "epoch": 0.14838825859895552,
      "grad_norm": 0.24840177595615387,
      "learning_rate": 2.858449687048628e-05,
      "loss": 1.4606,
      "step": 412
    },
    {
      "epoch": 0.14874842427516657,
      "grad_norm": 0.2464205026626587,
      "learning_rate": 2.858088589311507e-05,
      "loss": 1.463,
      "step": 413
    },
    {
      "epoch": 0.14910858995137763,
      "grad_norm": 0.253017783164978,
      "learning_rate": 2.857727491574386e-05,
      "loss": 1.4043,
      "step": 414
    },
    {
      "epoch": 0.14946875562758868,
      "grad_norm": 0.23361119627952576,
      "learning_rate": 2.8573663938372656e-05,
      "loss": 1.3011,
      "step": 415
    },
    {
      "epoch": 0.14982892130379974,
      "grad_norm": 0.23798801004886627,
      "learning_rate": 2.8570052961001445e-05,
      "loss": 1.2816,
      "step": 416
    },
    {
      "epoch": 0.1501890869800108,
      "grad_norm": 0.24625687301158905,
      "learning_rate": 2.8566441983630238e-05,
      "loss": 1.3357,
      "step": 417
    },
    {
      "epoch": 0.15054925265622185,
      "grad_norm": 0.2386818826198578,
      "learning_rate": 2.8562831006259026e-05,
      "loss": 1.2991,
      "step": 418
    },
    {
      "epoch": 0.15090941833243293,
      "grad_norm": 0.26528140902519226,
      "learning_rate": 2.855922002888782e-05,
      "loss": 1.4431,
      "step": 419
    },
    {
      "epoch": 0.151269584008644,
      "grad_norm": 0.27580496668815613,
      "learning_rate": 2.855560905151661e-05,
      "loss": 1.4215,
      "step": 420
    },
    {
      "epoch": 0.15162974968485504,
      "grad_norm": 0.24381551146507263,
      "learning_rate": 2.85519980741454e-05,
      "loss": 1.3012,
      "step": 421
    },
    {
      "epoch": 0.1519899153610661,
      "grad_norm": 0.25131964683532715,
      "learning_rate": 2.8548387096774196e-05,
      "loss": 1.4649,
      "step": 422
    },
    {
      "epoch": 0.15235008103727715,
      "grad_norm": 0.2569398880004883,
      "learning_rate": 2.8544776119402988e-05,
      "loss": 1.3596,
      "step": 423
    },
    {
      "epoch": 0.1527102467134882,
      "grad_norm": 0.2533249855041504,
      "learning_rate": 2.8541165142031777e-05,
      "loss": 1.2069,
      "step": 424
    },
    {
      "epoch": 0.15307041238969926,
      "grad_norm": 0.23537218570709229,
      "learning_rate": 2.853755416466057e-05,
      "loss": 1.2998,
      "step": 425
    },
    {
      "epoch": 0.15343057806591032,
      "grad_norm": 0.24241824448108673,
      "learning_rate": 2.8533943187289358e-05,
      "loss": 1.3815,
      "step": 426
    },
    {
      "epoch": 0.15379074374212137,
      "grad_norm": 0.2586810290813446,
      "learning_rate": 2.853033220991815e-05,
      "loss": 1.2369,
      "step": 427
    },
    {
      "epoch": 0.15415090941833243,
      "grad_norm": 0.252786248922348,
      "learning_rate": 2.8526721232546946e-05,
      "loss": 1.3615,
      "step": 428
    },
    {
      "epoch": 0.15451107509454348,
      "grad_norm": 0.2381085455417633,
      "learning_rate": 2.8523110255175735e-05,
      "loss": 1.2497,
      "step": 429
    },
    {
      "epoch": 0.15487124077075454,
      "grad_norm": 0.2524685859680176,
      "learning_rate": 2.8519499277804527e-05,
      "loss": 1.4199,
      "step": 430
    },
    {
      "epoch": 0.1552314064469656,
      "grad_norm": 0.25596901774406433,
      "learning_rate": 2.851588830043332e-05,
      "loss": 1.3473,
      "step": 431
    },
    {
      "epoch": 0.15559157212317667,
      "grad_norm": 0.25313469767570496,
      "learning_rate": 2.851227732306211e-05,
      "loss": 1.4446,
      "step": 432
    },
    {
      "epoch": 0.15595173779938773,
      "grad_norm": 0.25597772002220154,
      "learning_rate": 2.85086663456909e-05,
      "loss": 1.369,
      "step": 433
    },
    {
      "epoch": 0.15631190347559878,
      "grad_norm": 0.25913840532302856,
      "learning_rate": 2.850505536831969e-05,
      "loss": 1.4416,
      "step": 434
    },
    {
      "epoch": 0.15667206915180984,
      "grad_norm": 0.2559639513492584,
      "learning_rate": 2.8501444390948485e-05,
      "loss": 1.3935,
      "step": 435
    },
    {
      "epoch": 0.1570322348280209,
      "grad_norm": 0.26336461305618286,
      "learning_rate": 2.8497833413577278e-05,
      "loss": 1.387,
      "step": 436
    },
    {
      "epoch": 0.15739240050423195,
      "grad_norm": 0.24561816453933716,
      "learning_rate": 2.8494222436206067e-05,
      "loss": 1.2664,
      "step": 437
    },
    {
      "epoch": 0.157752566180443,
      "grad_norm": 0.24421754479408264,
      "learning_rate": 2.849061145883486e-05,
      "loss": 1.3346,
      "step": 438
    },
    {
      "epoch": 0.15811273185665406,
      "grad_norm": 0.2417510449886322,
      "learning_rate": 2.848700048146365e-05,
      "loss": 1.3733,
      "step": 439
    },
    {
      "epoch": 0.15847289753286511,
      "grad_norm": 0.25689801573753357,
      "learning_rate": 2.848338950409244e-05,
      "loss": 1.458,
      "step": 440
    },
    {
      "epoch": 0.15883306320907617,
      "grad_norm": 0.2503771483898163,
      "learning_rate": 2.8479778526721232e-05,
      "loss": 1.3159,
      "step": 441
    },
    {
      "epoch": 0.15919322888528722,
      "grad_norm": 0.2423420548439026,
      "learning_rate": 2.8476167549350025e-05,
      "loss": 1.2733,
      "step": 442
    },
    {
      "epoch": 0.15955339456149828,
      "grad_norm": 0.2559417486190796,
      "learning_rate": 2.8472556571978817e-05,
      "loss": 1.4159,
      "step": 443
    },
    {
      "epoch": 0.15991356023770933,
      "grad_norm": 0.23685067892074585,
      "learning_rate": 2.846894559460761e-05,
      "loss": 1.3914,
      "step": 444
    },
    {
      "epoch": 0.1602737259139204,
      "grad_norm": 0.24941034615039825,
      "learning_rate": 2.8465334617236398e-05,
      "loss": 1.2824,
      "step": 445
    },
    {
      "epoch": 0.16063389159013147,
      "grad_norm": 0.24399159848690033,
      "learning_rate": 2.846172363986519e-05,
      "loss": 1.3317,
      "step": 446
    },
    {
      "epoch": 0.16099405726634253,
      "grad_norm": 0.24865785241127014,
      "learning_rate": 2.8458112662493983e-05,
      "loss": 1.4649,
      "step": 447
    },
    {
      "epoch": 0.16135422294255358,
      "grad_norm": 0.2372186779975891,
      "learning_rate": 2.845450168512277e-05,
      "loss": 1.2322,
      "step": 448
    },
    {
      "epoch": 0.16171438861876464,
      "grad_norm": 0.25732487440109253,
      "learning_rate": 2.8450890707751567e-05,
      "loss": 1.3782,
      "step": 449
    },
    {
      "epoch": 0.1620745542949757,
      "grad_norm": 0.2514286935329437,
      "learning_rate": 2.8447279730380356e-05,
      "loss": 1.3301,
      "step": 450
    },
    {
      "epoch": 0.16243471997118675,
      "grad_norm": 0.25661346316337585,
      "learning_rate": 2.844366875300915e-05,
      "loss": 1.3379,
      "step": 451
    },
    {
      "epoch": 0.1627948856473978,
      "grad_norm": 0.26529207825660706,
      "learning_rate": 2.844005777563794e-05,
      "loss": 1.4162,
      "step": 452
    },
    {
      "epoch": 0.16315505132360886,
      "grad_norm": 0.24314865469932556,
      "learning_rate": 2.843644679826673e-05,
      "loss": 1.2891,
      "step": 453
    },
    {
      "epoch": 0.1635152169998199,
      "grad_norm": 0.2491666078567505,
      "learning_rate": 2.8432835820895522e-05,
      "loss": 1.3815,
      "step": 454
    },
    {
      "epoch": 0.16387538267603097,
      "grad_norm": 0.25214728713035583,
      "learning_rate": 2.8429224843524318e-05,
      "loss": 1.2958,
      "step": 455
    },
    {
      "epoch": 0.16423554835224202,
      "grad_norm": 0.25531384348869324,
      "learning_rate": 2.8425613866153107e-05,
      "loss": 1.2927,
      "step": 456
    },
    {
      "epoch": 0.16459571402845308,
      "grad_norm": 0.24969936907291412,
      "learning_rate": 2.84220028887819e-05,
      "loss": 1.3524,
      "step": 457
    },
    {
      "epoch": 0.16495587970466413,
      "grad_norm": 0.25487545132637024,
      "learning_rate": 2.8418391911410688e-05,
      "loss": 1.3252,
      "step": 458
    },
    {
      "epoch": 0.16531604538087522,
      "grad_norm": 0.2580616772174835,
      "learning_rate": 2.841478093403948e-05,
      "loss": 1.3692,
      "step": 459
    },
    {
      "epoch": 0.16567621105708627,
      "grad_norm": 0.26013222336769104,
      "learning_rate": 2.8411169956668272e-05,
      "loss": 1.4668,
      "step": 460
    },
    {
      "epoch": 0.16603637673329733,
      "grad_norm": 0.26236534118652344,
      "learning_rate": 2.840755897929706e-05,
      "loss": 1.5675,
      "step": 461
    },
    {
      "epoch": 0.16639654240950838,
      "grad_norm": 0.2401973456144333,
      "learning_rate": 2.8403948001925857e-05,
      "loss": 1.3202,
      "step": 462
    },
    {
      "epoch": 0.16675670808571944,
      "grad_norm": 0.24438291788101196,
      "learning_rate": 2.840033702455465e-05,
      "loss": 1.3703,
      "step": 463
    },
    {
      "epoch": 0.1671168737619305,
      "grad_norm": 0.25580090284347534,
      "learning_rate": 2.8396726047183438e-05,
      "loss": 1.3224,
      "step": 464
    },
    {
      "epoch": 0.16747703943814155,
      "grad_norm": 0.2433803528547287,
      "learning_rate": 2.839311506981223e-05,
      "loss": 1.3456,
      "step": 465
    },
    {
      "epoch": 0.1678372051143526,
      "grad_norm": 0.27077779173851013,
      "learning_rate": 2.838950409244102e-05,
      "loss": 1.3961,
      "step": 466
    },
    {
      "epoch": 0.16819737079056366,
      "grad_norm": 0.27289050817489624,
      "learning_rate": 2.8385893115069812e-05,
      "loss": 1.5715,
      "step": 467
    },
    {
      "epoch": 0.1685575364667747,
      "grad_norm": 0.25183534622192383,
      "learning_rate": 2.8382282137698604e-05,
      "loss": 1.2789,
      "step": 468
    },
    {
      "epoch": 0.16891770214298577,
      "grad_norm": 0.24346013367176056,
      "learning_rate": 2.8378671160327396e-05,
      "loss": 1.3255,
      "step": 469
    },
    {
      "epoch": 0.16927786781919682,
      "grad_norm": 0.2573685348033905,
      "learning_rate": 2.837506018295619e-05,
      "loss": 1.341,
      "step": 470
    },
    {
      "epoch": 0.16963803349540788,
      "grad_norm": 0.25776684284210205,
      "learning_rate": 2.837144920558498e-05,
      "loss": 1.3827,
      "step": 471
    },
    {
      "epoch": 0.16999819917161896,
      "grad_norm": 0.25026562809944153,
      "learning_rate": 2.836783822821377e-05,
      "loss": 1.419,
      "step": 472
    },
    {
      "epoch": 0.17035836484783,
      "grad_norm": 0.250898152589798,
      "learning_rate": 2.8364227250842562e-05,
      "loss": 1.3951,
      "step": 473
    },
    {
      "epoch": 0.17071853052404107,
      "grad_norm": 0.25710970163345337,
      "learning_rate": 2.836061627347135e-05,
      "loss": 1.337,
      "step": 474
    },
    {
      "epoch": 0.17107869620025212,
      "grad_norm": 0.2662560045719147,
      "learning_rate": 2.8357005296100143e-05,
      "loss": 1.3641,
      "step": 475
    },
    {
      "epoch": 0.17143886187646318,
      "grad_norm": 0.26196444034576416,
      "learning_rate": 2.835339431872894e-05,
      "loss": 1.3984,
      "step": 476
    },
    {
      "epoch": 0.17179902755267423,
      "grad_norm": 0.25869786739349365,
      "learning_rate": 2.8349783341357728e-05,
      "loss": 1.2475,
      "step": 477
    },
    {
      "epoch": 0.1721591932288853,
      "grad_norm": 0.25023365020751953,
      "learning_rate": 2.834617236398652e-05,
      "loss": 1.3042,
      "step": 478
    },
    {
      "epoch": 0.17251935890509634,
      "grad_norm": 0.26860469579696655,
      "learning_rate": 2.8342561386615312e-05,
      "loss": 1.3878,
      "step": 479
    },
    {
      "epoch": 0.1728795245813074,
      "grad_norm": 0.26923036575317383,
      "learning_rate": 2.83389504092441e-05,
      "loss": 1.4391,
      "step": 480
    },
    {
      "epoch": 0.17323969025751845,
      "grad_norm": 0.2535170018672943,
      "learning_rate": 2.8335339431872894e-05,
      "loss": 1.2567,
      "step": 481
    },
    {
      "epoch": 0.1735998559337295,
      "grad_norm": 0.2595418095588684,
      "learning_rate": 2.8331728454501686e-05,
      "loss": 1.3056,
      "step": 482
    },
    {
      "epoch": 0.17396002160994056,
      "grad_norm": 0.2707410752773285,
      "learning_rate": 2.8328117477130478e-05,
      "loss": 1.2578,
      "step": 483
    },
    {
      "epoch": 0.17432018728615162,
      "grad_norm": 0.26545143127441406,
      "learning_rate": 2.832450649975927e-05,
      "loss": 1.3511,
      "step": 484
    },
    {
      "epoch": 0.1746803529623627,
      "grad_norm": 0.25936517119407654,
      "learning_rate": 2.832089552238806e-05,
      "loss": 1.3238,
      "step": 485
    },
    {
      "epoch": 0.17504051863857376,
      "grad_norm": 0.2629929780960083,
      "learning_rate": 2.8317284545016852e-05,
      "loss": 1.4307,
      "step": 486
    },
    {
      "epoch": 0.1754006843147848,
      "grad_norm": 0.2632661759853363,
      "learning_rate": 2.8313673567645644e-05,
      "loss": 1.3366,
      "step": 487
    },
    {
      "epoch": 0.17576084999099587,
      "grad_norm": 0.2688330113887787,
      "learning_rate": 2.8310062590274433e-05,
      "loss": 1.2938,
      "step": 488
    },
    {
      "epoch": 0.17612101566720692,
      "grad_norm": 0.23693718016147614,
      "learning_rate": 2.830645161290323e-05,
      "loss": 1.2966,
      "step": 489
    },
    {
      "epoch": 0.17648118134341798,
      "grad_norm": 0.2455979436635971,
      "learning_rate": 2.8302840635532018e-05,
      "loss": 1.3945,
      "step": 490
    },
    {
      "epoch": 0.17684134701962903,
      "grad_norm": 0.25101518630981445,
      "learning_rate": 2.829922965816081e-05,
      "loss": 1.3127,
      "step": 491
    },
    {
      "epoch": 0.17720151269584009,
      "grad_norm": 0.2560841739177704,
      "learning_rate": 2.8295618680789602e-05,
      "loss": 1.3601,
      "step": 492
    },
    {
      "epoch": 0.17756167837205114,
      "grad_norm": 0.25661519169807434,
      "learning_rate": 2.829200770341839e-05,
      "loss": 1.397,
      "step": 493
    },
    {
      "epoch": 0.1779218440482622,
      "grad_norm": 0.2586567997932434,
      "learning_rate": 2.8288396726047183e-05,
      "loss": 1.373,
      "step": 494
    },
    {
      "epoch": 0.17828200972447325,
      "grad_norm": 0.2673134207725525,
      "learning_rate": 2.8284785748675976e-05,
      "loss": 1.4657,
      "step": 495
    },
    {
      "epoch": 0.1786421754006843,
      "grad_norm": 0.26050707697868347,
      "learning_rate": 2.8281174771304768e-05,
      "loss": 1.2505,
      "step": 496
    },
    {
      "epoch": 0.17900234107689536,
      "grad_norm": 0.26420027017593384,
      "learning_rate": 2.827756379393356e-05,
      "loss": 1.3908,
      "step": 497
    },
    {
      "epoch": 0.17936250675310642,
      "grad_norm": 0.25245505571365356,
      "learning_rate": 2.827395281656235e-05,
      "loss": 1.3153,
      "step": 498
    },
    {
      "epoch": 0.1797226724293175,
      "grad_norm": 0.2524753510951996,
      "learning_rate": 2.827034183919114e-05,
      "loss": 1.3463,
      "step": 499
    },
    {
      "epoch": 0.18008283810552855,
      "grad_norm": 0.27328649163246155,
      "learning_rate": 2.8266730861819934e-05,
      "loss": 1.4149,
      "step": 500
    },
    {
      "epoch": 0.1804430037817396,
      "grad_norm": 0.25963231921195984,
      "learning_rate": 2.8263119884448723e-05,
      "loss": 1.4676,
      "step": 501
    },
    {
      "epoch": 0.18080316945795066,
      "grad_norm": 0.2597085237503052,
      "learning_rate": 2.8259508907077515e-05,
      "loss": 1.2718,
      "step": 502
    },
    {
      "epoch": 0.18116333513416172,
      "grad_norm": 0.2584405839443207,
      "learning_rate": 2.825589792970631e-05,
      "loss": 1.2864,
      "step": 503
    },
    {
      "epoch": 0.18152350081037277,
      "grad_norm": 0.26166486740112305,
      "learning_rate": 2.82522869523351e-05,
      "loss": 1.3041,
      "step": 504
    },
    {
      "epoch": 0.18188366648658383,
      "grad_norm": 0.2673925757408142,
      "learning_rate": 2.8248675974963892e-05,
      "loss": 1.3737,
      "step": 505
    },
    {
      "epoch": 0.18224383216279488,
      "grad_norm": 0.27087676525115967,
      "learning_rate": 2.824506499759268e-05,
      "loss": 1.3318,
      "step": 506
    },
    {
      "epoch": 0.18260399783900594,
      "grad_norm": 0.2654988765716553,
      "learning_rate": 2.8241454020221473e-05,
      "loss": 1.4103,
      "step": 507
    },
    {
      "epoch": 0.182964163515217,
      "grad_norm": 0.26258209347724915,
      "learning_rate": 2.8237843042850265e-05,
      "loss": 1.4006,
      "step": 508
    },
    {
      "epoch": 0.18332432919142805,
      "grad_norm": 0.267860472202301,
      "learning_rate": 2.8234232065479058e-05,
      "loss": 1.4207,
      "step": 509
    },
    {
      "epoch": 0.1836844948676391,
      "grad_norm": 0.2657091021537781,
      "learning_rate": 2.823062108810785e-05,
      "loss": 1.4504,
      "step": 510
    },
    {
      "epoch": 0.18404466054385016,
      "grad_norm": 0.2809159755706787,
      "learning_rate": 2.8227010110736642e-05,
      "loss": 1.4659,
      "step": 511
    },
    {
      "epoch": 0.18440482622006124,
      "grad_norm": 0.2626526653766632,
      "learning_rate": 2.822339913336543e-05,
      "loss": 1.3896,
      "step": 512
    },
    {
      "epoch": 0.1847649918962723,
      "grad_norm": 0.2703706622123718,
      "learning_rate": 2.8219788155994223e-05,
      "loss": 1.3715,
      "step": 513
    },
    {
      "epoch": 0.18512515757248335,
      "grad_norm": 0.25637122988700867,
      "learning_rate": 2.8216177178623012e-05,
      "loss": 1.2437,
      "step": 514
    },
    {
      "epoch": 0.1854853232486944,
      "grad_norm": 0.2653293311595917,
      "learning_rate": 2.8212566201251805e-05,
      "loss": 1.2566,
      "step": 515
    },
    {
      "epoch": 0.18584548892490546,
      "grad_norm": 0.26703646779060364,
      "learning_rate": 2.82089552238806e-05,
      "loss": 1.3327,
      "step": 516
    },
    {
      "epoch": 0.18620565460111652,
      "grad_norm": 0.2632271349430084,
      "learning_rate": 2.820534424650939e-05,
      "loss": 1.4734,
      "step": 517
    },
    {
      "epoch": 0.18656582027732757,
      "grad_norm": 0.26362040638923645,
      "learning_rate": 2.820173326913818e-05,
      "loss": 1.3676,
      "step": 518
    },
    {
      "epoch": 0.18692598595353863,
      "grad_norm": 0.2551349103450775,
      "learning_rate": 2.8198122291766974e-05,
      "loss": 1.3303,
      "step": 519
    },
    {
      "epoch": 0.18728615162974968,
      "grad_norm": 0.25563597679138184,
      "learning_rate": 2.8194511314395763e-05,
      "loss": 1.2848,
      "step": 520
    },
    {
      "epoch": 0.18764631730596074,
      "grad_norm": 0.24836480617523193,
      "learning_rate": 2.8190900337024555e-05,
      "loss": 1.1925,
      "step": 521
    },
    {
      "epoch": 0.1880064829821718,
      "grad_norm": 0.2586628198623657,
      "learning_rate": 2.8187289359653344e-05,
      "loss": 1.4923,
      "step": 522
    },
    {
      "epoch": 0.18836664865838285,
      "grad_norm": 0.2597426474094391,
      "learning_rate": 2.818367838228214e-05,
      "loss": 1.3359,
      "step": 523
    },
    {
      "epoch": 0.1887268143345939,
      "grad_norm": 0.2808341085910797,
      "learning_rate": 2.8180067404910932e-05,
      "loss": 1.3383,
      "step": 524
    },
    {
      "epoch": 0.18908698001080498,
      "grad_norm": 0.255546510219574,
      "learning_rate": 2.817645642753972e-05,
      "loss": 1.3773,
      "step": 525
    },
    {
      "epoch": 0.18944714568701604,
      "grad_norm": 0.2618086338043213,
      "learning_rate": 2.8172845450168513e-05,
      "loss": 1.396,
      "step": 526
    },
    {
      "epoch": 0.1898073113632271,
      "grad_norm": 0.25963813066482544,
      "learning_rate": 2.8169234472797305e-05,
      "loss": 1.2454,
      "step": 527
    },
    {
      "epoch": 0.19016747703943815,
      "grad_norm": 0.27590423822402954,
      "learning_rate": 2.8165623495426094e-05,
      "loss": 1.4319,
      "step": 528
    },
    {
      "epoch": 0.1905276427156492,
      "grad_norm": 0.2742146849632263,
      "learning_rate": 2.8162012518054887e-05,
      "loss": 1.5238,
      "step": 529
    },
    {
      "epoch": 0.19088780839186026,
      "grad_norm": 0.2651248872280121,
      "learning_rate": 2.815840154068368e-05,
      "loss": 1.3389,
      "step": 530
    },
    {
      "epoch": 0.1912479740680713,
      "grad_norm": 0.26004019379615784,
      "learning_rate": 2.815479056331247e-05,
      "loss": 1.2724,
      "step": 531
    },
    {
      "epoch": 0.19160813974428237,
      "grad_norm": 0.2769915759563446,
      "learning_rate": 2.8151179585941264e-05,
      "loss": 1.4085,
      "step": 532
    },
    {
      "epoch": 0.19196830542049342,
      "grad_norm": 0.2843688726425171,
      "learning_rate": 2.8147568608570052e-05,
      "loss": 1.3765,
      "step": 533
    },
    {
      "epoch": 0.19232847109670448,
      "grad_norm": 0.2689242959022522,
      "learning_rate": 2.8143957631198845e-05,
      "loss": 1.3887,
      "step": 534
    },
    {
      "epoch": 0.19268863677291553,
      "grad_norm": 0.24955114722251892,
      "learning_rate": 2.8140346653827637e-05,
      "loss": 1.3614,
      "step": 535
    },
    {
      "epoch": 0.1930488024491266,
      "grad_norm": 0.27345794439315796,
      "learning_rate": 2.813673567645643e-05,
      "loss": 1.3737,
      "step": 536
    },
    {
      "epoch": 0.19340896812533764,
      "grad_norm": 0.26692527532577515,
      "learning_rate": 2.813312469908522e-05,
      "loss": 1.2485,
      "step": 537
    },
    {
      "epoch": 0.1937691338015487,
      "grad_norm": 0.28262534737586975,
      "learning_rate": 2.812951372171401e-05,
      "loss": 1.3618,
      "step": 538
    },
    {
      "epoch": 0.19412929947775978,
      "grad_norm": 0.26309987902641296,
      "learning_rate": 2.8125902744342803e-05,
      "loss": 1.3465,
      "step": 539
    },
    {
      "epoch": 0.19448946515397084,
      "grad_norm": 0.26913997530937195,
      "learning_rate": 2.8122291766971595e-05,
      "loss": 1.3663,
      "step": 540
    },
    {
      "epoch": 0.1948496308301819,
      "grad_norm": 0.2621552348136902,
      "learning_rate": 2.8118680789600384e-05,
      "loss": 1.4319,
      "step": 541
    },
    {
      "epoch": 0.19520979650639295,
      "grad_norm": 0.27202093601226807,
      "learning_rate": 2.8115069812229176e-05,
      "loss": 1.4083,
      "step": 542
    },
    {
      "epoch": 0.195569962182604,
      "grad_norm": 0.2613104283809662,
      "learning_rate": 2.8111458834857972e-05,
      "loss": 1.3027,
      "step": 543
    },
    {
      "epoch": 0.19593012785881506,
      "grad_norm": 0.2813377380371094,
      "learning_rate": 2.810784785748676e-05,
      "loss": 1.4324,
      "step": 544
    },
    {
      "epoch": 0.1962902935350261,
      "grad_norm": 0.26775309443473816,
      "learning_rate": 2.8104236880115553e-05,
      "loss": 1.3667,
      "step": 545
    },
    {
      "epoch": 0.19665045921123717,
      "grad_norm": 0.264944851398468,
      "learning_rate": 2.8100625902744342e-05,
      "loss": 1.2961,
      "step": 546
    },
    {
      "epoch": 0.19701062488744822,
      "grad_norm": 0.2738226652145386,
      "learning_rate": 2.8097014925373134e-05,
      "loss": 1.3897,
      "step": 547
    },
    {
      "epoch": 0.19737079056365928,
      "grad_norm": 0.2658888101577759,
      "learning_rate": 2.8093403948001927e-05,
      "loss": 1.4419,
      "step": 548
    },
    {
      "epoch": 0.19773095623987033,
      "grad_norm": 0.2655876874923706,
      "learning_rate": 2.8089792970630716e-05,
      "loss": 1.4044,
      "step": 549
    },
    {
      "epoch": 0.1980911219160814,
      "grad_norm": 0.26355186104774475,
      "learning_rate": 2.808618199325951e-05,
      "loss": 1.3184,
      "step": 550
    },
    {
      "epoch": 0.19845128759229244,
      "grad_norm": 0.2846860885620117,
      "learning_rate": 2.8082571015888304e-05,
      "loss": 1.3871,
      "step": 551
    },
    {
      "epoch": 0.19881145326850352,
      "grad_norm": 0.29469582438468933,
      "learning_rate": 2.8078960038517093e-05,
      "loss": 1.4908,
      "step": 552
    },
    {
      "epoch": 0.19917161894471458,
      "grad_norm": 0.2677130699157715,
      "learning_rate": 2.8075349061145885e-05,
      "loss": 1.2983,
      "step": 553
    },
    {
      "epoch": 0.19953178462092563,
      "grad_norm": 0.2681998908519745,
      "learning_rate": 2.8071738083774674e-05,
      "loss": 1.1809,
      "step": 554
    },
    {
      "epoch": 0.1998919502971367,
      "grad_norm": 0.26845404505729675,
      "learning_rate": 2.8068127106403466e-05,
      "loss": 1.3858,
      "step": 555
    },
    {
      "epoch": 0.20025211597334774,
      "grad_norm": 0.2711312472820282,
      "learning_rate": 2.806451612903226e-05,
      "loss": 1.322,
      "step": 556
    },
    {
      "epoch": 0.2006122816495588,
      "grad_norm": 0.27026835083961487,
      "learning_rate": 2.806090515166105e-05,
      "loss": 1.32,
      "step": 557
    },
    {
      "epoch": 0.20097244732576985,
      "grad_norm": 0.27886128425598145,
      "learning_rate": 2.8057294174289843e-05,
      "loss": 1.4609,
      "step": 558
    },
    {
      "epoch": 0.2013326130019809,
      "grad_norm": 0.27163177728652954,
      "learning_rate": 2.8053683196918635e-05,
      "loss": 1.3644,
      "step": 559
    },
    {
      "epoch": 0.20169277867819196,
      "grad_norm": 0.2569677233695984,
      "learning_rate": 2.8050072219547424e-05,
      "loss": 1.324,
      "step": 560
    },
    {
      "epoch": 0.20205294435440302,
      "grad_norm": 0.26655638217926025,
      "learning_rate": 2.8046461242176216e-05,
      "loss": 1.2822,
      "step": 561
    },
    {
      "epoch": 0.20241311003061407,
      "grad_norm": 0.2793097198009491,
      "learning_rate": 2.8042850264805005e-05,
      "loss": 1.299,
      "step": 562
    },
    {
      "epoch": 0.20277327570682513,
      "grad_norm": 0.2666964828968048,
      "learning_rate": 2.80392392874338e-05,
      "loss": 1.2532,
      "step": 563
    },
    {
      "epoch": 0.20313344138303618,
      "grad_norm": 0.2569803297519684,
      "learning_rate": 2.8035628310062593e-05,
      "loss": 1.2735,
      "step": 564
    },
    {
      "epoch": 0.20349360705924727,
      "grad_norm": 0.2815534770488739,
      "learning_rate": 2.8032017332691382e-05,
      "loss": 1.3995,
      "step": 565
    },
    {
      "epoch": 0.20385377273545832,
      "grad_norm": 0.2681579291820526,
      "learning_rate": 2.8028406355320175e-05,
      "loss": 1.4505,
      "step": 566
    },
    {
      "epoch": 0.20421393841166938,
      "grad_norm": 0.28067368268966675,
      "learning_rate": 2.8024795377948967e-05,
      "loss": 1.4735,
      "step": 567
    },
    {
      "epoch": 0.20457410408788043,
      "grad_norm": 0.26928281784057617,
      "learning_rate": 2.8021184400577756e-05,
      "loss": 1.3457,
      "step": 568
    },
    {
      "epoch": 0.2049342697640915,
      "grad_norm": 0.26833805441856384,
      "learning_rate": 2.8017573423206548e-05,
      "loss": 1.4553,
      "step": 569
    },
    {
      "epoch": 0.20529443544030254,
      "grad_norm": 0.26911506056785583,
      "learning_rate": 2.801396244583534e-05,
      "loss": 1.3012,
      "step": 570
    },
    {
      "epoch": 0.2056546011165136,
      "grad_norm": 0.27355918288230896,
      "learning_rate": 2.8010351468464133e-05,
      "loss": 1.4154,
      "step": 571
    },
    {
      "epoch": 0.20601476679272465,
      "grad_norm": 0.27175605297088623,
      "learning_rate": 2.8006740491092925e-05,
      "loss": 1.3157,
      "step": 572
    },
    {
      "epoch": 0.2063749324689357,
      "grad_norm": 0.26886698603630066,
      "learning_rate": 2.8003129513721714e-05,
      "loss": 1.4192,
      "step": 573
    },
    {
      "epoch": 0.20673509814514676,
      "grad_norm": 0.2795126140117645,
      "learning_rate": 2.7999518536350506e-05,
      "loss": 1.3555,
      "step": 574
    },
    {
      "epoch": 0.20709526382135782,
      "grad_norm": 0.28974229097366333,
      "learning_rate": 2.79959075589793e-05,
      "loss": 1.3966,
      "step": 575
    },
    {
      "epoch": 0.20745542949756887,
      "grad_norm": 0.2672049403190613,
      "learning_rate": 2.7992296581608087e-05,
      "loss": 1.2983,
      "step": 576
    },
    {
      "epoch": 0.20781559517377993,
      "grad_norm": 0.2821231484413147,
      "learning_rate": 2.7988685604236883e-05,
      "loss": 1.3665,
      "step": 577
    },
    {
      "epoch": 0.208175760849991,
      "grad_norm": 0.3095681071281433,
      "learning_rate": 2.7985074626865672e-05,
      "loss": 1.4797,
      "step": 578
    },
    {
      "epoch": 0.20853592652620206,
      "grad_norm": 0.2712690234184265,
      "learning_rate": 2.7981463649494464e-05,
      "loss": 1.3371,
      "step": 579
    },
    {
      "epoch": 0.20889609220241312,
      "grad_norm": 0.2747133672237396,
      "learning_rate": 2.7977852672123257e-05,
      "loss": 1.3652,
      "step": 580
    },
    {
      "epoch": 0.20925625787862417,
      "grad_norm": 0.2687844932079315,
      "learning_rate": 2.7974241694752045e-05,
      "loss": 1.2892,
      "step": 581
    },
    {
      "epoch": 0.20961642355483523,
      "grad_norm": 0.2928786277770996,
      "learning_rate": 2.7970630717380838e-05,
      "loss": 1.3911,
      "step": 582
    },
    {
      "epoch": 0.20997658923104628,
      "grad_norm": 0.2643890380859375,
      "learning_rate": 2.796701974000963e-05,
      "loss": 1.245,
      "step": 583
    },
    {
      "epoch": 0.21033675490725734,
      "grad_norm": 0.2753729224205017,
      "learning_rate": 2.7963408762638422e-05,
      "loss": 1.2814,
      "step": 584
    },
    {
      "epoch": 0.2106969205834684,
      "grad_norm": 0.2894085943698883,
      "learning_rate": 2.7959797785267215e-05,
      "loss": 1.3354,
      "step": 585
    },
    {
      "epoch": 0.21105708625967945,
      "grad_norm": 0.2651832401752472,
      "learning_rate": 2.7956186807896004e-05,
      "loss": 1.3239,
      "step": 586
    },
    {
      "epoch": 0.2114172519358905,
      "grad_norm": 0.26109594106674194,
      "learning_rate": 2.7952575830524796e-05,
      "loss": 1.4708,
      "step": 587
    },
    {
      "epoch": 0.21177741761210156,
      "grad_norm": 0.27555909752845764,
      "learning_rate": 2.7948964853153588e-05,
      "loss": 1.4564,
      "step": 588
    },
    {
      "epoch": 0.21213758328831261,
      "grad_norm": 0.2712487578392029,
      "learning_rate": 2.7945353875782377e-05,
      "loss": 1.2983,
      "step": 589
    },
    {
      "epoch": 0.21249774896452367,
      "grad_norm": 0.3166933059692383,
      "learning_rate": 2.7941742898411173e-05,
      "loss": 1.3117,
      "step": 590
    },
    {
      "epoch": 0.21285791464073472,
      "grad_norm": 0.268905371427536,
      "learning_rate": 2.793813192103996e-05,
      "loss": 1.3758,
      "step": 591
    },
    {
      "epoch": 0.2132180803169458,
      "grad_norm": 0.26821640133857727,
      "learning_rate": 2.7934520943668754e-05,
      "loss": 1.3098,
      "step": 592
    },
    {
      "epoch": 0.21357824599315686,
      "grad_norm": 0.28045403957366943,
      "learning_rate": 2.7930909966297546e-05,
      "loss": 1.4206,
      "step": 593
    },
    {
      "epoch": 0.21393841166936792,
      "grad_norm": 0.26054108142852783,
      "learning_rate": 2.7927298988926335e-05,
      "loss": 1.3256,
      "step": 594
    },
    {
      "epoch": 0.21429857734557897,
      "grad_norm": 0.2742781937122345,
      "learning_rate": 2.7923688011555127e-05,
      "loss": 1.2225,
      "step": 595
    },
    {
      "epoch": 0.21465874302179003,
      "grad_norm": 0.27364474534988403,
      "learning_rate": 2.792007703418392e-05,
      "loss": 1.3115,
      "step": 596
    },
    {
      "epoch": 0.21501890869800108,
      "grad_norm": 0.25773388147354126,
      "learning_rate": 2.7916466056812712e-05,
      "loss": 1.2889,
      "step": 597
    },
    {
      "epoch": 0.21537907437421214,
      "grad_norm": 0.26875966787338257,
      "learning_rate": 2.7912855079441504e-05,
      "loss": 1.3846,
      "step": 598
    },
    {
      "epoch": 0.2157392400504232,
      "grad_norm": 0.2714908719062805,
      "learning_rate": 2.7909244102070293e-05,
      "loss": 1.2623,
      "step": 599
    },
    {
      "epoch": 0.21609940572663425,
      "grad_norm": 0.2749921381473541,
      "learning_rate": 2.7905633124699086e-05,
      "loss": 1.5119,
      "step": 600
    },
    {
      "epoch": 0.2164595714028453,
      "grad_norm": 0.27660176157951355,
      "learning_rate": 2.7902022147327878e-05,
      "loss": 1.3842,
      "step": 601
    },
    {
      "epoch": 0.21681973707905636,
      "grad_norm": 0.27467766404151917,
      "learning_rate": 2.7898411169956667e-05,
      "loss": 1.3666,
      "step": 602
    },
    {
      "epoch": 0.2171799027552674,
      "grad_norm": 0.27978798747062683,
      "learning_rate": 2.789480019258546e-05,
      "loss": 1.5481,
      "step": 603
    },
    {
      "epoch": 0.21754006843147847,
      "grad_norm": 0.2664627432823181,
      "learning_rate": 2.7891189215214255e-05,
      "loss": 1.2919,
      "step": 604
    },
    {
      "epoch": 0.21790023410768955,
      "grad_norm": 0.2688221037387848,
      "learning_rate": 2.7887578237843044e-05,
      "loss": 1.3012,
      "step": 605
    },
    {
      "epoch": 0.2182603997839006,
      "grad_norm": 0.27157750725746155,
      "learning_rate": 2.7883967260471836e-05,
      "loss": 1.3852,
      "step": 606
    },
    {
      "epoch": 0.21862056546011166,
      "grad_norm": 0.26953619718551636,
      "learning_rate": 2.7880356283100625e-05,
      "loss": 1.359,
      "step": 607
    },
    {
      "epoch": 0.21898073113632271,
      "grad_norm": 0.27265456318855286,
      "learning_rate": 2.7876745305729417e-05,
      "loss": 1.2541,
      "step": 608
    },
    {
      "epoch": 0.21934089681253377,
      "grad_norm": 0.30342161655426025,
      "learning_rate": 2.787313432835821e-05,
      "loss": 1.5207,
      "step": 609
    },
    {
      "epoch": 0.21970106248874482,
      "grad_norm": 0.2751404643058777,
      "learning_rate": 2.7869523350986998e-05,
      "loss": 1.2771,
      "step": 610
    },
    {
      "epoch": 0.22006122816495588,
      "grad_norm": 0.27699050307273865,
      "learning_rate": 2.7865912373615794e-05,
      "loss": 1.3876,
      "step": 611
    },
    {
      "epoch": 0.22042139384116693,
      "grad_norm": 0.2662172019481659,
      "learning_rate": 2.7862301396244586e-05,
      "loss": 1.3121,
      "step": 612
    },
    {
      "epoch": 0.220781559517378,
      "grad_norm": 0.26905420422554016,
      "learning_rate": 2.7858690418873375e-05,
      "loss": 1.3582,
      "step": 613
    },
    {
      "epoch": 0.22114172519358904,
      "grad_norm": 0.3069027066230774,
      "learning_rate": 2.7855079441502167e-05,
      "loss": 1.4482,
      "step": 614
    },
    {
      "epoch": 0.2215018908698001,
      "grad_norm": 0.2742874324321747,
      "learning_rate": 2.7851468464130956e-05,
      "loss": 1.3389,
      "step": 615
    },
    {
      "epoch": 0.22186205654601115,
      "grad_norm": 0.28644612431526184,
      "learning_rate": 2.784785748675975e-05,
      "loss": 1.3013,
      "step": 616
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.2686258852481842,
      "learning_rate": 2.7844246509388544e-05,
      "loss": 1.2669,
      "step": 617
    },
    {
      "epoch": 0.2225823878984333,
      "grad_norm": 0.27251607179641724,
      "learning_rate": 2.7840635532017333e-05,
      "loss": 1.2995,
      "step": 618
    },
    {
      "epoch": 0.22294255357464435,
      "grad_norm": 0.29956182837486267,
      "learning_rate": 2.7837024554646126e-05,
      "loss": 1.2972,
      "step": 619
    },
    {
      "epoch": 0.2233027192508554,
      "grad_norm": 0.2685195803642273,
      "learning_rate": 2.7833413577274918e-05,
      "loss": 1.2494,
      "step": 620
    },
    {
      "epoch": 0.22366288492706646,
      "grad_norm": 0.28932929039001465,
      "learning_rate": 2.7829802599903707e-05,
      "loss": 1.3733,
      "step": 621
    },
    {
      "epoch": 0.2240230506032775,
      "grad_norm": 0.27893394231796265,
      "learning_rate": 2.78261916225325e-05,
      "loss": 1.2906,
      "step": 622
    },
    {
      "epoch": 0.22438321627948857,
      "grad_norm": 0.2881118953227997,
      "learning_rate": 2.7822580645161288e-05,
      "loss": 1.3372,
      "step": 623
    },
    {
      "epoch": 0.22474338195569962,
      "grad_norm": 0.278950959444046,
      "learning_rate": 2.7818969667790084e-05,
      "loss": 1.2773,
      "step": 624
    },
    {
      "epoch": 0.22510354763191068,
      "grad_norm": 0.28213658928871155,
      "learning_rate": 2.7815358690418876e-05,
      "loss": 1.3596,
      "step": 625
    },
    {
      "epoch": 0.22546371330812173,
      "grad_norm": 0.2792995274066925,
      "learning_rate": 2.7811747713047665e-05,
      "loss": 1.3844,
      "step": 626
    },
    {
      "epoch": 0.2258238789843328,
      "grad_norm": 0.28148671984672546,
      "learning_rate": 2.7808136735676457e-05,
      "loss": 1.374,
      "step": 627
    },
    {
      "epoch": 0.22618404466054384,
      "grad_norm": 0.27820709347724915,
      "learning_rate": 2.780452575830525e-05,
      "loss": 1.2664,
      "step": 628
    },
    {
      "epoch": 0.2265442103367549,
      "grad_norm": 0.2832881808280945,
      "learning_rate": 2.780091478093404e-05,
      "loss": 1.2906,
      "step": 629
    },
    {
      "epoch": 0.22690437601296595,
      "grad_norm": 0.3055690824985504,
      "learning_rate": 2.779730380356283e-05,
      "loss": 1.347,
      "step": 630
    },
    {
      "epoch": 0.227264541689177,
      "grad_norm": 0.29948690533638,
      "learning_rate": 2.7793692826191623e-05,
      "loss": 1.338,
      "step": 631
    },
    {
      "epoch": 0.2276247073653881,
      "grad_norm": 0.2799561023712158,
      "learning_rate": 2.7790081848820415e-05,
      "loss": 1.3995,
      "step": 632
    },
    {
      "epoch": 0.22798487304159915,
      "grad_norm": 0.2859586179256439,
      "learning_rate": 2.7786470871449208e-05,
      "loss": 1.3733,
      "step": 633
    },
    {
      "epoch": 0.2283450387178102,
      "grad_norm": 0.2870238423347473,
      "learning_rate": 2.7782859894077996e-05,
      "loss": 1.3045,
      "step": 634
    },
    {
      "epoch": 0.22870520439402126,
      "grad_norm": 0.2747364342212677,
      "learning_rate": 2.777924891670679e-05,
      "loss": 1.4026,
      "step": 635
    },
    {
      "epoch": 0.2290653700702323,
      "grad_norm": 0.2767102122306824,
      "learning_rate": 2.777563793933558e-05,
      "loss": 1.3358,
      "step": 636
    },
    {
      "epoch": 0.22942553574644337,
      "grad_norm": 0.2900938391685486,
      "learning_rate": 2.777202696196437e-05,
      "loss": 1.3526,
      "step": 637
    },
    {
      "epoch": 0.22978570142265442,
      "grad_norm": 0.28855571150779724,
      "learning_rate": 2.7768415984593166e-05,
      "loss": 1.3541,
      "step": 638
    },
    {
      "epoch": 0.23014586709886548,
      "grad_norm": 0.2783021032810211,
      "learning_rate": 2.7764805007221955e-05,
      "loss": 1.301,
      "step": 639
    },
    {
      "epoch": 0.23050603277507653,
      "grad_norm": 0.27905410528182983,
      "learning_rate": 2.7761194029850747e-05,
      "loss": 1.4092,
      "step": 640
    },
    {
      "epoch": 0.23086619845128759,
      "grad_norm": 0.2832120358943939,
      "learning_rate": 2.775758305247954e-05,
      "loss": 1.2928,
      "step": 641
    },
    {
      "epoch": 0.23122636412749864,
      "grad_norm": 0.2812343239784241,
      "learning_rate": 2.7753972075108328e-05,
      "loss": 1.382,
      "step": 642
    },
    {
      "epoch": 0.2315865298037097,
      "grad_norm": 0.2829282283782959,
      "learning_rate": 2.775036109773712e-05,
      "loss": 1.374,
      "step": 643
    },
    {
      "epoch": 0.23194669547992075,
      "grad_norm": 0.30112525820732117,
      "learning_rate": 2.7746750120365916e-05,
      "loss": 1.4931,
      "step": 644
    },
    {
      "epoch": 0.23230686115613183,
      "grad_norm": 0.29123303294181824,
      "learning_rate": 2.7743139142994705e-05,
      "loss": 1.3013,
      "step": 645
    },
    {
      "epoch": 0.2326670268323429,
      "grad_norm": 0.2918303310871124,
      "learning_rate": 2.7739528165623497e-05,
      "loss": 1.4505,
      "step": 646
    },
    {
      "epoch": 0.23302719250855394,
      "grad_norm": 0.2880604565143585,
      "learning_rate": 2.7735917188252286e-05,
      "loss": 1.3534,
      "step": 647
    },
    {
      "epoch": 0.233387358184765,
      "grad_norm": 0.2854354977607727,
      "learning_rate": 2.773230621088108e-05,
      "loss": 1.3981,
      "step": 648
    },
    {
      "epoch": 0.23374752386097605,
      "grad_norm": 0.2723201513290405,
      "learning_rate": 2.772869523350987e-05,
      "loss": 1.3049,
      "step": 649
    },
    {
      "epoch": 0.2341076895371871,
      "grad_norm": 0.28433099389076233,
      "learning_rate": 2.772508425613866e-05,
      "loss": 1.3771,
      "step": 650
    },
    {
      "epoch": 0.23446785521339816,
      "grad_norm": 0.2880338728427887,
      "learning_rate": 2.7721473278767455e-05,
      "loss": 1.3517,
      "step": 651
    },
    {
      "epoch": 0.23482802088960922,
      "grad_norm": 0.2778661251068115,
      "learning_rate": 2.7717862301396248e-05,
      "loss": 1.305,
      "step": 652
    },
    {
      "epoch": 0.23518818656582027,
      "grad_norm": 0.2778230607509613,
      "learning_rate": 2.7714251324025037e-05,
      "loss": 1.376,
      "step": 653
    },
    {
      "epoch": 0.23554835224203133,
      "grad_norm": 0.2822732627391815,
      "learning_rate": 2.771064034665383e-05,
      "loss": 1.2394,
      "step": 654
    },
    {
      "epoch": 0.23590851791824238,
      "grad_norm": 0.2989928722381592,
      "learning_rate": 2.7707029369282618e-05,
      "loss": 1.4462,
      "step": 655
    },
    {
      "epoch": 0.23626868359445344,
      "grad_norm": 0.28253063559532166,
      "learning_rate": 2.770341839191141e-05,
      "loss": 1.3074,
      "step": 656
    },
    {
      "epoch": 0.2366288492706645,
      "grad_norm": 0.28656795620918274,
      "learning_rate": 2.7699807414540202e-05,
      "loss": 1.3047,
      "step": 657
    },
    {
      "epoch": 0.23698901494687558,
      "grad_norm": 0.2735985815525055,
      "learning_rate": 2.7696196437168995e-05,
      "loss": 1.3181,
      "step": 658
    },
    {
      "epoch": 0.23734918062308663,
      "grad_norm": 0.2745605409145355,
      "learning_rate": 2.7692585459797787e-05,
      "loss": 1.3367,
      "step": 659
    },
    {
      "epoch": 0.23770934629929769,
      "grad_norm": 0.27610284090042114,
      "learning_rate": 2.768897448242658e-05,
      "loss": 1.3877,
      "step": 660
    },
    {
      "epoch": 0.23806951197550874,
      "grad_norm": 0.3025047481060028,
      "learning_rate": 2.7685363505055368e-05,
      "loss": 1.494,
      "step": 661
    },
    {
      "epoch": 0.2384296776517198,
      "grad_norm": 0.2941235899925232,
      "learning_rate": 2.768175252768416e-05,
      "loss": 1.3504,
      "step": 662
    },
    {
      "epoch": 0.23878984332793085,
      "grad_norm": 0.30114907026290894,
      "learning_rate": 2.767814155031295e-05,
      "loss": 1.2591,
      "step": 663
    },
    {
      "epoch": 0.2391500090041419,
      "grad_norm": 0.2763369381427765,
      "learning_rate": 2.767453057294174e-05,
      "loss": 1.2886,
      "step": 664
    },
    {
      "epoch": 0.23951017468035296,
      "grad_norm": 0.2948325574398041,
      "learning_rate": 2.7670919595570537e-05,
      "loss": 1.2755,
      "step": 665
    },
    {
      "epoch": 0.23987034035656402,
      "grad_norm": 0.28571563959121704,
      "learning_rate": 2.7667308618199326e-05,
      "loss": 1.2502,
      "step": 666
    },
    {
      "epoch": 0.24023050603277507,
      "grad_norm": 0.29170116782188416,
      "learning_rate": 2.766369764082812e-05,
      "loss": 1.2466,
      "step": 667
    },
    {
      "epoch": 0.24059067170898613,
      "grad_norm": 0.2775646150112152,
      "learning_rate": 2.766008666345691e-05,
      "loss": 1.3372,
      "step": 668
    },
    {
      "epoch": 0.24095083738519718,
      "grad_norm": 0.2750648558139801,
      "learning_rate": 2.76564756860857e-05,
      "loss": 1.3376,
      "step": 669
    },
    {
      "epoch": 0.24131100306140824,
      "grad_norm": 0.30202367901802063,
      "learning_rate": 2.7652864708714492e-05,
      "loss": 1.4161,
      "step": 670
    },
    {
      "epoch": 0.24167116873761932,
      "grad_norm": 0.2889367341995239,
      "learning_rate": 2.7649253731343284e-05,
      "loss": 1.3625,
      "step": 671
    },
    {
      "epoch": 0.24203133441383037,
      "grad_norm": 0.28385499119758606,
      "learning_rate": 2.7645642753972077e-05,
      "loss": 1.3791,
      "step": 672
    },
    {
      "epoch": 0.24239150009004143,
      "grad_norm": 0.27315056324005127,
      "learning_rate": 2.764203177660087e-05,
      "loss": 1.2905,
      "step": 673
    },
    {
      "epoch": 0.24275166576625248,
      "grad_norm": 0.28917357325553894,
      "learning_rate": 2.7638420799229658e-05,
      "loss": 1.3733,
      "step": 674
    },
    {
      "epoch": 0.24311183144246354,
      "grad_norm": 0.2767167389392853,
      "learning_rate": 2.763480982185845e-05,
      "loss": 1.3278,
      "step": 675
    },
    {
      "epoch": 0.2434719971186746,
      "grad_norm": 0.3052792549133301,
      "learning_rate": 2.7631198844487242e-05,
      "loss": 1.337,
      "step": 676
    },
    {
      "epoch": 0.24383216279488565,
      "grad_norm": 0.2913898825645447,
      "learning_rate": 2.762758786711603e-05,
      "loss": 1.3009,
      "step": 677
    },
    {
      "epoch": 0.2441923284710967,
      "grad_norm": 0.28967711329460144,
      "learning_rate": 2.7623976889744827e-05,
      "loss": 1.3034,
      "step": 678
    },
    {
      "epoch": 0.24455249414730776,
      "grad_norm": 0.28068798780441284,
      "learning_rate": 2.7620365912373616e-05,
      "loss": 1.2985,
      "step": 679
    },
    {
      "epoch": 0.2449126598235188,
      "grad_norm": 0.29097670316696167,
      "learning_rate": 2.7616754935002408e-05,
      "loss": 1.3349,
      "step": 680
    },
    {
      "epoch": 0.24527282549972987,
      "grad_norm": 0.28639358282089233,
      "learning_rate": 2.76131439576312e-05,
      "loss": 1.2793,
      "step": 681
    },
    {
      "epoch": 0.24563299117594092,
      "grad_norm": 0.2983005940914154,
      "learning_rate": 2.760953298025999e-05,
      "loss": 1.3646,
      "step": 682
    },
    {
      "epoch": 0.24599315685215198,
      "grad_norm": 0.3016238808631897,
      "learning_rate": 2.7605922002888782e-05,
      "loss": 1.3223,
      "step": 683
    },
    {
      "epoch": 0.24635332252836303,
      "grad_norm": 0.2887558341026306,
      "learning_rate": 2.7602311025517574e-05,
      "loss": 1.2712,
      "step": 684
    },
    {
      "epoch": 0.24671348820457412,
      "grad_norm": 0.29620596766471863,
      "learning_rate": 2.7598700048146366e-05,
      "loss": 1.2969,
      "step": 685
    },
    {
      "epoch": 0.24707365388078517,
      "grad_norm": 0.28696027398109436,
      "learning_rate": 2.759508907077516e-05,
      "loss": 1.2137,
      "step": 686
    },
    {
      "epoch": 0.24743381955699623,
      "grad_norm": 0.28480973839759827,
      "learning_rate": 2.7591478093403948e-05,
      "loss": 1.2596,
      "step": 687
    },
    {
      "epoch": 0.24779398523320728,
      "grad_norm": 0.27607694268226624,
      "learning_rate": 2.758786711603274e-05,
      "loss": 1.2527,
      "step": 688
    },
    {
      "epoch": 0.24815415090941834,
      "grad_norm": 0.2912229597568512,
      "learning_rate": 2.7584256138661532e-05,
      "loss": 1.2645,
      "step": 689
    },
    {
      "epoch": 0.2485143165856294,
      "grad_norm": 0.2842322885990143,
      "learning_rate": 2.758064516129032e-05,
      "loss": 1.3905,
      "step": 690
    },
    {
      "epoch": 0.24887448226184045,
      "grad_norm": 0.28624820709228516,
      "learning_rate": 2.7577034183919117e-05,
      "loss": 1.2943,
      "step": 691
    },
    {
      "epoch": 0.2492346479380515,
      "grad_norm": 0.28883394598960876,
      "learning_rate": 2.757342320654791e-05,
      "loss": 1.2547,
      "step": 692
    },
    {
      "epoch": 0.24959481361426256,
      "grad_norm": 0.2934320569038391,
      "learning_rate": 2.7569812229176698e-05,
      "loss": 1.3431,
      "step": 693
    },
    {
      "epoch": 0.2499549792904736,
      "grad_norm": 0.2819425165653229,
      "learning_rate": 2.756620125180549e-05,
      "loss": 1.2349,
      "step": 694
    },
    {
      "epoch": 0.25031514496668467,
      "grad_norm": 0.2849924564361572,
      "learning_rate": 2.756259027443428e-05,
      "loss": 1.2613,
      "step": 695
    },
    {
      "epoch": 0.2506753106428957,
      "grad_norm": 0.2942381501197815,
      "learning_rate": 2.755897929706307e-05,
      "loss": 1.368,
      "step": 696
    },
    {
      "epoch": 0.2510354763191068,
      "grad_norm": 0.29420724511146545,
      "learning_rate": 2.7555368319691864e-05,
      "loss": 1.3718,
      "step": 697
    },
    {
      "epoch": 0.25139564199531783,
      "grad_norm": 0.3033851981163025,
      "learning_rate": 2.7551757342320656e-05,
      "loss": 1.3678,
      "step": 698
    },
    {
      "epoch": 0.2517558076715289,
      "grad_norm": 0.30162757635116577,
      "learning_rate": 2.754814636494945e-05,
      "loss": 1.278,
      "step": 699
    },
    {
      "epoch": 0.25211597334773994,
      "grad_norm": 0.31739485263824463,
      "learning_rate": 2.754453538757824e-05,
      "loss": 1.444,
      "step": 700
    },
    {
      "epoch": 0.252476139023951,
      "grad_norm": 0.27907615900039673,
      "learning_rate": 2.754092441020703e-05,
      "loss": 1.2184,
      "step": 701
    },
    {
      "epoch": 0.25283630470016205,
      "grad_norm": 0.29374265670776367,
      "learning_rate": 2.7537313432835822e-05,
      "loss": 1.3029,
      "step": 702
    },
    {
      "epoch": 0.2531964703763731,
      "grad_norm": 0.2871192395687103,
      "learning_rate": 2.753370245546461e-05,
      "loss": 1.3744,
      "step": 703
    },
    {
      "epoch": 0.25355663605258416,
      "grad_norm": 0.2889671325683594,
      "learning_rate": 2.7530091478093403e-05,
      "loss": 1.3615,
      "step": 704
    },
    {
      "epoch": 0.25391680172879527,
      "grad_norm": 0.29443201422691345,
      "learning_rate": 2.75264805007222e-05,
      "loss": 1.3633,
      "step": 705
    },
    {
      "epoch": 0.2542769674050063,
      "grad_norm": 0.29962486028671265,
      "learning_rate": 2.7522869523350988e-05,
      "loss": 1.3265,
      "step": 706
    },
    {
      "epoch": 0.2546371330812174,
      "grad_norm": 0.2997744679450989,
      "learning_rate": 2.751925854597978e-05,
      "loss": 1.3074,
      "step": 707
    },
    {
      "epoch": 0.25499729875742844,
      "grad_norm": 0.30482229590415955,
      "learning_rate": 2.7515647568608572e-05,
      "loss": 1.2718,
      "step": 708
    },
    {
      "epoch": 0.2553574644336395,
      "grad_norm": 0.2909998595714569,
      "learning_rate": 2.751203659123736e-05,
      "loss": 1.3091,
      "step": 709
    },
    {
      "epoch": 0.25571763010985055,
      "grad_norm": 0.2863076627254486,
      "learning_rate": 2.7508425613866153e-05,
      "loss": 1.329,
      "step": 710
    },
    {
      "epoch": 0.2560777957860616,
      "grad_norm": 0.2767583429813385,
      "learning_rate": 2.7504814636494942e-05,
      "loss": 1.2315,
      "step": 711
    },
    {
      "epoch": 0.25643796146227266,
      "grad_norm": 0.29669275879859924,
      "learning_rate": 2.7501203659123738e-05,
      "loss": 1.4169,
      "step": 712
    },
    {
      "epoch": 0.2567981271384837,
      "grad_norm": 0.28957581520080566,
      "learning_rate": 2.749759268175253e-05,
      "loss": 1.2911,
      "step": 713
    },
    {
      "epoch": 0.25715829281469477,
      "grad_norm": 0.29582056403160095,
      "learning_rate": 2.749398170438132e-05,
      "loss": 1.2948,
      "step": 714
    },
    {
      "epoch": 0.2575184584909058,
      "grad_norm": 0.2936077415943146,
      "learning_rate": 2.749037072701011e-05,
      "loss": 1.306,
      "step": 715
    },
    {
      "epoch": 0.2578786241671169,
      "grad_norm": 0.2797905206680298,
      "learning_rate": 2.7486759749638904e-05,
      "loss": 1.2557,
      "step": 716
    },
    {
      "epoch": 0.25823878984332793,
      "grad_norm": 0.2895404100418091,
      "learning_rate": 2.7483148772267693e-05,
      "loss": 1.3063,
      "step": 717
    },
    {
      "epoch": 0.258598955519539,
      "grad_norm": 0.28061482310295105,
      "learning_rate": 2.747953779489649e-05,
      "loss": 1.222,
      "step": 718
    },
    {
      "epoch": 0.25895912119575004,
      "grad_norm": 0.280772864818573,
      "learning_rate": 2.7475926817525277e-05,
      "loss": 1.3354,
      "step": 719
    },
    {
      "epoch": 0.2593192868719611,
      "grad_norm": 0.2967293858528137,
      "learning_rate": 2.747231584015407e-05,
      "loss": 1.455,
      "step": 720
    },
    {
      "epoch": 0.25967945254817215,
      "grad_norm": 0.30881503224372864,
      "learning_rate": 2.7468704862782862e-05,
      "loss": 1.3866,
      "step": 721
    },
    {
      "epoch": 0.2600396182243832,
      "grad_norm": 0.30160823464393616,
      "learning_rate": 2.746509388541165e-05,
      "loss": 1.3876,
      "step": 722
    },
    {
      "epoch": 0.26039978390059426,
      "grad_norm": 0.3104308545589447,
      "learning_rate": 2.7461482908040443e-05,
      "loss": 1.3018,
      "step": 723
    },
    {
      "epoch": 0.2607599495768053,
      "grad_norm": 0.30122971534729004,
      "learning_rate": 2.7457871930669235e-05,
      "loss": 1.3261,
      "step": 724
    },
    {
      "epoch": 0.26112011525301637,
      "grad_norm": 0.30053579807281494,
      "learning_rate": 2.7454260953298028e-05,
      "loss": 1.3648,
      "step": 725
    },
    {
      "epoch": 0.2614802809292274,
      "grad_norm": 0.2860546112060547,
      "learning_rate": 2.745064997592682e-05,
      "loss": 1.4172,
      "step": 726
    },
    {
      "epoch": 0.2618404466054385,
      "grad_norm": 0.2862987816333771,
      "learning_rate": 2.744703899855561e-05,
      "loss": 1.2972,
      "step": 727
    },
    {
      "epoch": 0.26220061228164954,
      "grad_norm": 0.29381063580513,
      "learning_rate": 2.74434280211844e-05,
      "loss": 1.3751,
      "step": 728
    },
    {
      "epoch": 0.2625607779578606,
      "grad_norm": 0.30529680848121643,
      "learning_rate": 2.7439817043813193e-05,
      "loss": 1.387,
      "step": 729
    },
    {
      "epoch": 0.26292094363407165,
      "grad_norm": 0.29632073640823364,
      "learning_rate": 2.7436206066441982e-05,
      "loss": 1.381,
      "step": 730
    },
    {
      "epoch": 0.26328110931028276,
      "grad_norm": 0.2993248701095581,
      "learning_rate": 2.7432595089070775e-05,
      "loss": 1.4835,
      "step": 731
    },
    {
      "epoch": 0.2636412749864938,
      "grad_norm": 0.30198797583580017,
      "learning_rate": 2.742898411169957e-05,
      "loss": 1.4038,
      "step": 732
    },
    {
      "epoch": 0.26400144066270487,
      "grad_norm": 0.2940475046634674,
      "learning_rate": 2.742537313432836e-05,
      "loss": 1.3014,
      "step": 733
    },
    {
      "epoch": 0.2643616063389159,
      "grad_norm": 0.28811267018318176,
      "learning_rate": 2.742176215695715e-05,
      "loss": 1.2654,
      "step": 734
    },
    {
      "epoch": 0.264721772015127,
      "grad_norm": 0.2827238440513611,
      "learning_rate": 2.741815117958594e-05,
      "loss": 1.3276,
      "step": 735
    },
    {
      "epoch": 0.26508193769133803,
      "grad_norm": 0.29286137223243713,
      "learning_rate": 2.7414540202214733e-05,
      "loss": 1.353,
      "step": 736
    },
    {
      "epoch": 0.2654421033675491,
      "grad_norm": 0.29775896668434143,
      "learning_rate": 2.7410929224843525e-05,
      "loss": 1.2117,
      "step": 737
    },
    {
      "epoch": 0.26580226904376014,
      "grad_norm": 0.28772667050361633,
      "learning_rate": 2.7407318247472314e-05,
      "loss": 1.292,
      "step": 738
    },
    {
      "epoch": 0.2661624347199712,
      "grad_norm": 0.2844211757183075,
      "learning_rate": 2.740370727010111e-05,
      "loss": 1.4061,
      "step": 739
    },
    {
      "epoch": 0.26652260039618225,
      "grad_norm": 0.28219881653785706,
      "learning_rate": 2.7400096292729902e-05,
      "loss": 1.2168,
      "step": 740
    },
    {
      "epoch": 0.2668827660723933,
      "grad_norm": 0.2903546690940857,
      "learning_rate": 2.739648531535869e-05,
      "loss": 1.3519,
      "step": 741
    },
    {
      "epoch": 0.26724293174860436,
      "grad_norm": 0.30721136927604675,
      "learning_rate": 2.7392874337987483e-05,
      "loss": 1.3269,
      "step": 742
    },
    {
      "epoch": 0.2676030974248154,
      "grad_norm": 0.30088961124420166,
      "learning_rate": 2.7389263360616272e-05,
      "loss": 1.2181,
      "step": 743
    },
    {
      "epoch": 0.26796326310102647,
      "grad_norm": 0.28214821219444275,
      "learning_rate": 2.7385652383245064e-05,
      "loss": 1.2878,
      "step": 744
    },
    {
      "epoch": 0.2683234287772375,
      "grad_norm": 0.3117235600948334,
      "learning_rate": 2.738204140587386e-05,
      "loss": 1.4023,
      "step": 745
    },
    {
      "epoch": 0.2686835944534486,
      "grad_norm": 0.28872036933898926,
      "learning_rate": 2.737843042850265e-05,
      "loss": 1.3207,
      "step": 746
    },
    {
      "epoch": 0.26904376012965964,
      "grad_norm": 0.2846059501171112,
      "learning_rate": 2.737481945113144e-05,
      "loss": 1.2939,
      "step": 747
    },
    {
      "epoch": 0.2694039258058707,
      "grad_norm": 0.30740898847579956,
      "learning_rate": 2.7371208473760234e-05,
      "loss": 1.2956,
      "step": 748
    },
    {
      "epoch": 0.26976409148208175,
      "grad_norm": 0.30091556906700134,
      "learning_rate": 2.7367597496389022e-05,
      "loss": 1.4201,
      "step": 749
    },
    {
      "epoch": 0.2701242571582928,
      "grad_norm": 0.31020957231521606,
      "learning_rate": 2.7363986519017815e-05,
      "loss": 1.3383,
      "step": 750
    },
    {
      "epoch": 0.27048442283450386,
      "grad_norm": 0.2782958447933197,
      "learning_rate": 2.7360375541646604e-05,
      "loss": 1.311,
      "step": 751
    },
    {
      "epoch": 0.2708445885107149,
      "grad_norm": 0.29233047366142273,
      "learning_rate": 2.73567645642754e-05,
      "loss": 1.2621,
      "step": 752
    },
    {
      "epoch": 0.27120475418692597,
      "grad_norm": 0.278194785118103,
      "learning_rate": 2.735315358690419e-05,
      "loss": 1.3025,
      "step": 753
    },
    {
      "epoch": 0.271564919863137,
      "grad_norm": 0.2879451513290405,
      "learning_rate": 2.734954260953298e-05,
      "loss": 1.3812,
      "step": 754
    },
    {
      "epoch": 0.2719250855393481,
      "grad_norm": 0.3112140893936157,
      "learning_rate": 2.7345931632161773e-05,
      "loss": 1.2983,
      "step": 755
    },
    {
      "epoch": 0.27228525121555913,
      "grad_norm": 0.30143827199935913,
      "learning_rate": 2.7342320654790565e-05,
      "loss": 1.4263,
      "step": 756
    },
    {
      "epoch": 0.2726454168917702,
      "grad_norm": 0.2934825122356415,
      "learning_rate": 2.7338709677419354e-05,
      "loss": 1.316,
      "step": 757
    },
    {
      "epoch": 0.2730055825679813,
      "grad_norm": 0.30001169443130493,
      "learning_rate": 2.7335098700048146e-05,
      "loss": 1.3531,
      "step": 758
    },
    {
      "epoch": 0.27336574824419235,
      "grad_norm": 0.3059183359146118,
      "learning_rate": 2.733148772267694e-05,
      "loss": 1.3502,
      "step": 759
    },
    {
      "epoch": 0.2737259139204034,
      "grad_norm": 0.2896893620491028,
      "learning_rate": 2.732787674530573e-05,
      "loss": 1.2003,
      "step": 760
    },
    {
      "epoch": 0.27408607959661446,
      "grad_norm": 0.30019286274909973,
      "learning_rate": 2.7324265767934523e-05,
      "loss": 1.3178,
      "step": 761
    },
    {
      "epoch": 0.2744462452728255,
      "grad_norm": 0.29161977767944336,
      "learning_rate": 2.7320654790563312e-05,
      "loss": 1.3121,
      "step": 762
    },
    {
      "epoch": 0.2748064109490366,
      "grad_norm": 0.29570233821868896,
      "learning_rate": 2.7317043813192104e-05,
      "loss": 1.3512,
      "step": 763
    },
    {
      "epoch": 0.2751665766252476,
      "grad_norm": 0.2932857871055603,
      "learning_rate": 2.7313432835820897e-05,
      "loss": 1.2716,
      "step": 764
    },
    {
      "epoch": 0.2755267423014587,
      "grad_norm": 0.3036288321018219,
      "learning_rate": 2.7309821858449686e-05,
      "loss": 1.3624,
      "step": 765
    },
    {
      "epoch": 0.27588690797766974,
      "grad_norm": 0.29658886790275574,
      "learning_rate": 2.730621088107848e-05,
      "loss": 1.3294,
      "step": 766
    },
    {
      "epoch": 0.2762470736538808,
      "grad_norm": 0.31479474902153015,
      "learning_rate": 2.730259990370727e-05,
      "loss": 1.3642,
      "step": 767
    },
    {
      "epoch": 0.27660723933009185,
      "grad_norm": 0.3105478584766388,
      "learning_rate": 2.7298988926336063e-05,
      "loss": 1.403,
      "step": 768
    },
    {
      "epoch": 0.2769674050063029,
      "grad_norm": 0.28929999470710754,
      "learning_rate": 2.7295377948964855e-05,
      "loss": 1.2699,
      "step": 769
    },
    {
      "epoch": 0.27732757068251396,
      "grad_norm": 0.2887727618217468,
      "learning_rate": 2.7291766971593644e-05,
      "loss": 1.1925,
      "step": 770
    },
    {
      "epoch": 0.277687736358725,
      "grad_norm": 0.31517356634140015,
      "learning_rate": 2.7288155994222436e-05,
      "loss": 1.5191,
      "step": 771
    },
    {
      "epoch": 0.27804790203493607,
      "grad_norm": 0.29549461603164673,
      "learning_rate": 2.728454501685123e-05,
      "loss": 1.3865,
      "step": 772
    },
    {
      "epoch": 0.2784080677111471,
      "grad_norm": 0.30007222294807434,
      "learning_rate": 2.728093403948002e-05,
      "loss": 1.2505,
      "step": 773
    },
    {
      "epoch": 0.2787682333873582,
      "grad_norm": 0.3023131489753723,
      "learning_rate": 2.7277323062108813e-05,
      "loss": 1.3677,
      "step": 774
    },
    {
      "epoch": 0.27912839906356923,
      "grad_norm": 0.30216139554977417,
      "learning_rate": 2.7273712084737602e-05,
      "loss": 1.3674,
      "step": 775
    },
    {
      "epoch": 0.2794885647397803,
      "grad_norm": 0.30045461654663086,
      "learning_rate": 2.7270101107366394e-05,
      "loss": 1.386,
      "step": 776
    },
    {
      "epoch": 0.27984873041599134,
      "grad_norm": 0.2935870885848999,
      "learning_rate": 2.7266490129995186e-05,
      "loss": 1.3792,
      "step": 777
    },
    {
      "epoch": 0.2802088960922024,
      "grad_norm": 0.296747088432312,
      "learning_rate": 2.7262879152623975e-05,
      "loss": 1.3027,
      "step": 778
    },
    {
      "epoch": 0.28056906176841345,
      "grad_norm": 0.29942581057548523,
      "learning_rate": 2.725926817525277e-05,
      "loss": 1.3468,
      "step": 779
    },
    {
      "epoch": 0.2809292274446245,
      "grad_norm": 0.29300689697265625,
      "learning_rate": 2.725565719788156e-05,
      "loss": 1.4126,
      "step": 780
    },
    {
      "epoch": 0.28128939312083556,
      "grad_norm": 0.289156436920166,
      "learning_rate": 2.7252046220510352e-05,
      "loss": 1.3263,
      "step": 781
    },
    {
      "epoch": 0.2816495587970466,
      "grad_norm": 0.3054799437522888,
      "learning_rate": 2.7248435243139145e-05,
      "loss": 1.3477,
      "step": 782
    },
    {
      "epoch": 0.2820097244732577,
      "grad_norm": 0.2915526330471039,
      "learning_rate": 2.7244824265767933e-05,
      "loss": 1.2736,
      "step": 783
    },
    {
      "epoch": 0.2823698901494688,
      "grad_norm": 0.30573466420173645,
      "learning_rate": 2.7241213288396726e-05,
      "loss": 1.2821,
      "step": 784
    },
    {
      "epoch": 0.28273005582567984,
      "grad_norm": 0.2955000698566437,
      "learning_rate": 2.7237602311025518e-05,
      "loss": 1.3206,
      "step": 785
    },
    {
      "epoch": 0.2830902215018909,
      "grad_norm": 0.3001445233821869,
      "learning_rate": 2.723399133365431e-05,
      "loss": 1.3462,
      "step": 786
    },
    {
      "epoch": 0.28345038717810195,
      "grad_norm": 0.308866947889328,
      "learning_rate": 2.7230380356283103e-05,
      "loss": 1.3426,
      "step": 787
    },
    {
      "epoch": 0.283810552854313,
      "grad_norm": 0.3022698760032654,
      "learning_rate": 2.722676937891189e-05,
      "loss": 1.4264,
      "step": 788
    },
    {
      "epoch": 0.28417071853052406,
      "grad_norm": 0.284504234790802,
      "learning_rate": 2.7223158401540684e-05,
      "loss": 1.4141,
      "step": 789
    },
    {
      "epoch": 0.2845308842067351,
      "grad_norm": 0.28948432207107544,
      "learning_rate": 2.7219547424169476e-05,
      "loss": 1.3431,
      "step": 790
    },
    {
      "epoch": 0.28489104988294617,
      "grad_norm": 0.2975788414478302,
      "learning_rate": 2.7215936446798265e-05,
      "loss": 1.4197,
      "step": 791
    },
    {
      "epoch": 0.2852512155591572,
      "grad_norm": 0.30045607686042786,
      "learning_rate": 2.7212325469427057e-05,
      "loss": 1.4011,
      "step": 792
    },
    {
      "epoch": 0.2856113812353683,
      "grad_norm": 0.3057326674461365,
      "learning_rate": 2.7208714492055853e-05,
      "loss": 1.3074,
      "step": 793
    },
    {
      "epoch": 0.28597154691157933,
      "grad_norm": 0.3123806118965149,
      "learning_rate": 2.7205103514684642e-05,
      "loss": 1.2958,
      "step": 794
    },
    {
      "epoch": 0.2863317125877904,
      "grad_norm": 0.31058943271636963,
      "learning_rate": 2.7201492537313434e-05,
      "loss": 1.3095,
      "step": 795
    },
    {
      "epoch": 0.28669187826400144,
      "grad_norm": 0.3163312077522278,
      "learning_rate": 2.7197881559942223e-05,
      "loss": 1.3042,
      "step": 796
    },
    {
      "epoch": 0.2870520439402125,
      "grad_norm": 0.30693790316581726,
      "learning_rate": 2.7194270582571015e-05,
      "loss": 1.3051,
      "step": 797
    },
    {
      "epoch": 0.28741220961642355,
      "grad_norm": 0.3174930512905121,
      "learning_rate": 2.7190659605199808e-05,
      "loss": 1.3504,
      "step": 798
    },
    {
      "epoch": 0.2877723752926346,
      "grad_norm": 0.312980055809021,
      "learning_rate": 2.71870486278286e-05,
      "loss": 1.3146,
      "step": 799
    },
    {
      "epoch": 0.28813254096884566,
      "grad_norm": 0.31713050603866577,
      "learning_rate": 2.7183437650457392e-05,
      "loss": 1.2888,
      "step": 800
    },
    {
      "epoch": 0.2884927066450567,
      "grad_norm": 0.3020930886268616,
      "learning_rate": 2.7179826673086185e-05,
      "loss": 1.2531,
      "step": 801
    },
    {
      "epoch": 0.2888528723212678,
      "grad_norm": 0.3005390465259552,
      "learning_rate": 2.7176215695714974e-05,
      "loss": 1.4581,
      "step": 802
    },
    {
      "epoch": 0.28921303799747883,
      "grad_norm": 0.3115476071834564,
      "learning_rate": 2.7172604718343766e-05,
      "loss": 1.3119,
      "step": 803
    },
    {
      "epoch": 0.2895732036736899,
      "grad_norm": 0.3093145787715912,
      "learning_rate": 2.7168993740972555e-05,
      "loss": 1.3413,
      "step": 804
    },
    {
      "epoch": 0.28993336934990094,
      "grad_norm": 0.2975856363773346,
      "learning_rate": 2.7165382763601347e-05,
      "loss": 1.239,
      "step": 805
    },
    {
      "epoch": 0.290293535026112,
      "grad_norm": 0.3128691613674164,
      "learning_rate": 2.7161771786230143e-05,
      "loss": 1.2641,
      "step": 806
    },
    {
      "epoch": 0.29065370070232305,
      "grad_norm": 0.31466278433799744,
      "learning_rate": 2.715816080885893e-05,
      "loss": 1.2716,
      "step": 807
    },
    {
      "epoch": 0.2910138663785341,
      "grad_norm": 0.299411416053772,
      "learning_rate": 2.7154549831487724e-05,
      "loss": 1.4087,
      "step": 808
    },
    {
      "epoch": 0.29137403205474516,
      "grad_norm": 0.29967018961906433,
      "learning_rate": 2.7150938854116516e-05,
      "loss": 1.3056,
      "step": 809
    },
    {
      "epoch": 0.2917341977309562,
      "grad_norm": 0.31269508600234985,
      "learning_rate": 2.7147327876745305e-05,
      "loss": 1.44,
      "step": 810
    },
    {
      "epoch": 0.2920943634071673,
      "grad_norm": 0.29555895924568176,
      "learning_rate": 2.7143716899374097e-05,
      "loss": 1.2946,
      "step": 811
    },
    {
      "epoch": 0.2924545290833784,
      "grad_norm": 0.29735249280929565,
      "learning_rate": 2.7140105922002886e-05,
      "loss": 1.2737,
      "step": 812
    },
    {
      "epoch": 0.29281469475958943,
      "grad_norm": 0.3123103380203247,
      "learning_rate": 2.7136494944631682e-05,
      "loss": 1.3372,
      "step": 813
    },
    {
      "epoch": 0.2931748604358005,
      "grad_norm": 0.3104854226112366,
      "learning_rate": 2.7132883967260474e-05,
      "loss": 1.3488,
      "step": 814
    },
    {
      "epoch": 0.29353502611201154,
      "grad_norm": 0.2862030565738678,
      "learning_rate": 2.7129272989889263e-05,
      "loss": 1.3553,
      "step": 815
    },
    {
      "epoch": 0.2938951917882226,
      "grad_norm": 0.29660192131996155,
      "learning_rate": 2.7125662012518056e-05,
      "loss": 1.219,
      "step": 816
    },
    {
      "epoch": 0.29425535746443365,
      "grad_norm": 0.3046116828918457,
      "learning_rate": 2.7122051035146848e-05,
      "loss": 1.2054,
      "step": 817
    },
    {
      "epoch": 0.2946155231406447,
      "grad_norm": 0.29021742939949036,
      "learning_rate": 2.7118440057775637e-05,
      "loss": 1.2827,
      "step": 818
    },
    {
      "epoch": 0.29497568881685576,
      "grad_norm": 0.2942574918270111,
      "learning_rate": 2.711482908040443e-05,
      "loss": 1.3204,
      "step": 819
    },
    {
      "epoch": 0.2953358544930668,
      "grad_norm": 0.2938849925994873,
      "learning_rate": 2.711121810303322e-05,
      "loss": 1.4103,
      "step": 820
    },
    {
      "epoch": 0.2956960201692779,
      "grad_norm": 0.2899019420146942,
      "learning_rate": 2.7107607125662014e-05,
      "loss": 1.2183,
      "step": 821
    },
    {
      "epoch": 0.29605618584548893,
      "grad_norm": 0.30210208892822266,
      "learning_rate": 2.7103996148290806e-05,
      "loss": 1.2336,
      "step": 822
    },
    {
      "epoch": 0.2964163515217,
      "grad_norm": 0.3108983635902405,
      "learning_rate": 2.7100385170919595e-05,
      "loss": 1.4042,
      "step": 823
    },
    {
      "epoch": 0.29677651719791104,
      "grad_norm": 0.2926686704158783,
      "learning_rate": 2.7096774193548387e-05,
      "loss": 1.3129,
      "step": 824
    },
    {
      "epoch": 0.2971366828741221,
      "grad_norm": 0.3167264759540558,
      "learning_rate": 2.709316321617718e-05,
      "loss": 1.3628,
      "step": 825
    },
    {
      "epoch": 0.29749684855033315,
      "grad_norm": 0.299449622631073,
      "learning_rate": 2.7089552238805972e-05,
      "loss": 1.2887,
      "step": 826
    },
    {
      "epoch": 0.2978570142265442,
      "grad_norm": 0.31694546341896057,
      "learning_rate": 2.7085941261434764e-05,
      "loss": 1.5351,
      "step": 827
    },
    {
      "epoch": 0.29821717990275526,
      "grad_norm": 0.314327210187912,
      "learning_rate": 2.7082330284063553e-05,
      "loss": 1.2555,
      "step": 828
    },
    {
      "epoch": 0.2985773455789663,
      "grad_norm": 0.3094409108161926,
      "learning_rate": 2.7078719306692345e-05,
      "loss": 1.2951,
      "step": 829
    },
    {
      "epoch": 0.29893751125517737,
      "grad_norm": 0.31680428981781006,
      "learning_rate": 2.7075108329321138e-05,
      "loss": 1.3861,
      "step": 830
    },
    {
      "epoch": 0.2992976769313884,
      "grad_norm": 0.29598188400268555,
      "learning_rate": 2.7071497351949926e-05,
      "loss": 1.305,
      "step": 831
    },
    {
      "epoch": 0.2996578426075995,
      "grad_norm": 0.3141520023345947,
      "learning_rate": 2.706788637457872e-05,
      "loss": 1.3834,
      "step": 832
    },
    {
      "epoch": 0.30001800828381053,
      "grad_norm": 0.30583614110946655,
      "learning_rate": 2.7064275397207514e-05,
      "loss": 1.3251,
      "step": 833
    },
    {
      "epoch": 0.3003781739600216,
      "grad_norm": 0.3156920373439789,
      "learning_rate": 2.7060664419836303e-05,
      "loss": 1.5712,
      "step": 834
    },
    {
      "epoch": 0.30073833963623264,
      "grad_norm": 0.31473779678344727,
      "learning_rate": 2.7057053442465096e-05,
      "loss": 1.3655,
      "step": 835
    },
    {
      "epoch": 0.3010985053124437,
      "grad_norm": 0.30319926142692566,
      "learning_rate": 2.7053442465093885e-05,
      "loss": 1.467,
      "step": 836
    },
    {
      "epoch": 0.3014586709886548,
      "grad_norm": 0.3116908073425293,
      "learning_rate": 2.7049831487722677e-05,
      "loss": 1.4364,
      "step": 837
    },
    {
      "epoch": 0.30181883666486586,
      "grad_norm": 0.3178795874118805,
      "learning_rate": 2.704622051035147e-05,
      "loss": 1.3415,
      "step": 838
    },
    {
      "epoch": 0.3021790023410769,
      "grad_norm": 0.31440407037734985,
      "learning_rate": 2.7042609532980258e-05,
      "loss": 1.4132,
      "step": 839
    },
    {
      "epoch": 0.302539168017288,
      "grad_norm": 0.2996325194835663,
      "learning_rate": 2.7038998555609054e-05,
      "loss": 1.3845,
      "step": 840
    },
    {
      "epoch": 0.30289933369349903,
      "grad_norm": 0.31872066855430603,
      "learning_rate": 2.7035387578237846e-05,
      "loss": 1.353,
      "step": 841
    },
    {
      "epoch": 0.3032594993697101,
      "grad_norm": 0.306119829416275,
      "learning_rate": 2.7031776600866635e-05,
      "loss": 1.3437,
      "step": 842
    },
    {
      "epoch": 0.30361966504592114,
      "grad_norm": 0.29605838656425476,
      "learning_rate": 2.7028165623495427e-05,
      "loss": 1.2881,
      "step": 843
    },
    {
      "epoch": 0.3039798307221322,
      "grad_norm": 0.3145986795425415,
      "learning_rate": 2.7024554646124216e-05,
      "loss": 1.4248,
      "step": 844
    },
    {
      "epoch": 0.30433999639834325,
      "grad_norm": 0.31091734766960144,
      "learning_rate": 2.702094366875301e-05,
      "loss": 1.3835,
      "step": 845
    },
    {
      "epoch": 0.3047001620745543,
      "grad_norm": 0.2993714511394501,
      "learning_rate": 2.70173326913818e-05,
      "loss": 1.3289,
      "step": 846
    },
    {
      "epoch": 0.30506032775076536,
      "grad_norm": 0.31377530097961426,
      "learning_rate": 2.7013721714010593e-05,
      "loss": 1.2797,
      "step": 847
    },
    {
      "epoch": 0.3054204934269764,
      "grad_norm": 0.30012622475624084,
      "learning_rate": 2.7010110736639385e-05,
      "loss": 1.3208,
      "step": 848
    },
    {
      "epoch": 0.30578065910318747,
      "grad_norm": 0.3055904805660248,
      "learning_rate": 2.7006499759268178e-05,
      "loss": 1.4054,
      "step": 849
    },
    {
      "epoch": 0.3061408247793985,
      "grad_norm": 0.308337539434433,
      "learning_rate": 2.7002888781896967e-05,
      "loss": 1.3366,
      "step": 850
    },
    {
      "epoch": 0.3065009904556096,
      "grad_norm": 0.3261684477329254,
      "learning_rate": 2.699927780452576e-05,
      "loss": 1.2857,
      "step": 851
    },
    {
      "epoch": 0.30686115613182063,
      "grad_norm": 0.30796313285827637,
      "learning_rate": 2.6995666827154548e-05,
      "loss": 1.4106,
      "step": 852
    },
    {
      "epoch": 0.3072213218080317,
      "grad_norm": 0.3146258592605591,
      "learning_rate": 2.6992055849783343e-05,
      "loss": 1.4047,
      "step": 853
    },
    {
      "epoch": 0.30758148748424274,
      "grad_norm": 0.30207306146621704,
      "learning_rate": 2.6988444872412136e-05,
      "loss": 1.3165,
      "step": 854
    },
    {
      "epoch": 0.3079416531604538,
      "grad_norm": 0.29776132106781006,
      "learning_rate": 2.6984833895040925e-05,
      "loss": 1.1338,
      "step": 855
    },
    {
      "epoch": 0.30830181883666485,
      "grad_norm": 0.31295934319496155,
      "learning_rate": 2.6981222917669717e-05,
      "loss": 1.3031,
      "step": 856
    },
    {
      "epoch": 0.3086619845128759,
      "grad_norm": 0.30442050099372864,
      "learning_rate": 2.697761194029851e-05,
      "loss": 1.2333,
      "step": 857
    },
    {
      "epoch": 0.30902215018908696,
      "grad_norm": 0.30212146043777466,
      "learning_rate": 2.6974000962927298e-05,
      "loss": 1.413,
      "step": 858
    },
    {
      "epoch": 0.309382315865298,
      "grad_norm": 0.3010075092315674,
      "learning_rate": 2.697038998555609e-05,
      "loss": 1.3868,
      "step": 859
    },
    {
      "epoch": 0.3097424815415091,
      "grad_norm": 0.2936052083969116,
      "learning_rate": 2.6966779008184883e-05,
      "loss": 1.2663,
      "step": 860
    },
    {
      "epoch": 0.31010264721772013,
      "grad_norm": 0.31045159697532654,
      "learning_rate": 2.6963168030813675e-05,
      "loss": 1.3769,
      "step": 861
    },
    {
      "epoch": 0.3104628128939312,
      "grad_norm": 0.3074168264865875,
      "learning_rate": 2.6959557053442467e-05,
      "loss": 1.2461,
      "step": 862
    },
    {
      "epoch": 0.31082297857014224,
      "grad_norm": 0.30557069182395935,
      "learning_rate": 2.6955946076071256e-05,
      "loss": 1.2293,
      "step": 863
    },
    {
      "epoch": 0.31118314424635335,
      "grad_norm": 0.29654955863952637,
      "learning_rate": 2.695233509870005e-05,
      "loss": 1.2082,
      "step": 864
    },
    {
      "epoch": 0.3115433099225644,
      "grad_norm": 0.3154890835285187,
      "learning_rate": 2.694872412132884e-05,
      "loss": 1.2606,
      "step": 865
    },
    {
      "epoch": 0.31190347559877546,
      "grad_norm": 0.3139643669128418,
      "learning_rate": 2.694511314395763e-05,
      "loss": 1.303,
      "step": 866
    },
    {
      "epoch": 0.3122636412749865,
      "grad_norm": 0.29728275537490845,
      "learning_rate": 2.6941502166586425e-05,
      "loss": 1.3532,
      "step": 867
    },
    {
      "epoch": 0.31262380695119757,
      "grad_norm": 0.3083034157752991,
      "learning_rate": 2.6937891189215214e-05,
      "loss": 1.3107,
      "step": 868
    },
    {
      "epoch": 0.3129839726274086,
      "grad_norm": 0.31866714358329773,
      "learning_rate": 2.6934280211844007e-05,
      "loss": 1.1914,
      "step": 869
    },
    {
      "epoch": 0.3133441383036197,
      "grad_norm": 0.3218574821949005,
      "learning_rate": 2.69306692344728e-05,
      "loss": 1.3809,
      "step": 870
    },
    {
      "epoch": 0.31370430397983073,
      "grad_norm": 0.3050767779350281,
      "learning_rate": 2.6927058257101588e-05,
      "loss": 1.315,
      "step": 871
    },
    {
      "epoch": 0.3140644696560418,
      "grad_norm": 0.3264881372451782,
      "learning_rate": 2.692344727973038e-05,
      "loss": 1.5638,
      "step": 872
    },
    {
      "epoch": 0.31442463533225284,
      "grad_norm": 0.2949087917804718,
      "learning_rate": 2.6919836302359172e-05,
      "loss": 1.2802,
      "step": 873
    },
    {
      "epoch": 0.3147848010084639,
      "grad_norm": 0.3105723261833191,
      "learning_rate": 2.6916225324987965e-05,
      "loss": 1.2696,
      "step": 874
    },
    {
      "epoch": 0.31514496668467495,
      "grad_norm": 0.3223666250705719,
      "learning_rate": 2.6912614347616757e-05,
      "loss": 1.3555,
      "step": 875
    },
    {
      "epoch": 0.315505132360886,
      "grad_norm": 0.31693366169929504,
      "learning_rate": 2.6909003370245546e-05,
      "loss": 1.3368,
      "step": 876
    },
    {
      "epoch": 0.31586529803709706,
      "grad_norm": 0.3169599771499634,
      "learning_rate": 2.6905392392874338e-05,
      "loss": 1.2335,
      "step": 877
    },
    {
      "epoch": 0.3162254637133081,
      "grad_norm": 0.32307168841362,
      "learning_rate": 2.690178141550313e-05,
      "loss": 1.262,
      "step": 878
    },
    {
      "epoch": 0.3165856293895192,
      "grad_norm": 0.306305468082428,
      "learning_rate": 2.689817043813192e-05,
      "loss": 1.3186,
      "step": 879
    },
    {
      "epoch": 0.31694579506573023,
      "grad_norm": 0.29760023951530457,
      "learning_rate": 2.6894559460760715e-05,
      "loss": 1.2336,
      "step": 880
    },
    {
      "epoch": 0.3173059607419413,
      "grad_norm": 0.3064510226249695,
      "learning_rate": 2.6890948483389507e-05,
      "loss": 1.3216,
      "step": 881
    },
    {
      "epoch": 0.31766612641815234,
      "grad_norm": 0.32078900933265686,
      "learning_rate": 2.6887337506018296e-05,
      "loss": 1.4859,
      "step": 882
    },
    {
      "epoch": 0.3180262920943634,
      "grad_norm": 0.30442747473716736,
      "learning_rate": 2.688372652864709e-05,
      "loss": 1.2581,
      "step": 883
    },
    {
      "epoch": 0.31838645777057445,
      "grad_norm": 0.30862149596214294,
      "learning_rate": 2.6880115551275877e-05,
      "loss": 1.3397,
      "step": 884
    },
    {
      "epoch": 0.3187466234467855,
      "grad_norm": 0.3095163106918335,
      "learning_rate": 2.687650457390467e-05,
      "loss": 1.3573,
      "step": 885
    },
    {
      "epoch": 0.31910678912299656,
      "grad_norm": 0.3094938397407532,
      "learning_rate": 2.6872893596533462e-05,
      "loss": 1.4428,
      "step": 886
    },
    {
      "epoch": 0.3194669547992076,
      "grad_norm": 0.2962958812713623,
      "learning_rate": 2.6869282619162254e-05,
      "loss": 1.2425,
      "step": 887
    },
    {
      "epoch": 0.31982712047541867,
      "grad_norm": 0.30124998092651367,
      "learning_rate": 2.6865671641791047e-05,
      "loss": 1.2932,
      "step": 888
    },
    {
      "epoch": 0.3201872861516297,
      "grad_norm": 0.3035518229007721,
      "learning_rate": 2.686206066441984e-05,
      "loss": 1.2123,
      "step": 889
    },
    {
      "epoch": 0.3205474518278408,
      "grad_norm": 0.3028961420059204,
      "learning_rate": 2.6858449687048628e-05,
      "loss": 1.3567,
      "step": 890
    },
    {
      "epoch": 0.3209076175040519,
      "grad_norm": 0.2991254925727844,
      "learning_rate": 2.685483870967742e-05,
      "loss": 1.2871,
      "step": 891
    },
    {
      "epoch": 0.32126778318026294,
      "grad_norm": 0.3008140027523041,
      "learning_rate": 2.685122773230621e-05,
      "loss": 1.2683,
      "step": 892
    },
    {
      "epoch": 0.321627948856474,
      "grad_norm": 0.295658677816391,
      "learning_rate": 2.6847616754935e-05,
      "loss": 1.3656,
      "step": 893
    },
    {
      "epoch": 0.32198811453268505,
      "grad_norm": 0.30488190054893494,
      "learning_rate": 2.6844005777563797e-05,
      "loss": 1.408,
      "step": 894
    },
    {
      "epoch": 0.3223482802088961,
      "grad_norm": 0.31063956022262573,
      "learning_rate": 2.6840394800192586e-05,
      "loss": 1.3334,
      "step": 895
    },
    {
      "epoch": 0.32270844588510716,
      "grad_norm": 0.3144553303718567,
      "learning_rate": 2.6836783822821378e-05,
      "loss": 1.2469,
      "step": 896
    },
    {
      "epoch": 0.3230686115613182,
      "grad_norm": 0.29666540026664734,
      "learning_rate": 2.683317284545017e-05,
      "loss": 1.2781,
      "step": 897
    },
    {
      "epoch": 0.3234287772375293,
      "grad_norm": 0.3255404829978943,
      "learning_rate": 2.682956186807896e-05,
      "loss": 1.3355,
      "step": 898
    },
    {
      "epoch": 0.32378894291374033,
      "grad_norm": 0.3393751084804535,
      "learning_rate": 2.6825950890707752e-05,
      "loss": 1.5159,
      "step": 899
    },
    {
      "epoch": 0.3241491085899514,
      "grad_norm": 0.30498406291007996,
      "learning_rate": 2.682233991333654e-05,
      "loss": 1.2158,
      "step": 900
    },
    {
      "epoch": 0.32450927426616244,
      "grad_norm": 0.30974504351615906,
      "learning_rate": 2.6818728935965336e-05,
      "loss": 1.3679,
      "step": 901
    },
    {
      "epoch": 0.3248694399423735,
      "grad_norm": 0.30876481533050537,
      "learning_rate": 2.681511795859413e-05,
      "loss": 1.2575,
      "step": 902
    },
    {
      "epoch": 0.32522960561858455,
      "grad_norm": 0.3196663558483124,
      "learning_rate": 2.6811506981222918e-05,
      "loss": 1.2914,
      "step": 903
    },
    {
      "epoch": 0.3255897712947956,
      "grad_norm": 0.308004766702652,
      "learning_rate": 2.680789600385171e-05,
      "loss": 1.3295,
      "step": 904
    },
    {
      "epoch": 0.32594993697100666,
      "grad_norm": 0.3173050582408905,
      "learning_rate": 2.6804285026480502e-05,
      "loss": 1.3699,
      "step": 905
    },
    {
      "epoch": 0.3263101026472177,
      "grad_norm": 0.3159114420413971,
      "learning_rate": 2.680067404910929e-05,
      "loss": 1.2715,
      "step": 906
    },
    {
      "epoch": 0.32667026832342877,
      "grad_norm": 0.3164500594139099,
      "learning_rate": 2.6797063071738087e-05,
      "loss": 1.3219,
      "step": 907
    },
    {
      "epoch": 0.3270304339996398,
      "grad_norm": 0.31781166791915894,
      "learning_rate": 2.6793452094366876e-05,
      "loss": 1.379,
      "step": 908
    },
    {
      "epoch": 0.3273905996758509,
      "grad_norm": 0.31062597036361694,
      "learning_rate": 2.6789841116995668e-05,
      "loss": 1.2503,
      "step": 909
    },
    {
      "epoch": 0.32775076535206193,
      "grad_norm": 0.306317538022995,
      "learning_rate": 2.678623013962446e-05,
      "loss": 1.2451,
      "step": 910
    },
    {
      "epoch": 0.328110931028273,
      "grad_norm": 0.31638532876968384,
      "learning_rate": 2.678261916225325e-05,
      "loss": 1.4494,
      "step": 911
    },
    {
      "epoch": 0.32847109670448404,
      "grad_norm": 0.3159773349761963,
      "learning_rate": 2.677900818488204e-05,
      "loss": 1.2885,
      "step": 912
    },
    {
      "epoch": 0.3288312623806951,
      "grad_norm": 0.322319895029068,
      "learning_rate": 2.6775397207510834e-05,
      "loss": 1.3238,
      "step": 913
    },
    {
      "epoch": 0.32919142805690615,
      "grad_norm": 0.31149232387542725,
      "learning_rate": 2.6771786230139626e-05,
      "loss": 1.337,
      "step": 914
    },
    {
      "epoch": 0.3295515937331172,
      "grad_norm": 0.307744562625885,
      "learning_rate": 2.676817525276842e-05,
      "loss": 1.3119,
      "step": 915
    },
    {
      "epoch": 0.32991175940932826,
      "grad_norm": 0.31508904695510864,
      "learning_rate": 2.6764564275397207e-05,
      "loss": 1.4516,
      "step": 916
    },
    {
      "epoch": 0.3302719250855394,
      "grad_norm": 0.3035276234149933,
      "learning_rate": 2.6760953298026e-05,
      "loss": 1.2505,
      "step": 917
    },
    {
      "epoch": 0.33063209076175043,
      "grad_norm": 0.30765488743782043,
      "learning_rate": 2.6757342320654792e-05,
      "loss": 1.262,
      "step": 918
    },
    {
      "epoch": 0.3309922564379615,
      "grad_norm": 0.3269232213497162,
      "learning_rate": 2.675373134328358e-05,
      "loss": 1.5401,
      "step": 919
    },
    {
      "epoch": 0.33135242211417254,
      "grad_norm": 0.2966175079345703,
      "learning_rate": 2.6750120365912373e-05,
      "loss": 1.2624,
      "step": 920
    },
    {
      "epoch": 0.3317125877903836,
      "grad_norm": 0.32523658871650696,
      "learning_rate": 2.674650938854117e-05,
      "loss": 1.411,
      "step": 921
    },
    {
      "epoch": 0.33207275346659465,
      "grad_norm": 0.3197718560695648,
      "learning_rate": 2.6742898411169958e-05,
      "loss": 1.2507,
      "step": 922
    },
    {
      "epoch": 0.3324329191428057,
      "grad_norm": 0.29901304841041565,
      "learning_rate": 2.673928743379875e-05,
      "loss": 1.1799,
      "step": 923
    },
    {
      "epoch": 0.33279308481901676,
      "grad_norm": 0.3177932798862457,
      "learning_rate": 2.673567645642754e-05,
      "loss": 1.2621,
      "step": 924
    },
    {
      "epoch": 0.3331532504952278,
      "grad_norm": 0.3188462555408478,
      "learning_rate": 2.673206547905633e-05,
      "loss": 1.4749,
      "step": 925
    },
    {
      "epoch": 0.33351341617143887,
      "grad_norm": 0.2900705933570862,
      "learning_rate": 2.6728454501685123e-05,
      "loss": 1.3781,
      "step": 926
    },
    {
      "epoch": 0.3338735818476499,
      "grad_norm": 0.3128795921802521,
      "learning_rate": 2.6724843524313912e-05,
      "loss": 1.4392,
      "step": 927
    },
    {
      "epoch": 0.334233747523861,
      "grad_norm": 0.32057735323905945,
      "learning_rate": 2.6721232546942708e-05,
      "loss": 1.4971,
      "step": 928
    },
    {
      "epoch": 0.33459391320007204,
      "grad_norm": 0.3132125735282898,
      "learning_rate": 2.67176215695715e-05,
      "loss": 1.3148,
      "step": 929
    },
    {
      "epoch": 0.3349540788762831,
      "grad_norm": 0.33949634432792664,
      "learning_rate": 2.671401059220029e-05,
      "loss": 1.4415,
      "step": 930
    },
    {
      "epoch": 0.33531424455249415,
      "grad_norm": 0.3050568699836731,
      "learning_rate": 2.671039961482908e-05,
      "loss": 1.4613,
      "step": 931
    },
    {
      "epoch": 0.3356744102287052,
      "grad_norm": 0.3221439719200134,
      "learning_rate": 2.670678863745787e-05,
      "loss": 1.3048,
      "step": 932
    },
    {
      "epoch": 0.33603457590491626,
      "grad_norm": 0.30523744225502014,
      "learning_rate": 2.6703177660086663e-05,
      "loss": 1.2929,
      "step": 933
    },
    {
      "epoch": 0.3363947415811273,
      "grad_norm": 0.30980607867240906,
      "learning_rate": 2.669956668271546e-05,
      "loss": 1.2466,
      "step": 934
    },
    {
      "epoch": 0.33675490725733837,
      "grad_norm": 0.31478121876716614,
      "learning_rate": 2.6695955705344247e-05,
      "loss": 1.1373,
      "step": 935
    },
    {
      "epoch": 0.3371150729335494,
      "grad_norm": 0.3292517364025116,
      "learning_rate": 2.669234472797304e-05,
      "loss": 1.4232,
      "step": 936
    },
    {
      "epoch": 0.3374752386097605,
      "grad_norm": 0.31015658378601074,
      "learning_rate": 2.6688733750601832e-05,
      "loss": 1.2664,
      "step": 937
    },
    {
      "epoch": 0.33783540428597153,
      "grad_norm": 0.303130567073822,
      "learning_rate": 2.668512277323062e-05,
      "loss": 1.3003,
      "step": 938
    },
    {
      "epoch": 0.3381955699621826,
      "grad_norm": 0.30301159620285034,
      "learning_rate": 2.6681511795859413e-05,
      "loss": 1.2837,
      "step": 939
    },
    {
      "epoch": 0.33855573563839364,
      "grad_norm": 0.3173447549343109,
      "learning_rate": 2.6677900818488202e-05,
      "loss": 1.3078,
      "step": 940
    },
    {
      "epoch": 0.3389159013146047,
      "grad_norm": 0.32142606377601624,
      "learning_rate": 2.6674289841116998e-05,
      "loss": 1.3461,
      "step": 941
    },
    {
      "epoch": 0.33927606699081575,
      "grad_norm": 0.30919232964515686,
      "learning_rate": 2.667067886374579e-05,
      "loss": 1.4202,
      "step": 942
    },
    {
      "epoch": 0.3396362326670268,
      "grad_norm": 0.3010205030441284,
      "learning_rate": 2.666706788637458e-05,
      "loss": 1.2973,
      "step": 943
    },
    {
      "epoch": 0.3399963983432379,
      "grad_norm": 0.32242071628570557,
      "learning_rate": 2.666345690900337e-05,
      "loss": 1.2964,
      "step": 944
    },
    {
      "epoch": 0.34035656401944897,
      "grad_norm": 0.31086698174476624,
      "learning_rate": 2.6659845931632164e-05,
      "loss": 1.2905,
      "step": 945
    },
    {
      "epoch": 0.34071672969566,
      "grad_norm": 0.3138304054737091,
      "learning_rate": 2.6656234954260952e-05,
      "loss": 1.2956,
      "step": 946
    },
    {
      "epoch": 0.3410768953718711,
      "grad_norm": 0.3104650378227234,
      "learning_rate": 2.6652623976889745e-05,
      "loss": 1.2977,
      "step": 947
    },
    {
      "epoch": 0.34143706104808214,
      "grad_norm": 0.30215486884117126,
      "learning_rate": 2.6649012999518537e-05,
      "loss": 1.2795,
      "step": 948
    },
    {
      "epoch": 0.3417972267242932,
      "grad_norm": 0.29401224851608276,
      "learning_rate": 2.664540202214733e-05,
      "loss": 1.2656,
      "step": 949
    },
    {
      "epoch": 0.34215739240050425,
      "grad_norm": 0.29951968789100647,
      "learning_rate": 2.664179104477612e-05,
      "loss": 1.2942,
      "step": 950
    },
    {
      "epoch": 0.3425175580767153,
      "grad_norm": 0.31482797861099243,
      "learning_rate": 2.663818006740491e-05,
      "loss": 1.3913,
      "step": 951
    },
    {
      "epoch": 0.34287772375292636,
      "grad_norm": 0.2966248393058777,
      "learning_rate": 2.6634569090033703e-05,
      "loss": 1.2696,
      "step": 952
    },
    {
      "epoch": 0.3432378894291374,
      "grad_norm": 0.3395747244358063,
      "learning_rate": 2.6630958112662495e-05,
      "loss": 1.423,
      "step": 953
    },
    {
      "epoch": 0.34359805510534847,
      "grad_norm": 0.3182545304298401,
      "learning_rate": 2.6627347135291287e-05,
      "loss": 1.3545,
      "step": 954
    },
    {
      "epoch": 0.3439582207815595,
      "grad_norm": 0.32524052262306213,
      "learning_rate": 2.662373615792008e-05,
      "loss": 1.2923,
      "step": 955
    },
    {
      "epoch": 0.3443183864577706,
      "grad_norm": 0.32625728845596313,
      "learning_rate": 2.662012518054887e-05,
      "loss": 1.3753,
      "step": 956
    },
    {
      "epoch": 0.34467855213398163,
      "grad_norm": 0.28993508219718933,
      "learning_rate": 2.661651420317766e-05,
      "loss": 1.2081,
      "step": 957
    },
    {
      "epoch": 0.3450387178101927,
      "grad_norm": 0.30709773302078247,
      "learning_rate": 2.6612903225806453e-05,
      "loss": 1.3569,
      "step": 958
    },
    {
      "epoch": 0.34539888348640374,
      "grad_norm": 0.3175033926963806,
      "learning_rate": 2.6609292248435242e-05,
      "loss": 1.3591,
      "step": 959
    },
    {
      "epoch": 0.3457590491626148,
      "grad_norm": 0.3171972334384918,
      "learning_rate": 2.6605681271064034e-05,
      "loss": 1.2343,
      "step": 960
    },
    {
      "epoch": 0.34611921483882585,
      "grad_norm": 0.3242783546447754,
      "learning_rate": 2.6602070293692827e-05,
      "loss": 1.3449,
      "step": 961
    },
    {
      "epoch": 0.3464793805150369,
      "grad_norm": 0.322494775056839,
      "learning_rate": 2.659845931632162e-05,
      "loss": 1.3009,
      "step": 962
    },
    {
      "epoch": 0.34683954619124796,
      "grad_norm": 0.32056286931037903,
      "learning_rate": 2.659484833895041e-05,
      "loss": 1.4055,
      "step": 963
    },
    {
      "epoch": 0.347199711867459,
      "grad_norm": 0.316307932138443,
      "learning_rate": 2.65912373615792e-05,
      "loss": 1.3261,
      "step": 964
    },
    {
      "epoch": 0.34755987754367007,
      "grad_norm": 0.32180389761924744,
      "learning_rate": 2.6587626384207993e-05,
      "loss": 1.325,
      "step": 965
    },
    {
      "epoch": 0.3479200432198811,
      "grad_norm": 0.29972201585769653,
      "learning_rate": 2.6584015406836785e-05,
      "loss": 1.2519,
      "step": 966
    },
    {
      "epoch": 0.3482802088960922,
      "grad_norm": 0.32356128096580505,
      "learning_rate": 2.6580404429465574e-05,
      "loss": 1.3238,
      "step": 967
    },
    {
      "epoch": 0.34864037457230324,
      "grad_norm": 0.3227563202381134,
      "learning_rate": 2.657679345209437e-05,
      "loss": 1.3651,
      "step": 968
    },
    {
      "epoch": 0.3490005402485143,
      "grad_norm": 0.312639981508255,
      "learning_rate": 2.657318247472316e-05,
      "loss": 1.2568,
      "step": 969
    },
    {
      "epoch": 0.3493607059247254,
      "grad_norm": 0.3124435245990753,
      "learning_rate": 2.656957149735195e-05,
      "loss": 1.2801,
      "step": 970
    },
    {
      "epoch": 0.34972087160093646,
      "grad_norm": 0.3123398721218109,
      "learning_rate": 2.6565960519980743e-05,
      "loss": 1.255,
      "step": 971
    },
    {
      "epoch": 0.3500810372771475,
      "grad_norm": 0.31362852454185486,
      "learning_rate": 2.6562349542609532e-05,
      "loss": 1.3141,
      "step": 972
    },
    {
      "epoch": 0.35044120295335857,
      "grad_norm": 0.3098389804363251,
      "learning_rate": 2.6558738565238324e-05,
      "loss": 1.3765,
      "step": 973
    },
    {
      "epoch": 0.3508013686295696,
      "grad_norm": 0.30971962213516235,
      "learning_rate": 2.6555127587867116e-05,
      "loss": 1.2231,
      "step": 974
    },
    {
      "epoch": 0.3511615343057807,
      "grad_norm": 0.32277241349220276,
      "learning_rate": 2.655151661049591e-05,
      "loss": 1.3177,
      "step": 975
    },
    {
      "epoch": 0.35152169998199173,
      "grad_norm": 0.32582104206085205,
      "learning_rate": 2.65479056331247e-05,
      "loss": 1.2759,
      "step": 976
    },
    {
      "epoch": 0.3518818656582028,
      "grad_norm": 0.31561049818992615,
      "learning_rate": 2.654429465575349e-05,
      "loss": 1.2436,
      "step": 977
    },
    {
      "epoch": 0.35224203133441384,
      "grad_norm": 0.325528621673584,
      "learning_rate": 2.6540683678382282e-05,
      "loss": 1.3524,
      "step": 978
    },
    {
      "epoch": 0.3526021970106249,
      "grad_norm": 0.3201073110103607,
      "learning_rate": 2.6537072701011075e-05,
      "loss": 1.2348,
      "step": 979
    },
    {
      "epoch": 0.35296236268683595,
      "grad_norm": 0.3061332106590271,
      "learning_rate": 2.6533461723639863e-05,
      "loss": 1.1475,
      "step": 980
    },
    {
      "epoch": 0.353322528363047,
      "grad_norm": 0.3009202778339386,
      "learning_rate": 2.652985074626866e-05,
      "loss": 1.2157,
      "step": 981
    },
    {
      "epoch": 0.35368269403925806,
      "grad_norm": 0.32668808102607727,
      "learning_rate": 2.652623976889745e-05,
      "loss": 1.3249,
      "step": 982
    },
    {
      "epoch": 0.3540428597154691,
      "grad_norm": 0.3195321559906006,
      "learning_rate": 2.652262879152624e-05,
      "loss": 1.4165,
      "step": 983
    },
    {
      "epoch": 0.35440302539168017,
      "grad_norm": 0.337101548910141,
      "learning_rate": 2.6519017814155033e-05,
      "loss": 1.2609,
      "step": 984
    },
    {
      "epoch": 0.3547631910678912,
      "grad_norm": 0.343722939491272,
      "learning_rate": 2.651540683678382e-05,
      "loss": 1.4088,
      "step": 985
    },
    {
      "epoch": 0.3551233567441023,
      "grad_norm": 0.30952543020248413,
      "learning_rate": 2.6511795859412614e-05,
      "loss": 1.3244,
      "step": 986
    },
    {
      "epoch": 0.35548352242031334,
      "grad_norm": 0.3234108090400696,
      "learning_rate": 2.6508184882041406e-05,
      "loss": 1.2623,
      "step": 987
    },
    {
      "epoch": 0.3558436880965244,
      "grad_norm": 0.3168228268623352,
      "learning_rate": 2.65045739046702e-05,
      "loss": 1.3035,
      "step": 988
    },
    {
      "epoch": 0.35620385377273545,
      "grad_norm": 0.31394335627555847,
      "learning_rate": 2.650096292729899e-05,
      "loss": 1.3302,
      "step": 989
    },
    {
      "epoch": 0.3565640194489465,
      "grad_norm": 0.32064470648765564,
      "learning_rate": 2.6497351949927783e-05,
      "loss": 1.2864,
      "step": 990
    },
    {
      "epoch": 0.35692418512515756,
      "grad_norm": 0.3328489065170288,
      "learning_rate": 2.6493740972556572e-05,
      "loss": 1.2611,
      "step": 991
    },
    {
      "epoch": 0.3572843508013686,
      "grad_norm": 0.32113415002822876,
      "learning_rate": 2.6490129995185364e-05,
      "loss": 1.2012,
      "step": 992
    },
    {
      "epoch": 0.35764451647757967,
      "grad_norm": 0.33228108286857605,
      "learning_rate": 2.6486519017814153e-05,
      "loss": 1.3122,
      "step": 993
    },
    {
      "epoch": 0.3580046821537907,
      "grad_norm": 0.3213750422000885,
      "learning_rate": 2.6482908040442945e-05,
      "loss": 1.3151,
      "step": 994
    },
    {
      "epoch": 0.3583648478300018,
      "grad_norm": 0.3232221007347107,
      "learning_rate": 2.647929706307174e-05,
      "loss": 1.3351,
      "step": 995
    },
    {
      "epoch": 0.35872501350621283,
      "grad_norm": 0.33038169145584106,
      "learning_rate": 2.647568608570053e-05,
      "loss": 1.1597,
      "step": 996
    },
    {
      "epoch": 0.35908517918242394,
      "grad_norm": 0.3305199146270752,
      "learning_rate": 2.6472075108329322e-05,
      "loss": 1.4183,
      "step": 997
    },
    {
      "epoch": 0.359445344858635,
      "grad_norm": 0.32078200578689575,
      "learning_rate": 2.6468464130958115e-05,
      "loss": 1.2767,
      "step": 998
    },
    {
      "epoch": 0.35980551053484605,
      "grad_norm": 0.3095801770687103,
      "learning_rate": 2.6464853153586903e-05,
      "loss": 1.3482,
      "step": 999
    },
    {
      "epoch": 0.3601656762110571,
      "grad_norm": 0.3232813775539398,
      "learning_rate": 2.6461242176215696e-05,
      "loss": 1.194,
      "step": 1000
    },
    {
      "epoch": 0.36052584188726816,
      "grad_norm": 0.31496870517730713,
      "learning_rate": 2.6457631198844485e-05,
      "loss": 1.2378,
      "step": 1001
    },
    {
      "epoch": 0.3608860075634792,
      "grad_norm": 0.34714192152023315,
      "learning_rate": 2.645402022147328e-05,
      "loss": 1.196,
      "step": 1002
    },
    {
      "epoch": 0.36124617323969027,
      "grad_norm": 0.3028219938278198,
      "learning_rate": 2.6450409244102073e-05,
      "loss": 1.2866,
      "step": 1003
    },
    {
      "epoch": 0.3616063389159013,
      "grad_norm": 0.31296873092651367,
      "learning_rate": 2.644679826673086e-05,
      "loss": 1.1696,
      "step": 1004
    },
    {
      "epoch": 0.3619665045921124,
      "grad_norm": 0.31530827283859253,
      "learning_rate": 2.6443187289359654e-05,
      "loss": 1.1922,
      "step": 1005
    },
    {
      "epoch": 0.36232667026832344,
      "grad_norm": 0.3235622048377991,
      "learning_rate": 2.6439576311988446e-05,
      "loss": 1.3981,
      "step": 1006
    },
    {
      "epoch": 0.3626868359445345,
      "grad_norm": 0.3193400204181671,
      "learning_rate": 2.6435965334617235e-05,
      "loss": 1.3177,
      "step": 1007
    },
    {
      "epoch": 0.36304700162074555,
      "grad_norm": 0.311530202627182,
      "learning_rate": 2.643235435724603e-05,
      "loss": 1.2925,
      "step": 1008
    },
    {
      "epoch": 0.3634071672969566,
      "grad_norm": 0.30382728576660156,
      "learning_rate": 2.642874337987482e-05,
      "loss": 1.3224,
      "step": 1009
    },
    {
      "epoch": 0.36376733297316766,
      "grad_norm": 0.33904191851615906,
      "learning_rate": 2.6425132402503612e-05,
      "loss": 1.4595,
      "step": 1010
    },
    {
      "epoch": 0.3641274986493787,
      "grad_norm": 0.3159455955028534,
      "learning_rate": 2.6421521425132404e-05,
      "loss": 1.1942,
      "step": 1011
    },
    {
      "epoch": 0.36448766432558977,
      "grad_norm": 0.32319802045822144,
      "learning_rate": 2.6417910447761193e-05,
      "loss": 1.3285,
      "step": 1012
    },
    {
      "epoch": 0.3648478300018008,
      "grad_norm": 0.3146122992038727,
      "learning_rate": 2.6414299470389985e-05,
      "loss": 1.3348,
      "step": 1013
    },
    {
      "epoch": 0.3652079956780119,
      "grad_norm": 0.3071313202381134,
      "learning_rate": 2.6410688493018778e-05,
      "loss": 1.2841,
      "step": 1014
    },
    {
      "epoch": 0.36556816135422293,
      "grad_norm": 0.3193351924419403,
      "learning_rate": 2.640707751564757e-05,
      "loss": 1.4023,
      "step": 1015
    },
    {
      "epoch": 0.365928327030434,
      "grad_norm": 0.32430875301361084,
      "learning_rate": 2.6403466538276362e-05,
      "loss": 1.2979,
      "step": 1016
    },
    {
      "epoch": 0.36628849270664504,
      "grad_norm": 0.3204919099807739,
      "learning_rate": 2.639985556090515e-05,
      "loss": 1.2644,
      "step": 1017
    },
    {
      "epoch": 0.3666486583828561,
      "grad_norm": 0.3118840157985687,
      "learning_rate": 2.6396244583533944e-05,
      "loss": 1.3126,
      "step": 1018
    },
    {
      "epoch": 0.36700882405906715,
      "grad_norm": 0.3150581419467926,
      "learning_rate": 2.6392633606162736e-05,
      "loss": 1.213,
      "step": 1019
    },
    {
      "epoch": 0.3673689897352782,
      "grad_norm": 0.3087487518787384,
      "learning_rate": 2.6389022628791525e-05,
      "loss": 1.4154,
      "step": 1020
    },
    {
      "epoch": 0.36772915541148926,
      "grad_norm": 0.30920422077178955,
      "learning_rate": 2.6385411651420317e-05,
      "loss": 1.313,
      "step": 1021
    },
    {
      "epoch": 0.3680893210877003,
      "grad_norm": 0.3220742642879486,
      "learning_rate": 2.6381800674049113e-05,
      "loss": 1.3464,
      "step": 1022
    },
    {
      "epoch": 0.3684494867639114,
      "grad_norm": 0.33471646904945374,
      "learning_rate": 2.63781896966779e-05,
      "loss": 1.1627,
      "step": 1023
    },
    {
      "epoch": 0.3688096524401225,
      "grad_norm": 0.3083063066005707,
      "learning_rate": 2.6374578719306694e-05,
      "loss": 1.2276,
      "step": 1024
    },
    {
      "epoch": 0.36916981811633354,
      "grad_norm": 0.33272886276245117,
      "learning_rate": 2.6370967741935483e-05,
      "loss": 1.3353,
      "step": 1025
    },
    {
      "epoch": 0.3695299837925446,
      "grad_norm": 0.3151063323020935,
      "learning_rate": 2.6367356764564275e-05,
      "loss": 1.305,
      "step": 1026
    },
    {
      "epoch": 0.36989014946875565,
      "grad_norm": 0.32022905349731445,
      "learning_rate": 2.6363745787193067e-05,
      "loss": 1.2244,
      "step": 1027
    },
    {
      "epoch": 0.3702503151449667,
      "grad_norm": 0.3290402591228485,
      "learning_rate": 2.6360134809821856e-05,
      "loss": 1.2609,
      "step": 1028
    },
    {
      "epoch": 0.37061048082117776,
      "grad_norm": 0.3305075168609619,
      "learning_rate": 2.6356523832450652e-05,
      "loss": 1.3597,
      "step": 1029
    },
    {
      "epoch": 0.3709706464973888,
      "grad_norm": 0.3017394542694092,
      "learning_rate": 2.6352912855079444e-05,
      "loss": 1.2857,
      "step": 1030
    },
    {
      "epoch": 0.37133081217359987,
      "grad_norm": 0.30015239119529724,
      "learning_rate": 2.6349301877708233e-05,
      "loss": 1.3127,
      "step": 1031
    },
    {
      "epoch": 0.3716909778498109,
      "grad_norm": 0.32709938287734985,
      "learning_rate": 2.6345690900337026e-05,
      "loss": 1.2731,
      "step": 1032
    },
    {
      "epoch": 0.372051143526022,
      "grad_norm": 0.34332117438316345,
      "learning_rate": 2.6342079922965814e-05,
      "loss": 1.4731,
      "step": 1033
    },
    {
      "epoch": 0.37241130920223303,
      "grad_norm": 0.30965837836265564,
      "learning_rate": 2.6338468945594607e-05,
      "loss": 1.2726,
      "step": 1034
    },
    {
      "epoch": 0.3727714748784441,
      "grad_norm": 0.3423227369785309,
      "learning_rate": 2.6334857968223402e-05,
      "loss": 1.4046,
      "step": 1035
    },
    {
      "epoch": 0.37313164055465514,
      "grad_norm": 0.30307453870773315,
      "learning_rate": 2.633124699085219e-05,
      "loss": 1.2653,
      "step": 1036
    },
    {
      "epoch": 0.3734918062308662,
      "grad_norm": 0.3335278630256653,
      "learning_rate": 2.6327636013480984e-05,
      "loss": 1.4441,
      "step": 1037
    },
    {
      "epoch": 0.37385197190707725,
      "grad_norm": 0.33244213461875916,
      "learning_rate": 2.6324025036109776e-05,
      "loss": 1.3037,
      "step": 1038
    },
    {
      "epoch": 0.3742121375832883,
      "grad_norm": 0.3097994327545166,
      "learning_rate": 2.6320414058738565e-05,
      "loss": 1.3594,
      "step": 1039
    },
    {
      "epoch": 0.37457230325949936,
      "grad_norm": 0.3376712203025818,
      "learning_rate": 2.6316803081367357e-05,
      "loss": 1.2877,
      "step": 1040
    },
    {
      "epoch": 0.3749324689357104,
      "grad_norm": 0.3076876699924469,
      "learning_rate": 2.6313192103996146e-05,
      "loss": 1.344,
      "step": 1041
    },
    {
      "epoch": 0.37529263461192147,
      "grad_norm": 0.3254151940345764,
      "learning_rate": 2.6309581126624942e-05,
      "loss": 1.3891,
      "step": 1042
    },
    {
      "epoch": 0.3756528002881325,
      "grad_norm": 0.3171617090702057,
      "learning_rate": 2.6305970149253734e-05,
      "loss": 1.3634,
      "step": 1043
    },
    {
      "epoch": 0.3760129659643436,
      "grad_norm": 0.32823464274406433,
      "learning_rate": 2.6302359171882523e-05,
      "loss": 1.3612,
      "step": 1044
    },
    {
      "epoch": 0.37637313164055464,
      "grad_norm": 0.32300329208374023,
      "learning_rate": 2.6298748194511315e-05,
      "loss": 1.308,
      "step": 1045
    },
    {
      "epoch": 0.3767332973167657,
      "grad_norm": 0.3287125825881958,
      "learning_rate": 2.6295137217140108e-05,
      "loss": 1.299,
      "step": 1046
    },
    {
      "epoch": 0.37709346299297675,
      "grad_norm": 0.32276245951652527,
      "learning_rate": 2.6291526239768896e-05,
      "loss": 1.2659,
      "step": 1047
    },
    {
      "epoch": 0.3774536286691878,
      "grad_norm": 0.315036416053772,
      "learning_rate": 2.628791526239769e-05,
      "loss": 1.3776,
      "step": 1048
    },
    {
      "epoch": 0.37781379434539886,
      "grad_norm": 0.33221402764320374,
      "learning_rate": 2.628430428502648e-05,
      "loss": 1.3141,
      "step": 1049
    },
    {
      "epoch": 0.37817396002160997,
      "grad_norm": 0.31885507702827454,
      "learning_rate": 2.6280693307655273e-05,
      "loss": 1.3518,
      "step": 1050
    },
    {
      "epoch": 0.378534125697821,
      "grad_norm": 0.32353073358535767,
      "learning_rate": 2.6277082330284066e-05,
      "loss": 1.3335,
      "step": 1051
    },
    {
      "epoch": 0.3788942913740321,
      "grad_norm": 0.3311023712158203,
      "learning_rate": 2.6273471352912855e-05,
      "loss": 1.311,
      "step": 1052
    },
    {
      "epoch": 0.37925445705024313,
      "grad_norm": 0.3330022692680359,
      "learning_rate": 2.6269860375541647e-05,
      "loss": 1.27,
      "step": 1053
    },
    {
      "epoch": 0.3796146227264542,
      "grad_norm": 0.3214002251625061,
      "learning_rate": 2.626624939817044e-05,
      "loss": 1.4107,
      "step": 1054
    },
    {
      "epoch": 0.37997478840266524,
      "grad_norm": 0.32936400175094604,
      "learning_rate": 2.6262638420799228e-05,
      "loss": 1.3139,
      "step": 1055
    },
    {
      "epoch": 0.3803349540788763,
      "grad_norm": 0.3272179961204529,
      "learning_rate": 2.6259027443428024e-05,
      "loss": 1.4172,
      "step": 1056
    },
    {
      "epoch": 0.38069511975508735,
      "grad_norm": 0.3254511058330536,
      "learning_rate": 2.6255416466056813e-05,
      "loss": 1.3357,
      "step": 1057
    },
    {
      "epoch": 0.3810552854312984,
      "grad_norm": 0.32297828793525696,
      "learning_rate": 2.6251805488685605e-05,
      "loss": 1.4074,
      "step": 1058
    },
    {
      "epoch": 0.38141545110750946,
      "grad_norm": 0.3328172564506531,
      "learning_rate": 2.6248194511314397e-05,
      "loss": 1.3745,
      "step": 1059
    },
    {
      "epoch": 0.3817756167837205,
      "grad_norm": 0.3177134692668915,
      "learning_rate": 2.6244583533943186e-05,
      "loss": 1.2731,
      "step": 1060
    },
    {
      "epoch": 0.38213578245993157,
      "grad_norm": 0.33122125267982483,
      "learning_rate": 2.624097255657198e-05,
      "loss": 1.3233,
      "step": 1061
    },
    {
      "epoch": 0.3824959481361426,
      "grad_norm": 0.3392486274242401,
      "learning_rate": 2.6237361579200774e-05,
      "loss": 1.4571,
      "step": 1062
    },
    {
      "epoch": 0.3828561138123537,
      "grad_norm": 0.3181159198284149,
      "learning_rate": 2.6233750601829563e-05,
      "loss": 1.3363,
      "step": 1063
    },
    {
      "epoch": 0.38321627948856474,
      "grad_norm": 0.33376818895339966,
      "learning_rate": 2.6230139624458355e-05,
      "loss": 1.2296,
      "step": 1064
    },
    {
      "epoch": 0.3835764451647758,
      "grad_norm": 0.3223359286785126,
      "learning_rate": 2.6226528647087144e-05,
      "loss": 1.3079,
      "step": 1065
    },
    {
      "epoch": 0.38393661084098685,
      "grad_norm": 0.3233873248100281,
      "learning_rate": 2.6222917669715937e-05,
      "loss": 1.3773,
      "step": 1066
    },
    {
      "epoch": 0.3842967765171979,
      "grad_norm": 0.32969385385513306,
      "learning_rate": 2.621930669234473e-05,
      "loss": 1.3355,
      "step": 1067
    },
    {
      "epoch": 0.38465694219340896,
      "grad_norm": 0.3259814381599426,
      "learning_rate": 2.6215695714973518e-05,
      "loss": 1.3868,
      "step": 1068
    },
    {
      "epoch": 0.38501710786962,
      "grad_norm": 0.32139140367507935,
      "learning_rate": 2.6212084737602313e-05,
      "loss": 1.2808,
      "step": 1069
    },
    {
      "epoch": 0.38537727354583107,
      "grad_norm": 0.32923123240470886,
      "learning_rate": 2.6208473760231106e-05,
      "loss": 1.3865,
      "step": 1070
    },
    {
      "epoch": 0.3857374392220421,
      "grad_norm": 0.31991055607795715,
      "learning_rate": 2.6204862782859895e-05,
      "loss": 1.2935,
      "step": 1071
    },
    {
      "epoch": 0.3860976048982532,
      "grad_norm": 0.3360276222229004,
      "learning_rate": 2.6201251805488687e-05,
      "loss": 1.2925,
      "step": 1072
    },
    {
      "epoch": 0.38645777057446423,
      "grad_norm": 0.3295302987098694,
      "learning_rate": 2.6197640828117476e-05,
      "loss": 1.3448,
      "step": 1073
    },
    {
      "epoch": 0.3868179362506753,
      "grad_norm": 0.3194162845611572,
      "learning_rate": 2.6194029850746268e-05,
      "loss": 1.3586,
      "step": 1074
    },
    {
      "epoch": 0.38717810192688634,
      "grad_norm": 0.3278861343860626,
      "learning_rate": 2.619041887337506e-05,
      "loss": 1.352,
      "step": 1075
    },
    {
      "epoch": 0.3875382676030974,
      "grad_norm": 0.3334444761276245,
      "learning_rate": 2.6186807896003853e-05,
      "loss": 1.4065,
      "step": 1076
    },
    {
      "epoch": 0.3878984332793085,
      "grad_norm": 0.32194945216178894,
      "learning_rate": 2.6183196918632645e-05,
      "loss": 1.186,
      "step": 1077
    },
    {
      "epoch": 0.38825859895551956,
      "grad_norm": 0.3366781175136566,
      "learning_rate": 2.6179585941261437e-05,
      "loss": 1.3221,
      "step": 1078
    },
    {
      "epoch": 0.3886187646317306,
      "grad_norm": 0.2972774803638458,
      "learning_rate": 2.6175974963890226e-05,
      "loss": 1.3621,
      "step": 1079
    },
    {
      "epoch": 0.3889789303079417,
      "grad_norm": 0.3184162378311157,
      "learning_rate": 2.617236398651902e-05,
      "loss": 1.2049,
      "step": 1080
    },
    {
      "epoch": 0.3893390959841527,
      "grad_norm": 0.32604706287384033,
      "learning_rate": 2.6168753009147807e-05,
      "loss": 1.4179,
      "step": 1081
    },
    {
      "epoch": 0.3896992616603638,
      "grad_norm": 0.3226037323474884,
      "learning_rate": 2.61651420317766e-05,
      "loss": 1.3195,
      "step": 1082
    },
    {
      "epoch": 0.39005942733657484,
      "grad_norm": 0.31765422224998474,
      "learning_rate": 2.6161531054405395e-05,
      "loss": 1.3322,
      "step": 1083
    },
    {
      "epoch": 0.3904195930127859,
      "grad_norm": 0.33098936080932617,
      "learning_rate": 2.6157920077034184e-05,
      "loss": 1.3341,
      "step": 1084
    },
    {
      "epoch": 0.39077975868899695,
      "grad_norm": 0.3331529200077057,
      "learning_rate": 2.6154309099662977e-05,
      "loss": 1.4007,
      "step": 1085
    },
    {
      "epoch": 0.391139924365208,
      "grad_norm": 0.33707982301712036,
      "learning_rate": 2.615069812229177e-05,
      "loss": 1.1978,
      "step": 1086
    },
    {
      "epoch": 0.39150009004141906,
      "grad_norm": 0.32881370186805725,
      "learning_rate": 2.6147087144920558e-05,
      "loss": 1.2751,
      "step": 1087
    },
    {
      "epoch": 0.3918602557176301,
      "grad_norm": 0.31667715311050415,
      "learning_rate": 2.614347616754935e-05,
      "loss": 1.2302,
      "step": 1088
    },
    {
      "epoch": 0.39222042139384117,
      "grad_norm": 0.33396950364112854,
      "learning_rate": 2.6139865190178142e-05,
      "loss": 1.3749,
      "step": 1089
    },
    {
      "epoch": 0.3925805870700522,
      "grad_norm": 0.32150208950042725,
      "learning_rate": 2.6136254212806935e-05,
      "loss": 1.2877,
      "step": 1090
    },
    {
      "epoch": 0.3929407527462633,
      "grad_norm": 0.3345465362071991,
      "learning_rate": 2.6132643235435727e-05,
      "loss": 1.3515,
      "step": 1091
    },
    {
      "epoch": 0.39330091842247433,
      "grad_norm": 0.33716708421707153,
      "learning_rate": 2.6129032258064516e-05,
      "loss": 1.4682,
      "step": 1092
    },
    {
      "epoch": 0.3936610840986854,
      "grad_norm": 0.3220948576927185,
      "learning_rate": 2.6125421280693308e-05,
      "loss": 1.3593,
      "step": 1093
    },
    {
      "epoch": 0.39402124977489644,
      "grad_norm": 0.31979942321777344,
      "learning_rate": 2.61218103033221e-05,
      "loss": 1.3388,
      "step": 1094
    },
    {
      "epoch": 0.3943814154511075,
      "grad_norm": 0.3176611661911011,
      "learning_rate": 2.611819932595089e-05,
      "loss": 1.2915,
      "step": 1095
    },
    {
      "epoch": 0.39474158112731855,
      "grad_norm": 0.34779468178749084,
      "learning_rate": 2.6114588348579685e-05,
      "loss": 1.418,
      "step": 1096
    },
    {
      "epoch": 0.3951017468035296,
      "grad_norm": 0.3162435293197632,
      "learning_rate": 2.6110977371208474e-05,
      "loss": 1.3325,
      "step": 1097
    },
    {
      "epoch": 0.39546191247974066,
      "grad_norm": 0.32590290904045105,
      "learning_rate": 2.6107366393837266e-05,
      "loss": 1.3111,
      "step": 1098
    },
    {
      "epoch": 0.3958220781559517,
      "grad_norm": 0.3406870365142822,
      "learning_rate": 2.610375541646606e-05,
      "loss": 1.4085,
      "step": 1099
    },
    {
      "epoch": 0.3961822438321628,
      "grad_norm": 0.32313990592956543,
      "learning_rate": 2.6100144439094848e-05,
      "loss": 1.4317,
      "step": 1100
    },
    {
      "epoch": 0.3965424095083738,
      "grad_norm": 0.33259350061416626,
      "learning_rate": 2.609653346172364e-05,
      "loss": 1.3442,
      "step": 1101
    },
    {
      "epoch": 0.3969025751845849,
      "grad_norm": 0.32251402735710144,
      "learning_rate": 2.6092922484352432e-05,
      "loss": 1.2622,
      "step": 1102
    },
    {
      "epoch": 0.397262740860796,
      "grad_norm": 0.3217095732688904,
      "learning_rate": 2.6089311506981224e-05,
      "loss": 1.307,
      "step": 1103
    },
    {
      "epoch": 0.39762290653700705,
      "grad_norm": 0.3245779275894165,
      "learning_rate": 2.6085700529610017e-05,
      "loss": 1.3707,
      "step": 1104
    },
    {
      "epoch": 0.3979830722132181,
      "grad_norm": 0.3160456418991089,
      "learning_rate": 2.6082089552238806e-05,
      "loss": 1.3013,
      "step": 1105
    },
    {
      "epoch": 0.39834323788942916,
      "grad_norm": 0.3187664747238159,
      "learning_rate": 2.6078478574867598e-05,
      "loss": 1.3497,
      "step": 1106
    },
    {
      "epoch": 0.3987034035656402,
      "grad_norm": 0.3239864408969879,
      "learning_rate": 2.607486759749639e-05,
      "loss": 1.2749,
      "step": 1107
    },
    {
      "epoch": 0.39906356924185127,
      "grad_norm": 0.33544641733169556,
      "learning_rate": 2.607125662012518e-05,
      "loss": 1.3038,
      "step": 1108
    },
    {
      "epoch": 0.3994237349180623,
      "grad_norm": 0.3224370777606964,
      "learning_rate": 2.606764564275397e-05,
      "loss": 1.3017,
      "step": 1109
    },
    {
      "epoch": 0.3997839005942734,
      "grad_norm": 0.31542426347732544,
      "learning_rate": 2.6064034665382767e-05,
      "loss": 1.3424,
      "step": 1110
    },
    {
      "epoch": 0.40014406627048443,
      "grad_norm": 0.34158048033714294,
      "learning_rate": 2.6060423688011556e-05,
      "loss": 1.3181,
      "step": 1111
    },
    {
      "epoch": 0.4005042319466955,
      "grad_norm": 0.3370940387248993,
      "learning_rate": 2.6056812710640348e-05,
      "loss": 1.424,
      "step": 1112
    },
    {
      "epoch": 0.40086439762290654,
      "grad_norm": 0.32592493295669556,
      "learning_rate": 2.6053201733269137e-05,
      "loss": 1.3335,
      "step": 1113
    },
    {
      "epoch": 0.4012245632991176,
      "grad_norm": 0.32399982213974,
      "learning_rate": 2.604959075589793e-05,
      "loss": 1.3221,
      "step": 1114
    },
    {
      "epoch": 0.40158472897532865,
      "grad_norm": 0.33001554012298584,
      "learning_rate": 2.6045979778526722e-05,
      "loss": 1.3295,
      "step": 1115
    },
    {
      "epoch": 0.4019448946515397,
      "grad_norm": 0.31902340054512024,
      "learning_rate": 2.6042368801155514e-05,
      "loss": 1.1914,
      "step": 1116
    },
    {
      "epoch": 0.40230506032775076,
      "grad_norm": 0.3185466527938843,
      "learning_rate": 2.6038757823784306e-05,
      "loss": 1.3788,
      "step": 1117
    },
    {
      "epoch": 0.4026652260039618,
      "grad_norm": 0.31966596841812134,
      "learning_rate": 2.60351468464131e-05,
      "loss": 1.4157,
      "step": 1118
    },
    {
      "epoch": 0.4030253916801729,
      "grad_norm": 0.3353741466999054,
      "learning_rate": 2.6031535869041888e-05,
      "loss": 1.3364,
      "step": 1119
    },
    {
      "epoch": 0.40338555735638393,
      "grad_norm": 0.31505027413368225,
      "learning_rate": 2.602792489167068e-05,
      "loss": 1.3366,
      "step": 1120
    },
    {
      "epoch": 0.403745723032595,
      "grad_norm": 0.30072876811027527,
      "learning_rate": 2.602431391429947e-05,
      "loss": 1.3161,
      "step": 1121
    },
    {
      "epoch": 0.40410588870880604,
      "grad_norm": 0.3197629451751709,
      "learning_rate": 2.602070293692826e-05,
      "loss": 1.3072,
      "step": 1122
    },
    {
      "epoch": 0.4044660543850171,
      "grad_norm": 0.3343706727027893,
      "learning_rate": 2.6017091959557057e-05,
      "loss": 1.3312,
      "step": 1123
    },
    {
      "epoch": 0.40482622006122815,
      "grad_norm": 0.32533732056617737,
      "learning_rate": 2.6013480982185846e-05,
      "loss": 1.2566,
      "step": 1124
    },
    {
      "epoch": 0.4051863857374392,
      "grad_norm": 0.3362985849380493,
      "learning_rate": 2.6009870004814638e-05,
      "loss": 1.3879,
      "step": 1125
    },
    {
      "epoch": 0.40554655141365026,
      "grad_norm": 0.34558162093162537,
      "learning_rate": 2.600625902744343e-05,
      "loss": 1.3645,
      "step": 1126
    },
    {
      "epoch": 0.4059067170898613,
      "grad_norm": 0.330897718667984,
      "learning_rate": 2.600264805007222e-05,
      "loss": 1.2436,
      "step": 1127
    },
    {
      "epoch": 0.40626688276607237,
      "grad_norm": 0.353772908449173,
      "learning_rate": 2.599903707270101e-05,
      "loss": 1.4078,
      "step": 1128
    },
    {
      "epoch": 0.4066270484422834,
      "grad_norm": 0.3148053288459778,
      "learning_rate": 2.59954260953298e-05,
      "loss": 1.3337,
      "step": 1129
    },
    {
      "epoch": 0.40698721411849453,
      "grad_norm": 0.3259131908416748,
      "learning_rate": 2.5991815117958596e-05,
      "loss": 1.4171,
      "step": 1130
    },
    {
      "epoch": 0.4073473797947056,
      "grad_norm": 0.32179155945777893,
      "learning_rate": 2.598820414058739e-05,
      "loss": 1.4635,
      "step": 1131
    },
    {
      "epoch": 0.40770754547091664,
      "grad_norm": 0.3405478596687317,
      "learning_rate": 2.5984593163216177e-05,
      "loss": 1.3397,
      "step": 1132
    },
    {
      "epoch": 0.4080677111471277,
      "grad_norm": 0.32933732867240906,
      "learning_rate": 2.598098218584497e-05,
      "loss": 1.2835,
      "step": 1133
    },
    {
      "epoch": 0.40842787682333875,
      "grad_norm": 0.33911046385765076,
      "learning_rate": 2.5977371208473762e-05,
      "loss": 1.2934,
      "step": 1134
    },
    {
      "epoch": 0.4087880424995498,
      "grad_norm": 0.30520978569984436,
      "learning_rate": 2.597376023110255e-05,
      "loss": 1.2812,
      "step": 1135
    },
    {
      "epoch": 0.40914820817576086,
      "grad_norm": 0.3296959400177002,
      "learning_rate": 2.5970149253731343e-05,
      "loss": 1.328,
      "step": 1136
    },
    {
      "epoch": 0.4095083738519719,
      "grad_norm": 0.3329487144947052,
      "learning_rate": 2.5966538276360135e-05,
      "loss": 1.3275,
      "step": 1137
    },
    {
      "epoch": 0.409868539528183,
      "grad_norm": 0.33509767055511475,
      "learning_rate": 2.5962927298988928e-05,
      "loss": 1.3803,
      "step": 1138
    },
    {
      "epoch": 0.41022870520439403,
      "grad_norm": 0.31249040365219116,
      "learning_rate": 2.595931632161772e-05,
      "loss": 1.2718,
      "step": 1139
    },
    {
      "epoch": 0.4105888708806051,
      "grad_norm": 0.32452264428138733,
      "learning_rate": 2.595570534424651e-05,
      "loss": 1.4754,
      "step": 1140
    },
    {
      "epoch": 0.41094903655681614,
      "grad_norm": 0.3086150288581848,
      "learning_rate": 2.59520943668753e-05,
      "loss": 1.2299,
      "step": 1141
    },
    {
      "epoch": 0.4113092022330272,
      "grad_norm": 0.32238295674324036,
      "learning_rate": 2.5948483389504093e-05,
      "loss": 1.2763,
      "step": 1142
    },
    {
      "epoch": 0.41166936790923825,
      "grad_norm": 0.3061293959617615,
      "learning_rate": 2.5944872412132886e-05,
      "loss": 1.3501,
      "step": 1143
    },
    {
      "epoch": 0.4120295335854493,
      "grad_norm": 0.32055723667144775,
      "learning_rate": 2.5941261434761678e-05,
      "loss": 1.306,
      "step": 1144
    },
    {
      "epoch": 0.41238969926166036,
      "grad_norm": 0.33320313692092896,
      "learning_rate": 2.5937650457390467e-05,
      "loss": 1.3094,
      "step": 1145
    },
    {
      "epoch": 0.4127498649378714,
      "grad_norm": 0.32985326647758484,
      "learning_rate": 2.593403948001926e-05,
      "loss": 1.1695,
      "step": 1146
    },
    {
      "epoch": 0.41311003061408247,
      "grad_norm": 0.3236633241176605,
      "learning_rate": 2.593042850264805e-05,
      "loss": 1.3968,
      "step": 1147
    },
    {
      "epoch": 0.4134701962902935,
      "grad_norm": 0.32500484585762024,
      "learning_rate": 2.592681752527684e-05,
      "loss": 1.4039,
      "step": 1148
    },
    {
      "epoch": 0.4138303619665046,
      "grad_norm": 0.3442186117172241,
      "learning_rate": 2.5923206547905633e-05,
      "loss": 1.3236,
      "step": 1149
    },
    {
      "epoch": 0.41419052764271563,
      "grad_norm": 0.3207985758781433,
      "learning_rate": 2.5919595570534425e-05,
      "loss": 1.2341,
      "step": 1150
    },
    {
      "epoch": 0.4145506933189267,
      "grad_norm": 0.32380878925323486,
      "learning_rate": 2.5915984593163217e-05,
      "loss": 1.3847,
      "step": 1151
    },
    {
      "epoch": 0.41491085899513774,
      "grad_norm": 0.32702359557151794,
      "learning_rate": 2.591237361579201e-05,
      "loss": 1.3296,
      "step": 1152
    },
    {
      "epoch": 0.4152710246713488,
      "grad_norm": 0.33013665676116943,
      "learning_rate": 2.59087626384208e-05,
      "loss": 1.3403,
      "step": 1153
    },
    {
      "epoch": 0.41563119034755985,
      "grad_norm": 0.3685583174228668,
      "learning_rate": 2.590515166104959e-05,
      "loss": 1.4356,
      "step": 1154
    },
    {
      "epoch": 0.4159913560237709,
      "grad_norm": 0.33833223581314087,
      "learning_rate": 2.5901540683678383e-05,
      "loss": 1.3295,
      "step": 1155
    },
    {
      "epoch": 0.416351521699982,
      "grad_norm": 0.3221275210380554,
      "learning_rate": 2.5897929706307172e-05,
      "loss": 1.1833,
      "step": 1156
    },
    {
      "epoch": 0.4167116873761931,
      "grad_norm": 0.3289901912212372,
      "learning_rate": 2.5894318728935968e-05,
      "loss": 1.3147,
      "step": 1157
    },
    {
      "epoch": 0.41707185305240413,
      "grad_norm": 0.3372722268104553,
      "learning_rate": 2.5890707751564757e-05,
      "loss": 1.357,
      "step": 1158
    },
    {
      "epoch": 0.4174320187286152,
      "grad_norm": 0.3237413167953491,
      "learning_rate": 2.588709677419355e-05,
      "loss": 1.2915,
      "step": 1159
    },
    {
      "epoch": 0.41779218440482624,
      "grad_norm": 0.3235025703907013,
      "learning_rate": 2.588348579682234e-05,
      "loss": 1.2916,
      "step": 1160
    },
    {
      "epoch": 0.4181523500810373,
      "grad_norm": 0.32692497968673706,
      "learning_rate": 2.587987481945113e-05,
      "loss": 1.3985,
      "step": 1161
    },
    {
      "epoch": 0.41851251575724835,
      "grad_norm": 0.33316493034362793,
      "learning_rate": 2.5876263842079922e-05,
      "loss": 1.3982,
      "step": 1162
    },
    {
      "epoch": 0.4188726814334594,
      "grad_norm": 0.3273858428001404,
      "learning_rate": 2.5872652864708715e-05,
      "loss": 1.2756,
      "step": 1163
    },
    {
      "epoch": 0.41923284710967046,
      "grad_norm": 0.33068424463272095,
      "learning_rate": 2.5869041887337507e-05,
      "loss": 1.2586,
      "step": 1164
    },
    {
      "epoch": 0.4195930127858815,
      "grad_norm": 0.3346961438655853,
      "learning_rate": 2.58654309099663e-05,
      "loss": 1.3336,
      "step": 1165
    },
    {
      "epoch": 0.41995317846209257,
      "grad_norm": 0.35699617862701416,
      "learning_rate": 2.5861819932595088e-05,
      "loss": 1.3232,
      "step": 1166
    },
    {
      "epoch": 0.4203133441383036,
      "grad_norm": 0.3214932680130005,
      "learning_rate": 2.585820895522388e-05,
      "loss": 1.2585,
      "step": 1167
    },
    {
      "epoch": 0.4206735098145147,
      "grad_norm": 0.3158808946609497,
      "learning_rate": 2.5854597977852673e-05,
      "loss": 1.2619,
      "step": 1168
    },
    {
      "epoch": 0.42103367549072573,
      "grad_norm": 0.3309345841407776,
      "learning_rate": 2.5850987000481462e-05,
      "loss": 1.2752,
      "step": 1169
    },
    {
      "epoch": 0.4213938411669368,
      "grad_norm": 0.35201042890548706,
      "learning_rate": 2.5847376023110257e-05,
      "loss": 1.4758,
      "step": 1170
    },
    {
      "epoch": 0.42175400684314784,
      "grad_norm": 0.3224453628063202,
      "learning_rate": 2.584376504573905e-05,
      "loss": 1.1937,
      "step": 1171
    },
    {
      "epoch": 0.4221141725193589,
      "grad_norm": 0.34400302171707153,
      "learning_rate": 2.584015406836784e-05,
      "loss": 1.3542,
      "step": 1172
    },
    {
      "epoch": 0.42247433819556995,
      "grad_norm": 0.3319680392742157,
      "learning_rate": 2.583654309099663e-05,
      "loss": 1.2765,
      "step": 1173
    },
    {
      "epoch": 0.422834503871781,
      "grad_norm": 0.3405955135822296,
      "learning_rate": 2.583293211362542e-05,
      "loss": 1.4133,
      "step": 1174
    },
    {
      "epoch": 0.42319466954799206,
      "grad_norm": 0.31790629029273987,
      "learning_rate": 2.5829321136254212e-05,
      "loss": 1.2503,
      "step": 1175
    },
    {
      "epoch": 0.4235548352242031,
      "grad_norm": 0.3459677994251251,
      "learning_rate": 2.5825710158883004e-05,
      "loss": 1.4134,
      "step": 1176
    },
    {
      "epoch": 0.4239150009004142,
      "grad_norm": 0.3183130919933319,
      "learning_rate": 2.5822099181511797e-05,
      "loss": 1.2252,
      "step": 1177
    },
    {
      "epoch": 0.42427516657662523,
      "grad_norm": 0.3317658305168152,
      "learning_rate": 2.581848820414059e-05,
      "loss": 1.3689,
      "step": 1178
    },
    {
      "epoch": 0.4246353322528363,
      "grad_norm": 0.33449700474739075,
      "learning_rate": 2.581487722676938e-05,
      "loss": 1.398,
      "step": 1179
    },
    {
      "epoch": 0.42499549792904734,
      "grad_norm": 0.3275371491909027,
      "learning_rate": 2.581126624939817e-05,
      "loss": 1.3167,
      "step": 1180
    },
    {
      "epoch": 0.4253556636052584,
      "grad_norm": 0.32535865902900696,
      "learning_rate": 2.5807655272026963e-05,
      "loss": 1.3386,
      "step": 1181
    },
    {
      "epoch": 0.42571582928146945,
      "grad_norm": 0.34381216764450073,
      "learning_rate": 2.580404429465575e-05,
      "loss": 1.3798,
      "step": 1182
    },
    {
      "epoch": 0.42607599495768056,
      "grad_norm": 0.32999682426452637,
      "learning_rate": 2.5800433317284544e-05,
      "loss": 1.3613,
      "step": 1183
    },
    {
      "epoch": 0.4264361606338916,
      "grad_norm": 0.3353017270565033,
      "learning_rate": 2.579682233991334e-05,
      "loss": 1.3069,
      "step": 1184
    },
    {
      "epoch": 0.42679632631010267,
      "grad_norm": 0.33571818470954895,
      "learning_rate": 2.579321136254213e-05,
      "loss": 1.3593,
      "step": 1185
    },
    {
      "epoch": 0.4271564919863137,
      "grad_norm": 0.327068030834198,
      "learning_rate": 2.578960038517092e-05,
      "loss": 1.3471,
      "step": 1186
    },
    {
      "epoch": 0.4275166576625248,
      "grad_norm": 0.3317216634750366,
      "learning_rate": 2.5785989407799713e-05,
      "loss": 1.3153,
      "step": 1187
    },
    {
      "epoch": 0.42787682333873583,
      "grad_norm": 0.3258547782897949,
      "learning_rate": 2.5782378430428502e-05,
      "loss": 1.28,
      "step": 1188
    },
    {
      "epoch": 0.4282369890149469,
      "grad_norm": 0.33759015798568726,
      "learning_rate": 2.5778767453057294e-05,
      "loss": 1.4418,
      "step": 1189
    },
    {
      "epoch": 0.42859715469115794,
      "grad_norm": 0.3353176712989807,
      "learning_rate": 2.5775156475686086e-05,
      "loss": 1.2162,
      "step": 1190
    },
    {
      "epoch": 0.428957320367369,
      "grad_norm": 0.33904704451560974,
      "learning_rate": 2.577154549831488e-05,
      "loss": 1.2639,
      "step": 1191
    },
    {
      "epoch": 0.42931748604358005,
      "grad_norm": 0.3306702971458435,
      "learning_rate": 2.576793452094367e-05,
      "loss": 1.3005,
      "step": 1192
    },
    {
      "epoch": 0.4296776517197911,
      "grad_norm": 0.3287489414215088,
      "learning_rate": 2.576432354357246e-05,
      "loss": 1.2551,
      "step": 1193
    },
    {
      "epoch": 0.43003781739600216,
      "grad_norm": 0.33707576990127563,
      "learning_rate": 2.5760712566201252e-05,
      "loss": 1.3278,
      "step": 1194
    },
    {
      "epoch": 0.4303979830722132,
      "grad_norm": 0.33452507853507996,
      "learning_rate": 2.5757101588830045e-05,
      "loss": 1.2695,
      "step": 1195
    },
    {
      "epoch": 0.4307581487484243,
      "grad_norm": 0.33669400215148926,
      "learning_rate": 2.5753490611458833e-05,
      "loss": 1.2904,
      "step": 1196
    },
    {
      "epoch": 0.43111831442463533,
      "grad_norm": 0.3232422173023224,
      "learning_rate": 2.574987963408763e-05,
      "loss": 1.2036,
      "step": 1197
    },
    {
      "epoch": 0.4314784801008464,
      "grad_norm": 0.3285102844238281,
      "learning_rate": 2.5746268656716418e-05,
      "loss": 1.2677,
      "step": 1198
    },
    {
      "epoch": 0.43183864577705744,
      "grad_norm": 0.32246649265289307,
      "learning_rate": 2.574265767934521e-05,
      "loss": 1.2684,
      "step": 1199
    },
    {
      "epoch": 0.4321988114532685,
      "grad_norm": 0.3366278409957886,
      "learning_rate": 2.5739046701974003e-05,
      "loss": 1.3361,
      "step": 1200
    },
    {
      "epoch": 0.43255897712947955,
      "grad_norm": 0.33382663130760193,
      "learning_rate": 2.573543572460279e-05,
      "loss": 1.3153,
      "step": 1201
    },
    {
      "epoch": 0.4329191428056906,
      "grad_norm": 0.34307074546813965,
      "learning_rate": 2.5731824747231584e-05,
      "loss": 1.4192,
      "step": 1202
    },
    {
      "epoch": 0.43327930848190166,
      "grad_norm": 0.32244032621383667,
      "learning_rate": 2.5728213769860376e-05,
      "loss": 1.2568,
      "step": 1203
    },
    {
      "epoch": 0.4336394741581127,
      "grad_norm": 0.333463579416275,
      "learning_rate": 2.572460279248917e-05,
      "loss": 1.4511,
      "step": 1204
    },
    {
      "epoch": 0.43399963983432377,
      "grad_norm": 0.33677762746810913,
      "learning_rate": 2.572099181511796e-05,
      "loss": 1.4346,
      "step": 1205
    },
    {
      "epoch": 0.4343598055105348,
      "grad_norm": 0.34690994024276733,
      "learning_rate": 2.571738083774675e-05,
      "loss": 1.4267,
      "step": 1206
    },
    {
      "epoch": 0.4347199711867459,
      "grad_norm": 0.32804855704307556,
      "learning_rate": 2.5713769860375542e-05,
      "loss": 1.2367,
      "step": 1207
    },
    {
      "epoch": 0.43508013686295693,
      "grad_norm": 0.3296102285385132,
      "learning_rate": 2.5710158883004334e-05,
      "loss": 1.3271,
      "step": 1208
    },
    {
      "epoch": 0.43544030253916804,
      "grad_norm": 0.3342428505420685,
      "learning_rate": 2.5706547905633123e-05,
      "loss": 1.3085,
      "step": 1209
    },
    {
      "epoch": 0.4358004682153791,
      "grad_norm": 0.33425191044807434,
      "learning_rate": 2.5702936928261915e-05,
      "loss": 1.309,
      "step": 1210
    },
    {
      "epoch": 0.43616063389159015,
      "grad_norm": 0.344566285610199,
      "learning_rate": 2.569932595089071e-05,
      "loss": 1.3215,
      "step": 1211
    },
    {
      "epoch": 0.4365207995678012,
      "grad_norm": 0.32783305644989014,
      "learning_rate": 2.56957149735195e-05,
      "loss": 1.3943,
      "step": 1212
    },
    {
      "epoch": 0.43688096524401226,
      "grad_norm": 0.32462427020072937,
      "learning_rate": 2.5692103996148292e-05,
      "loss": 1.2782,
      "step": 1213
    },
    {
      "epoch": 0.4372411309202233,
      "grad_norm": 0.30565008521080017,
      "learning_rate": 2.568849301877708e-05,
      "loss": 1.2026,
      "step": 1214
    },
    {
      "epoch": 0.4376012965964344,
      "grad_norm": 0.3362361192703247,
      "learning_rate": 2.5684882041405874e-05,
      "loss": 1.3352,
      "step": 1215
    },
    {
      "epoch": 0.43796146227264543,
      "grad_norm": 0.31863829493522644,
      "learning_rate": 2.5681271064034666e-05,
      "loss": 1.3127,
      "step": 1216
    },
    {
      "epoch": 0.4383216279488565,
      "grad_norm": 0.33164355158805847,
      "learning_rate": 2.5677660086663458e-05,
      "loss": 1.2269,
      "step": 1217
    },
    {
      "epoch": 0.43868179362506754,
      "grad_norm": 0.31214869022369385,
      "learning_rate": 2.567404910929225e-05,
      "loss": 1.3326,
      "step": 1218
    },
    {
      "epoch": 0.4390419593012786,
      "grad_norm": 0.348992258310318,
      "learning_rate": 2.5670438131921043e-05,
      "loss": 1.229,
      "step": 1219
    },
    {
      "epoch": 0.43940212497748965,
      "grad_norm": 0.30758312344551086,
      "learning_rate": 2.566682715454983e-05,
      "loss": 1.2955,
      "step": 1220
    },
    {
      "epoch": 0.4397622906537007,
      "grad_norm": 0.3306303322315216,
      "learning_rate": 2.5663216177178624e-05,
      "loss": 1.3402,
      "step": 1221
    },
    {
      "epoch": 0.44012245632991176,
      "grad_norm": 0.35033249855041504,
      "learning_rate": 2.5659605199807413e-05,
      "loss": 1.4102,
      "step": 1222
    },
    {
      "epoch": 0.4404826220061228,
      "grad_norm": 0.32870301604270935,
      "learning_rate": 2.5655994222436205e-05,
      "loss": 1.2984,
      "step": 1223
    },
    {
      "epoch": 0.44084278768233387,
      "grad_norm": 0.3508017957210541,
      "learning_rate": 2.5652383245065e-05,
      "loss": 1.4201,
      "step": 1224
    },
    {
      "epoch": 0.4412029533585449,
      "grad_norm": 0.3407013714313507,
      "learning_rate": 2.564877226769379e-05,
      "loss": 1.402,
      "step": 1225
    },
    {
      "epoch": 0.441563119034756,
      "grad_norm": 0.3217882513999939,
      "learning_rate": 2.5645161290322582e-05,
      "loss": 1.2232,
      "step": 1226
    },
    {
      "epoch": 0.44192328471096703,
      "grad_norm": 0.32631582021713257,
      "learning_rate": 2.5641550312951374e-05,
      "loss": 1.3196,
      "step": 1227
    },
    {
      "epoch": 0.4422834503871781,
      "grad_norm": 0.3423531949520111,
      "learning_rate": 2.5637939335580163e-05,
      "loss": 1.3053,
      "step": 1228
    },
    {
      "epoch": 0.44264361606338914,
      "grad_norm": 0.33958378434181213,
      "learning_rate": 2.5634328358208956e-05,
      "loss": 1.1709,
      "step": 1229
    },
    {
      "epoch": 0.4430037817396002,
      "grad_norm": 0.33105528354644775,
      "learning_rate": 2.5630717380837744e-05,
      "loss": 1.3826,
      "step": 1230
    },
    {
      "epoch": 0.44336394741581125,
      "grad_norm": 0.3105284869670868,
      "learning_rate": 2.562710640346654e-05,
      "loss": 1.2798,
      "step": 1231
    },
    {
      "epoch": 0.4437241130920223,
      "grad_norm": 0.3483918607234955,
      "learning_rate": 2.5623495426095332e-05,
      "loss": 1.2245,
      "step": 1232
    },
    {
      "epoch": 0.44408427876823336,
      "grad_norm": 0.33986780047416687,
      "learning_rate": 2.561988444872412e-05,
      "loss": 1.3111,
      "step": 1233
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.32052478194236755,
      "learning_rate": 2.5616273471352914e-05,
      "loss": 1.2109,
      "step": 1234
    },
    {
      "epoch": 0.4448046101206555,
      "grad_norm": 0.33974385261535645,
      "learning_rate": 2.5612662493981706e-05,
      "loss": 1.4397,
      "step": 1235
    },
    {
      "epoch": 0.4451647757968666,
      "grad_norm": 0.33153849840164185,
      "learning_rate": 2.5609051516610495e-05,
      "loss": 1.4172,
      "step": 1236
    },
    {
      "epoch": 0.44552494147307764,
      "grad_norm": 0.3342544436454773,
      "learning_rate": 2.5605440539239287e-05,
      "loss": 1.3245,
      "step": 1237
    },
    {
      "epoch": 0.4458851071492887,
      "grad_norm": 0.345909982919693,
      "learning_rate": 2.560182956186808e-05,
      "loss": 1.4179,
      "step": 1238
    },
    {
      "epoch": 0.44624527282549975,
      "grad_norm": 0.33018627762794495,
      "learning_rate": 2.5598218584496872e-05,
      "loss": 1.2457,
      "step": 1239
    },
    {
      "epoch": 0.4466054385017108,
      "grad_norm": 0.33850452303886414,
      "learning_rate": 2.5594607607125664e-05,
      "loss": 1.2079,
      "step": 1240
    },
    {
      "epoch": 0.44696560417792186,
      "grad_norm": 0.3338346779346466,
      "learning_rate": 2.5590996629754453e-05,
      "loss": 1.3317,
      "step": 1241
    },
    {
      "epoch": 0.4473257698541329,
      "grad_norm": 0.32196691632270813,
      "learning_rate": 2.5587385652383245e-05,
      "loss": 1.3294,
      "step": 1242
    },
    {
      "epoch": 0.44768593553034397,
      "grad_norm": 0.3212527632713318,
      "learning_rate": 2.5583774675012037e-05,
      "loss": 1.2968,
      "step": 1243
    },
    {
      "epoch": 0.448046101206555,
      "grad_norm": 0.32485342025756836,
      "learning_rate": 2.558016369764083e-05,
      "loss": 1.3072,
      "step": 1244
    },
    {
      "epoch": 0.4484062668827661,
      "grad_norm": 0.32762405276298523,
      "learning_rate": 2.5576552720269622e-05,
      "loss": 1.3686,
      "step": 1245
    },
    {
      "epoch": 0.44876643255897714,
      "grad_norm": 0.3571457862854004,
      "learning_rate": 2.557294174289841e-05,
      "loss": 1.4507,
      "step": 1246
    },
    {
      "epoch": 0.4491265982351882,
      "grad_norm": 0.34158286452293396,
      "learning_rate": 2.5569330765527203e-05,
      "loss": 1.4273,
      "step": 1247
    },
    {
      "epoch": 0.44948676391139925,
      "grad_norm": 0.33534085750579834,
      "learning_rate": 2.5565719788155996e-05,
      "loss": 1.2473,
      "step": 1248
    },
    {
      "epoch": 0.4498469295876103,
      "grad_norm": 0.3308763802051544,
      "learning_rate": 2.5562108810784784e-05,
      "loss": 1.2323,
      "step": 1249
    },
    {
      "epoch": 0.45020709526382136,
      "grad_norm": 0.3330187499523163,
      "learning_rate": 2.5558497833413577e-05,
      "loss": 1.3137,
      "step": 1250
    },
    {
      "epoch": 0.4505672609400324,
      "grad_norm": 0.3287593722343445,
      "learning_rate": 2.5554886856042372e-05,
      "loss": 1.3169,
      "step": 1251
    },
    {
      "epoch": 0.45092742661624347,
      "grad_norm": 0.3389659821987152,
      "learning_rate": 2.555127587867116e-05,
      "loss": 1.2973,
      "step": 1252
    },
    {
      "epoch": 0.4512875922924545,
      "grad_norm": 0.34130674600601196,
      "learning_rate": 2.5547664901299954e-05,
      "loss": 1.3054,
      "step": 1253
    },
    {
      "epoch": 0.4516477579686656,
      "grad_norm": 0.32815834879875183,
      "learning_rate": 2.5544053923928743e-05,
      "loss": 1.2475,
      "step": 1254
    },
    {
      "epoch": 0.45200792364487663,
      "grad_norm": 0.3162743151187897,
      "learning_rate": 2.5540442946557535e-05,
      "loss": 1.2754,
      "step": 1255
    },
    {
      "epoch": 0.4523680893210877,
      "grad_norm": 0.34294119477272034,
      "learning_rate": 2.5536831969186327e-05,
      "loss": 1.2275,
      "step": 1256
    },
    {
      "epoch": 0.45272825499729874,
      "grad_norm": 0.3396015763282776,
      "learning_rate": 2.5533220991815116e-05,
      "loss": 1.3037,
      "step": 1257
    },
    {
      "epoch": 0.4530884206735098,
      "grad_norm": 0.3120208978652954,
      "learning_rate": 2.5529610014443912e-05,
      "loss": 1.2808,
      "step": 1258
    },
    {
      "epoch": 0.45344858634972085,
      "grad_norm": 0.33789822459220886,
      "learning_rate": 2.5525999037072704e-05,
      "loss": 1.337,
      "step": 1259
    },
    {
      "epoch": 0.4538087520259319,
      "grad_norm": 0.3367375135421753,
      "learning_rate": 2.5522388059701493e-05,
      "loss": 1.4424,
      "step": 1260
    },
    {
      "epoch": 0.45416891770214296,
      "grad_norm": 0.3416728973388672,
      "learning_rate": 2.5518777082330285e-05,
      "loss": 1.3581,
      "step": 1261
    },
    {
      "epoch": 0.454529083378354,
      "grad_norm": 0.33323192596435547,
      "learning_rate": 2.5515166104959074e-05,
      "loss": 1.3104,
      "step": 1262
    },
    {
      "epoch": 0.4548892490545651,
      "grad_norm": 0.3274956941604614,
      "learning_rate": 2.5511555127587866e-05,
      "loss": 1.2979,
      "step": 1263
    },
    {
      "epoch": 0.4552494147307762,
      "grad_norm": 0.4385654628276825,
      "learning_rate": 2.550794415021666e-05,
      "loss": 1.2787,
      "step": 1264
    },
    {
      "epoch": 0.45560958040698724,
      "grad_norm": 0.341212660074234,
      "learning_rate": 2.550433317284545e-05,
      "loss": 1.284,
      "step": 1265
    },
    {
      "epoch": 0.4559697460831983,
      "grad_norm": 0.3425145149230957,
      "learning_rate": 2.5500722195474243e-05,
      "loss": 1.282,
      "step": 1266
    },
    {
      "epoch": 0.45632991175940935,
      "grad_norm": 0.32212698459625244,
      "learning_rate": 2.5497111218103036e-05,
      "loss": 1.2457,
      "step": 1267
    },
    {
      "epoch": 0.4566900774356204,
      "grad_norm": 0.3380884826183319,
      "learning_rate": 2.5493500240731825e-05,
      "loss": 1.3668,
      "step": 1268
    },
    {
      "epoch": 0.45705024311183146,
      "grad_norm": 0.3416071832180023,
      "learning_rate": 2.5489889263360617e-05,
      "loss": 1.2896,
      "step": 1269
    },
    {
      "epoch": 0.4574104087880425,
      "grad_norm": 0.34419599175453186,
      "learning_rate": 2.5486278285989406e-05,
      "loss": 1.2486,
      "step": 1270
    },
    {
      "epoch": 0.45777057446425357,
      "grad_norm": 0.33438757061958313,
      "learning_rate": 2.54826673086182e-05,
      "loss": 1.3272,
      "step": 1271
    },
    {
      "epoch": 0.4581307401404646,
      "grad_norm": 0.33108964562416077,
      "learning_rate": 2.5479056331246994e-05,
      "loss": 1.2638,
      "step": 1272
    },
    {
      "epoch": 0.4584909058166757,
      "grad_norm": 0.33005017042160034,
      "learning_rate": 2.5475445353875783e-05,
      "loss": 1.2737,
      "step": 1273
    },
    {
      "epoch": 0.45885107149288673,
      "grad_norm": 0.3181484341621399,
      "learning_rate": 2.5471834376504575e-05,
      "loss": 1.3573,
      "step": 1274
    },
    {
      "epoch": 0.4592112371690978,
      "grad_norm": 0.3164401054382324,
      "learning_rate": 2.5468223399133367e-05,
      "loss": 1.2678,
      "step": 1275
    },
    {
      "epoch": 0.45957140284530884,
      "grad_norm": 0.3174886703491211,
      "learning_rate": 2.5464612421762156e-05,
      "loss": 1.3441,
      "step": 1276
    },
    {
      "epoch": 0.4599315685215199,
      "grad_norm": 0.3256984353065491,
      "learning_rate": 2.546100144439095e-05,
      "loss": 1.3618,
      "step": 1277
    },
    {
      "epoch": 0.46029173419773095,
      "grad_norm": 0.3309071660041809,
      "learning_rate": 2.545739046701974e-05,
      "loss": 1.2725,
      "step": 1278
    },
    {
      "epoch": 0.460651899873942,
      "grad_norm": 0.32382333278656006,
      "learning_rate": 2.5453779489648533e-05,
      "loss": 1.3729,
      "step": 1279
    },
    {
      "epoch": 0.46101206555015306,
      "grad_norm": 0.3218545615673065,
      "learning_rate": 2.5450168512277325e-05,
      "loss": 1.1717,
      "step": 1280
    },
    {
      "epoch": 0.4613722312263641,
      "grad_norm": 0.32314687967300415,
      "learning_rate": 2.5446557534906114e-05,
      "loss": 1.2548,
      "step": 1281
    },
    {
      "epoch": 0.46173239690257517,
      "grad_norm": 0.3315862715244293,
      "learning_rate": 2.5442946557534907e-05,
      "loss": 1.3313,
      "step": 1282
    },
    {
      "epoch": 0.4620925625787862,
      "grad_norm": 0.3326660692691803,
      "learning_rate": 2.54393355801637e-05,
      "loss": 1.5206,
      "step": 1283
    },
    {
      "epoch": 0.4624527282549973,
      "grad_norm": 0.316313236951828,
      "learning_rate": 2.5435724602792488e-05,
      "loss": 1.3227,
      "step": 1284
    },
    {
      "epoch": 0.46281289393120834,
      "grad_norm": 0.3299567699432373,
      "learning_rate": 2.5432113625421283e-05,
      "loss": 1.2528,
      "step": 1285
    },
    {
      "epoch": 0.4631730596074194,
      "grad_norm": 0.3248937427997589,
      "learning_rate": 2.5428502648050072e-05,
      "loss": 1.136,
      "step": 1286
    },
    {
      "epoch": 0.46353322528363045,
      "grad_norm": 0.3388470411300659,
      "learning_rate": 2.5424891670678865e-05,
      "loss": 1.3717,
      "step": 1287
    },
    {
      "epoch": 0.4638933909598415,
      "grad_norm": 0.34012705087661743,
      "learning_rate": 2.5421280693307657e-05,
      "loss": 1.3978,
      "step": 1288
    },
    {
      "epoch": 0.4642535566360526,
      "grad_norm": 0.3459305465221405,
      "learning_rate": 2.5417669715936446e-05,
      "loss": 1.3368,
      "step": 1289
    },
    {
      "epoch": 0.46461372231226367,
      "grad_norm": 0.34794533252716064,
      "learning_rate": 2.5414058738565238e-05,
      "loss": 1.3341,
      "step": 1290
    },
    {
      "epoch": 0.4649738879884747,
      "grad_norm": 0.34943634271621704,
      "learning_rate": 2.541044776119403e-05,
      "loss": 1.4003,
      "step": 1291
    },
    {
      "epoch": 0.4653340536646858,
      "grad_norm": 0.3285568356513977,
      "learning_rate": 2.5406836783822823e-05,
      "loss": 1.3108,
      "step": 1292
    },
    {
      "epoch": 0.46569421934089683,
      "grad_norm": 0.3392154276371002,
      "learning_rate": 2.5403225806451615e-05,
      "loss": 1.4302,
      "step": 1293
    },
    {
      "epoch": 0.4660543850171079,
      "grad_norm": 0.3203786313533783,
      "learning_rate": 2.5399614829080404e-05,
      "loss": 1.3341,
      "step": 1294
    },
    {
      "epoch": 0.46641455069331894,
      "grad_norm": 0.33744195103645325,
      "learning_rate": 2.5396003851709196e-05,
      "loss": 1.3826,
      "step": 1295
    },
    {
      "epoch": 0.46677471636953,
      "grad_norm": 0.34241482615470886,
      "learning_rate": 2.539239287433799e-05,
      "loss": 1.3012,
      "step": 1296
    },
    {
      "epoch": 0.46713488204574105,
      "grad_norm": 0.3299082815647125,
      "learning_rate": 2.5388781896966777e-05,
      "loss": 1.3263,
      "step": 1297
    },
    {
      "epoch": 0.4674950477219521,
      "grad_norm": 0.35704123973846436,
      "learning_rate": 2.5385170919595573e-05,
      "loss": 1.3935,
      "step": 1298
    },
    {
      "epoch": 0.46785521339816316,
      "grad_norm": 0.3451521396636963,
      "learning_rate": 2.5381559942224365e-05,
      "loss": 1.3154,
      "step": 1299
    },
    {
      "epoch": 0.4682153790743742,
      "grad_norm": 0.33462047576904297,
      "learning_rate": 2.5377948964853154e-05,
      "loss": 1.3484,
      "step": 1300
    },
    {
      "epoch": 0.46857554475058527,
      "grad_norm": 0.3403688073158264,
      "learning_rate": 2.5374337987481947e-05,
      "loss": 1.3632,
      "step": 1301
    },
    {
      "epoch": 0.4689357104267963,
      "grad_norm": 0.3320171535015106,
      "learning_rate": 2.5370727010110736e-05,
      "loss": 1.2871,
      "step": 1302
    },
    {
      "epoch": 0.4692958761030074,
      "grad_norm": 0.33083418011665344,
      "learning_rate": 2.5367116032739528e-05,
      "loss": 1.307,
      "step": 1303
    },
    {
      "epoch": 0.46965604177921844,
      "grad_norm": 0.33307063579559326,
      "learning_rate": 2.536350505536832e-05,
      "loss": 1.3527,
      "step": 1304
    },
    {
      "epoch": 0.4700162074554295,
      "grad_norm": 0.335347443819046,
      "learning_rate": 2.5359894077997112e-05,
      "loss": 1.4282,
      "step": 1305
    },
    {
      "epoch": 0.47037637313164055,
      "grad_norm": 0.32688841223716736,
      "learning_rate": 2.5356283100625905e-05,
      "loss": 1.285,
      "step": 1306
    },
    {
      "epoch": 0.4707365388078516,
      "grad_norm": 0.33591368794441223,
      "learning_rate": 2.5352672123254697e-05,
      "loss": 1.3488,
      "step": 1307
    },
    {
      "epoch": 0.47109670448406266,
      "grad_norm": 0.33565381169319153,
      "learning_rate": 2.5349061145883486e-05,
      "loss": 1.4122,
      "step": 1308
    },
    {
      "epoch": 0.4714568701602737,
      "grad_norm": 0.35074782371520996,
      "learning_rate": 2.5345450168512278e-05,
      "loss": 1.3439,
      "step": 1309
    },
    {
      "epoch": 0.47181703583648477,
      "grad_norm": 0.3528682291507721,
      "learning_rate": 2.5341839191141067e-05,
      "loss": 1.2963,
      "step": 1310
    },
    {
      "epoch": 0.4721772015126958,
      "grad_norm": 0.3457191586494446,
      "learning_rate": 2.533822821376986e-05,
      "loss": 1.3226,
      "step": 1311
    },
    {
      "epoch": 0.4725373671889069,
      "grad_norm": 0.3315495550632477,
      "learning_rate": 2.5334617236398655e-05,
      "loss": 1.3238,
      "step": 1312
    },
    {
      "epoch": 0.47289753286511793,
      "grad_norm": 0.3172934353351593,
      "learning_rate": 2.5331006259027444e-05,
      "loss": 1.2628,
      "step": 1313
    },
    {
      "epoch": 0.473257698541329,
      "grad_norm": 0.3253627121448517,
      "learning_rate": 2.5327395281656236e-05,
      "loss": 1.2912,
      "step": 1314
    },
    {
      "epoch": 0.47361786421754004,
      "grad_norm": 0.3320024907588959,
      "learning_rate": 2.532378430428503e-05,
      "loss": 1.2633,
      "step": 1315
    },
    {
      "epoch": 0.47397802989375115,
      "grad_norm": 0.331440269947052,
      "learning_rate": 2.5320173326913818e-05,
      "loss": 1.236,
      "step": 1316
    },
    {
      "epoch": 0.4743381955699622,
      "grad_norm": 0.3709624111652374,
      "learning_rate": 2.531656234954261e-05,
      "loss": 1.3205,
      "step": 1317
    },
    {
      "epoch": 0.47469836124617326,
      "grad_norm": 0.3264703154563904,
      "learning_rate": 2.53129513721714e-05,
      "loss": 1.4035,
      "step": 1318
    },
    {
      "epoch": 0.4750585269223843,
      "grad_norm": 0.3372619152069092,
      "learning_rate": 2.5309340394800194e-05,
      "loss": 1.4004,
      "step": 1319
    },
    {
      "epoch": 0.47541869259859537,
      "grad_norm": 0.33595842123031616,
      "learning_rate": 2.5305729417428987e-05,
      "loss": 1.3583,
      "step": 1320
    },
    {
      "epoch": 0.4757788582748064,
      "grad_norm": 0.35513532161712646,
      "learning_rate": 2.5302118440057776e-05,
      "loss": 1.2816,
      "step": 1321
    },
    {
      "epoch": 0.4761390239510175,
      "grad_norm": 0.34307417273521423,
      "learning_rate": 2.5298507462686568e-05,
      "loss": 1.264,
      "step": 1322
    },
    {
      "epoch": 0.47649918962722854,
      "grad_norm": 0.33192145824432373,
      "learning_rate": 2.529489648531536e-05,
      "loss": 1.3278,
      "step": 1323
    },
    {
      "epoch": 0.4768593553034396,
      "grad_norm": 0.33799850940704346,
      "learning_rate": 2.529128550794415e-05,
      "loss": 1.3385,
      "step": 1324
    },
    {
      "epoch": 0.47721952097965065,
      "grad_norm": 0.3218618929386139,
      "learning_rate": 2.5287674530572945e-05,
      "loss": 1.2638,
      "step": 1325
    },
    {
      "epoch": 0.4775796866558617,
      "grad_norm": 0.33162805438041687,
      "learning_rate": 2.5284063553201734e-05,
      "loss": 1.2897,
      "step": 1326
    },
    {
      "epoch": 0.47793985233207276,
      "grad_norm": 0.35244864225387573,
      "learning_rate": 2.5280452575830526e-05,
      "loss": 1.3099,
      "step": 1327
    },
    {
      "epoch": 0.4783000180082838,
      "grad_norm": 0.32510218024253845,
      "learning_rate": 2.527684159845932e-05,
      "loss": 1.3583,
      "step": 1328
    },
    {
      "epoch": 0.47866018368449487,
      "grad_norm": 0.33919960260391235,
      "learning_rate": 2.5273230621088107e-05,
      "loss": 1.3359,
      "step": 1329
    },
    {
      "epoch": 0.4790203493607059,
      "grad_norm": 0.3237421214580536,
      "learning_rate": 2.52696196437169e-05,
      "loss": 1.2186,
      "step": 1330
    },
    {
      "epoch": 0.479380515036917,
      "grad_norm": 0.332920640707016,
      "learning_rate": 2.5266008666345692e-05,
      "loss": 1.2692,
      "step": 1331
    },
    {
      "epoch": 0.47974068071312803,
      "grad_norm": 0.35657018423080444,
      "learning_rate": 2.5262397688974484e-05,
      "loss": 1.3164,
      "step": 1332
    },
    {
      "epoch": 0.4801008463893391,
      "grad_norm": 0.3448393642902374,
      "learning_rate": 2.5258786711603276e-05,
      "loss": 1.3297,
      "step": 1333
    },
    {
      "epoch": 0.48046101206555014,
      "grad_norm": 0.3499555289745331,
      "learning_rate": 2.5255175734232065e-05,
      "loss": 1.509,
      "step": 1334
    },
    {
      "epoch": 0.4808211777417612,
      "grad_norm": 0.35805410146713257,
      "learning_rate": 2.5251564756860858e-05,
      "loss": 1.4313,
      "step": 1335
    },
    {
      "epoch": 0.48118134341797225,
      "grad_norm": 0.3554595112800598,
      "learning_rate": 2.524795377948965e-05,
      "loss": 1.324,
      "step": 1336
    },
    {
      "epoch": 0.4815415090941833,
      "grad_norm": 0.33654701709747314,
      "learning_rate": 2.524434280211844e-05,
      "loss": 1.2305,
      "step": 1337
    },
    {
      "epoch": 0.48190167477039436,
      "grad_norm": 0.34149304032325745,
      "learning_rate": 2.524073182474723e-05,
      "loss": 1.2729,
      "step": 1338
    },
    {
      "epoch": 0.4822618404466054,
      "grad_norm": 0.35308605432510376,
      "learning_rate": 2.5237120847376023e-05,
      "loss": 1.3376,
      "step": 1339
    },
    {
      "epoch": 0.48262200612281647,
      "grad_norm": 0.33804190158843994,
      "learning_rate": 2.5233509870004816e-05,
      "loss": 1.3608,
      "step": 1340
    },
    {
      "epoch": 0.4829821717990275,
      "grad_norm": 0.33007100224494934,
      "learning_rate": 2.5229898892633608e-05,
      "loss": 1.3239,
      "step": 1341
    },
    {
      "epoch": 0.48334233747523864,
      "grad_norm": 0.32911980152130127,
      "learning_rate": 2.5226287915262397e-05,
      "loss": 1.1628,
      "step": 1342
    },
    {
      "epoch": 0.4837025031514497,
      "grad_norm": 0.35327285528182983,
      "learning_rate": 2.522267693789119e-05,
      "loss": 1.388,
      "step": 1343
    },
    {
      "epoch": 0.48406266882766075,
      "grad_norm": 0.34821999073028564,
      "learning_rate": 2.521906596051998e-05,
      "loss": 1.1966,
      "step": 1344
    },
    {
      "epoch": 0.4844228345038718,
      "grad_norm": 0.3406696021556854,
      "learning_rate": 2.521545498314877e-05,
      "loss": 1.3255,
      "step": 1345
    },
    {
      "epoch": 0.48478300018008286,
      "grad_norm": 0.3343546688556671,
      "learning_rate": 2.5211844005777566e-05,
      "loss": 1.2153,
      "step": 1346
    },
    {
      "epoch": 0.4851431658562939,
      "grad_norm": 0.34487131237983704,
      "learning_rate": 2.5208233028406355e-05,
      "loss": 1.4296,
      "step": 1347
    },
    {
      "epoch": 0.48550333153250497,
      "grad_norm": 0.3469792902469635,
      "learning_rate": 2.5204622051035147e-05,
      "loss": 1.4532,
      "step": 1348
    },
    {
      "epoch": 0.485863497208716,
      "grad_norm": 0.33908018469810486,
      "learning_rate": 2.520101107366394e-05,
      "loss": 1.4196,
      "step": 1349
    },
    {
      "epoch": 0.4862236628849271,
      "grad_norm": 0.3175349235534668,
      "learning_rate": 2.519740009629273e-05,
      "loss": 1.2839,
      "step": 1350
    },
    {
      "epoch": 0.48658382856113813,
      "grad_norm": 0.3338222801685333,
      "learning_rate": 2.519378911892152e-05,
      "loss": 1.3752,
      "step": 1351
    },
    {
      "epoch": 0.4869439942373492,
      "grad_norm": 0.3416502773761749,
      "learning_rate": 2.5190178141550316e-05,
      "loss": 1.1828,
      "step": 1352
    },
    {
      "epoch": 0.48730415991356024,
      "grad_norm": 0.33523309230804443,
      "learning_rate": 2.5186567164179105e-05,
      "loss": 1.3354,
      "step": 1353
    },
    {
      "epoch": 0.4876643255897713,
      "grad_norm": 0.32843226194381714,
      "learning_rate": 2.5182956186807898e-05,
      "loss": 1.2476,
      "step": 1354
    },
    {
      "epoch": 0.48802449126598235,
      "grad_norm": 0.33135733008384705,
      "learning_rate": 2.5179345209436687e-05,
      "loss": 1.1188,
      "step": 1355
    },
    {
      "epoch": 0.4883846569421934,
      "grad_norm": 0.33318960666656494,
      "learning_rate": 2.517573423206548e-05,
      "loss": 1.2265,
      "step": 1356
    },
    {
      "epoch": 0.48874482261840446,
      "grad_norm": 0.3446335196495056,
      "learning_rate": 2.517212325469427e-05,
      "loss": 1.3486,
      "step": 1357
    },
    {
      "epoch": 0.4891049882946155,
      "grad_norm": 0.32224491238594055,
      "learning_rate": 2.516851227732306e-05,
      "loss": 1.2578,
      "step": 1358
    },
    {
      "epoch": 0.48946515397082657,
      "grad_norm": 0.34145891666412354,
      "learning_rate": 2.5164901299951856e-05,
      "loss": 1.2804,
      "step": 1359
    },
    {
      "epoch": 0.4898253196470376,
      "grad_norm": 0.338030070066452,
      "learning_rate": 2.5161290322580648e-05,
      "loss": 1.3104,
      "step": 1360
    },
    {
      "epoch": 0.4901854853232487,
      "grad_norm": 0.35689783096313477,
      "learning_rate": 2.5157679345209437e-05,
      "loss": 1.2924,
      "step": 1361
    },
    {
      "epoch": 0.49054565099945974,
      "grad_norm": 0.3566490411758423,
      "learning_rate": 2.515406836783823e-05,
      "loss": 1.2947,
      "step": 1362
    },
    {
      "epoch": 0.4909058166756708,
      "grad_norm": 0.3270628750324249,
      "learning_rate": 2.5150457390467018e-05,
      "loss": 1.1998,
      "step": 1363
    },
    {
      "epoch": 0.49126598235188185,
      "grad_norm": 0.3327234387397766,
      "learning_rate": 2.514684641309581e-05,
      "loss": 1.3431,
      "step": 1364
    },
    {
      "epoch": 0.4916261480280929,
      "grad_norm": 0.322450190782547,
      "learning_rate": 2.5143235435724603e-05,
      "loss": 1.2656,
      "step": 1365
    },
    {
      "epoch": 0.49198631370430396,
      "grad_norm": 0.32648757100105286,
      "learning_rate": 2.5139624458353395e-05,
      "loss": 1.2739,
      "step": 1366
    },
    {
      "epoch": 0.492346479380515,
      "grad_norm": 0.3490039110183716,
      "learning_rate": 2.5136013480982187e-05,
      "loss": 1.3469,
      "step": 1367
    },
    {
      "epoch": 0.49270664505672607,
      "grad_norm": 0.351334810256958,
      "learning_rate": 2.513240250361098e-05,
      "loss": 1.3316,
      "step": 1368
    },
    {
      "epoch": 0.4930668107329372,
      "grad_norm": 0.32616907358169556,
      "learning_rate": 2.512879152623977e-05,
      "loss": 1.1935,
      "step": 1369
    },
    {
      "epoch": 0.49342697640914823,
      "grad_norm": 0.33769360184669495,
      "learning_rate": 2.512518054886856e-05,
      "loss": 1.2487,
      "step": 1370
    },
    {
      "epoch": 0.4937871420853593,
      "grad_norm": 0.36618441343307495,
      "learning_rate": 2.512156957149735e-05,
      "loss": 1.3043,
      "step": 1371
    },
    {
      "epoch": 0.49414730776157034,
      "grad_norm": 0.345064252614975,
      "learning_rate": 2.5117958594126142e-05,
      "loss": 1.2483,
      "step": 1372
    },
    {
      "epoch": 0.4945074734377814,
      "grad_norm": 0.35128799080848694,
      "learning_rate": 2.5114347616754938e-05,
      "loss": 1.4048,
      "step": 1373
    },
    {
      "epoch": 0.49486763911399245,
      "grad_norm": 0.36042946577072144,
      "learning_rate": 2.5110736639383727e-05,
      "loss": 1.4278,
      "step": 1374
    },
    {
      "epoch": 0.4952278047902035,
      "grad_norm": 0.3543175160884857,
      "learning_rate": 2.510712566201252e-05,
      "loss": 1.3399,
      "step": 1375
    },
    {
      "epoch": 0.49558797046641456,
      "grad_norm": 0.34946900606155396,
      "learning_rate": 2.510351468464131e-05,
      "loss": 1.2326,
      "step": 1376
    },
    {
      "epoch": 0.4959481361426256,
      "grad_norm": 0.3477536141872406,
      "learning_rate": 2.50999037072701e-05,
      "loss": 1.41,
      "step": 1377
    },
    {
      "epoch": 0.4963083018188367,
      "grad_norm": 0.3348460793495178,
      "learning_rate": 2.5096292729898892e-05,
      "loss": 1.3142,
      "step": 1378
    },
    {
      "epoch": 0.4966684674950477,
      "grad_norm": 0.3343603014945984,
      "learning_rate": 2.5092681752527685e-05,
      "loss": 1.3217,
      "step": 1379
    },
    {
      "epoch": 0.4970286331712588,
      "grad_norm": 0.3515634834766388,
      "learning_rate": 2.5089070775156477e-05,
      "loss": 1.4378,
      "step": 1380
    },
    {
      "epoch": 0.49738879884746984,
      "grad_norm": 0.3224111497402191,
      "learning_rate": 2.508545979778527e-05,
      "loss": 1.2644,
      "step": 1381
    },
    {
      "epoch": 0.4977489645236809,
      "grad_norm": 0.334468275308609,
      "learning_rate": 2.5081848820414058e-05,
      "loss": 1.2435,
      "step": 1382
    },
    {
      "epoch": 0.49810913019989195,
      "grad_norm": 0.33932843804359436,
      "learning_rate": 2.507823784304285e-05,
      "loss": 1.2783,
      "step": 1383
    },
    {
      "epoch": 0.498469295876103,
      "grad_norm": 0.3361186385154724,
      "learning_rate": 2.5074626865671643e-05,
      "loss": 1.3165,
      "step": 1384
    },
    {
      "epoch": 0.49882946155231406,
      "grad_norm": 0.3482387959957123,
      "learning_rate": 2.5071015888300432e-05,
      "loss": 1.405,
      "step": 1385
    },
    {
      "epoch": 0.4991896272285251,
      "grad_norm": 0.34049874544143677,
      "learning_rate": 2.5067404910929227e-05,
      "loss": 1.3116,
      "step": 1386
    },
    {
      "epoch": 0.49954979290473617,
      "grad_norm": 0.33904942870140076,
      "learning_rate": 2.5063793933558016e-05,
      "loss": 1.3525,
      "step": 1387
    },
    {
      "epoch": 0.4999099585809472,
      "grad_norm": 0.3322778344154358,
      "learning_rate": 2.506018295618681e-05,
      "loss": 1.2583,
      "step": 1388
    },
    {
      "epoch": 0.5002701242571583,
      "grad_norm": 0.35763269662857056,
      "learning_rate": 2.50565719788156e-05,
      "loss": 1.3235,
      "step": 1389
    },
    {
      "epoch": 0.5006302899333693,
      "grad_norm": 0.34453776478767395,
      "learning_rate": 2.505296100144439e-05,
      "loss": 1.2563,
      "step": 1390
    },
    {
      "epoch": 0.5009904556095804,
      "grad_norm": 0.3498585522174835,
      "learning_rate": 2.5049350024073182e-05,
      "loss": 1.3543,
      "step": 1391
    },
    {
      "epoch": 0.5013506212857914,
      "grad_norm": 0.36396926641464233,
      "learning_rate": 2.5045739046701974e-05,
      "loss": 1.3196,
      "step": 1392
    },
    {
      "epoch": 0.5017107869620026,
      "grad_norm": 0.3502817749977112,
      "learning_rate": 2.5042128069330767e-05,
      "loss": 1.2489,
      "step": 1393
    },
    {
      "epoch": 0.5020709526382136,
      "grad_norm": 0.35268113017082214,
      "learning_rate": 2.503851709195956e-05,
      "loss": 1.2254,
      "step": 1394
    },
    {
      "epoch": 0.5024311183144247,
      "grad_norm": 0.3578442335128784,
      "learning_rate": 2.5034906114588348e-05,
      "loss": 1.242,
      "step": 1395
    },
    {
      "epoch": 0.5027912839906357,
      "grad_norm": 0.3464513421058655,
      "learning_rate": 2.503129513721714e-05,
      "loss": 1.3166,
      "step": 1396
    },
    {
      "epoch": 0.5031514496668468,
      "grad_norm": 0.3188076913356781,
      "learning_rate": 2.5027684159845933e-05,
      "loss": 1.2157,
      "step": 1397
    },
    {
      "epoch": 0.5035116153430578,
      "grad_norm": 0.3500337302684784,
      "learning_rate": 2.502407318247472e-05,
      "loss": 1.3041,
      "step": 1398
    },
    {
      "epoch": 0.5038717810192689,
      "grad_norm": 0.34717172384262085,
      "learning_rate": 2.5020462205103514e-05,
      "loss": 1.3335,
      "step": 1399
    },
    {
      "epoch": 0.5042319466954799,
      "grad_norm": 0.3499991297721863,
      "learning_rate": 2.501685122773231e-05,
      "loss": 1.3384,
      "step": 1400
    },
    {
      "epoch": 0.504592112371691,
      "grad_norm": 0.33137109875679016,
      "learning_rate": 2.50132402503611e-05,
      "loss": 1.3605,
      "step": 1401
    },
    {
      "epoch": 0.504952278047902,
      "grad_norm": 0.34458106756210327,
      "learning_rate": 2.500962927298989e-05,
      "loss": 1.3097,
      "step": 1402
    },
    {
      "epoch": 0.5053124437241131,
      "grad_norm": 0.36398664116859436,
      "learning_rate": 2.500601829561868e-05,
      "loss": 1.3564,
      "step": 1403
    },
    {
      "epoch": 0.5056726094003241,
      "grad_norm": 0.3390893042087555,
      "learning_rate": 2.5002407318247472e-05,
      "loss": 1.2895,
      "step": 1404
    },
    {
      "epoch": 0.5060327750765352,
      "grad_norm": 0.33869799971580505,
      "learning_rate": 2.4998796340876264e-05,
      "loss": 1.189,
      "step": 1405
    },
    {
      "epoch": 0.5063929407527462,
      "grad_norm": 0.3302311897277832,
      "learning_rate": 2.4995185363505056e-05,
      "loss": 1.2798,
      "step": 1406
    },
    {
      "epoch": 0.5067531064289573,
      "grad_norm": 0.3405863642692566,
      "learning_rate": 2.499157438613385e-05,
      "loss": 1.1793,
      "step": 1407
    },
    {
      "epoch": 0.5071132721051683,
      "grad_norm": 0.3356940448284149,
      "learning_rate": 2.498796340876264e-05,
      "loss": 1.3792,
      "step": 1408
    },
    {
      "epoch": 0.5074734377813794,
      "grad_norm": 0.3552071452140808,
      "learning_rate": 2.498435243139143e-05,
      "loss": 1.2381,
      "step": 1409
    },
    {
      "epoch": 0.5078336034575905,
      "grad_norm": 0.34699153900146484,
      "learning_rate": 2.4980741454020222e-05,
      "loss": 1.296,
      "step": 1410
    },
    {
      "epoch": 0.5081937691338015,
      "grad_norm": 0.3527882993221283,
      "learning_rate": 2.497713047664901e-05,
      "loss": 1.3058,
      "step": 1411
    },
    {
      "epoch": 0.5085539348100127,
      "grad_norm": 0.35029149055480957,
      "learning_rate": 2.4973519499277803e-05,
      "loss": 1.3548,
      "step": 1412
    },
    {
      "epoch": 0.5089141004862237,
      "grad_norm": 0.32227638363838196,
      "learning_rate": 2.49699085219066e-05,
      "loss": 1.3805,
      "step": 1413
    },
    {
      "epoch": 0.5092742661624348,
      "grad_norm": 0.3444371819496155,
      "learning_rate": 2.4966297544535388e-05,
      "loss": 1.3333,
      "step": 1414
    },
    {
      "epoch": 0.5096344318386458,
      "grad_norm": 0.3460196256637573,
      "learning_rate": 2.496268656716418e-05,
      "loss": 1.3685,
      "step": 1415
    },
    {
      "epoch": 0.5099945975148569,
      "grad_norm": 0.32513225078582764,
      "learning_rate": 2.4959075589792973e-05,
      "loss": 1.3284,
      "step": 1416
    },
    {
      "epoch": 0.5103547631910679,
      "grad_norm": 0.33848005533218384,
      "learning_rate": 2.495546461242176e-05,
      "loss": 1.3619,
      "step": 1417
    },
    {
      "epoch": 0.510714928867279,
      "grad_norm": 0.3460274040699005,
      "learning_rate": 2.4951853635050554e-05,
      "loss": 1.3093,
      "step": 1418
    },
    {
      "epoch": 0.51107509454349,
      "grad_norm": 0.3519996106624603,
      "learning_rate": 2.4948242657679343e-05,
      "loss": 1.3255,
      "step": 1419
    },
    {
      "epoch": 0.5114352602197011,
      "grad_norm": 0.3276009261608124,
      "learning_rate": 2.494463168030814e-05,
      "loss": 1.357,
      "step": 1420
    },
    {
      "epoch": 0.5117954258959121,
      "grad_norm": 0.35158079862594604,
      "learning_rate": 2.494102070293693e-05,
      "loss": 1.3622,
      "step": 1421
    },
    {
      "epoch": 0.5121555915721232,
      "grad_norm": 0.3402582108974457,
      "learning_rate": 2.493740972556572e-05,
      "loss": 1.3428,
      "step": 1422
    },
    {
      "epoch": 0.5125157572483342,
      "grad_norm": 0.3416025936603546,
      "learning_rate": 2.4933798748194512e-05,
      "loss": 1.4614,
      "step": 1423
    },
    {
      "epoch": 0.5128759229245453,
      "grad_norm": 0.3431903123855591,
      "learning_rate": 2.4930187770823304e-05,
      "loss": 1.2073,
      "step": 1424
    },
    {
      "epoch": 0.5132360886007563,
      "grad_norm": 0.3234867453575134,
      "learning_rate": 2.4926576793452093e-05,
      "loss": 1.3724,
      "step": 1425
    },
    {
      "epoch": 0.5135962542769674,
      "grad_norm": 0.33675718307495117,
      "learning_rate": 2.4922965816080885e-05,
      "loss": 1.3132,
      "step": 1426
    },
    {
      "epoch": 0.5139564199531784,
      "grad_norm": 0.3780987858772278,
      "learning_rate": 2.4919354838709678e-05,
      "loss": 1.4407,
      "step": 1427
    },
    {
      "epoch": 0.5143165856293895,
      "grad_norm": 0.3643515706062317,
      "learning_rate": 2.491574386133847e-05,
      "loss": 1.4577,
      "step": 1428
    },
    {
      "epoch": 0.5146767513056005,
      "grad_norm": 0.35688987374305725,
      "learning_rate": 2.4912132883967262e-05,
      "loss": 1.3966,
      "step": 1429
    },
    {
      "epoch": 0.5150369169818116,
      "grad_norm": 0.34823623299598694,
      "learning_rate": 2.490852190659605e-05,
      "loss": 1.2909,
      "step": 1430
    },
    {
      "epoch": 0.5153970826580226,
      "grad_norm": 0.32858702540397644,
      "learning_rate": 2.4904910929224844e-05,
      "loss": 1.2677,
      "step": 1431
    },
    {
      "epoch": 0.5157572483342338,
      "grad_norm": 0.34624218940734863,
      "learning_rate": 2.4901299951853636e-05,
      "loss": 1.2835,
      "step": 1432
    },
    {
      "epoch": 0.5161174140104448,
      "grad_norm": 0.3484257459640503,
      "learning_rate": 2.4897688974482428e-05,
      "loss": 1.2825,
      "step": 1433
    },
    {
      "epoch": 0.5164775796866559,
      "grad_norm": 0.34675514698028564,
      "learning_rate": 2.489407799711122e-05,
      "loss": 1.3214,
      "step": 1434
    },
    {
      "epoch": 0.516837745362867,
      "grad_norm": 0.35606101155281067,
      "learning_rate": 2.489046701974001e-05,
      "loss": 1.3934,
      "step": 1435
    },
    {
      "epoch": 0.517197911039078,
      "grad_norm": 0.33175572752952576,
      "learning_rate": 2.48868560423688e-05,
      "loss": 1.2418,
      "step": 1436
    },
    {
      "epoch": 0.5175580767152891,
      "grad_norm": 0.35495778918266296,
      "learning_rate": 2.4883245064997594e-05,
      "loss": 1.4973,
      "step": 1437
    },
    {
      "epoch": 0.5179182423915001,
      "grad_norm": 0.3628728985786438,
      "learning_rate": 2.4879634087626383e-05,
      "loss": 1.2687,
      "step": 1438
    },
    {
      "epoch": 0.5182784080677112,
      "grad_norm": 0.3537653982639313,
      "learning_rate": 2.4876023110255175e-05,
      "loss": 1.313,
      "step": 1439
    },
    {
      "epoch": 0.5186385737439222,
      "grad_norm": 0.3437899947166443,
      "learning_rate": 2.487241213288397e-05,
      "loss": 1.296,
      "step": 1440
    },
    {
      "epoch": 0.5189987394201333,
      "grad_norm": 0.3385436534881592,
      "learning_rate": 2.486880115551276e-05,
      "loss": 1.2772,
      "step": 1441
    },
    {
      "epoch": 0.5193589050963443,
      "grad_norm": 0.36249786615371704,
      "learning_rate": 2.4865190178141552e-05,
      "loss": 1.2349,
      "step": 1442
    },
    {
      "epoch": 0.5197190707725554,
      "grad_norm": 0.3445950150489807,
      "learning_rate": 2.486157920077034e-05,
      "loss": 1.3694,
      "step": 1443
    },
    {
      "epoch": 0.5200792364487664,
      "grad_norm": 0.35541433095932007,
      "learning_rate": 2.4857968223399133e-05,
      "loss": 1.339,
      "step": 1444
    },
    {
      "epoch": 0.5204394021249775,
      "grad_norm": 0.3202570676803589,
      "learning_rate": 2.4854357246027926e-05,
      "loss": 1.2455,
      "step": 1445
    },
    {
      "epoch": 0.5207995678011885,
      "grad_norm": 0.36445072293281555,
      "learning_rate": 2.4850746268656714e-05,
      "loss": 1.3222,
      "step": 1446
    },
    {
      "epoch": 0.5211597334773996,
      "grad_norm": 0.3512912392616272,
      "learning_rate": 2.484713529128551e-05,
      "loss": 1.2685,
      "step": 1447
    },
    {
      "epoch": 0.5215198991536106,
      "grad_norm": 0.3444731831550598,
      "learning_rate": 2.4843524313914302e-05,
      "loss": 1.3261,
      "step": 1448
    },
    {
      "epoch": 0.5218800648298217,
      "grad_norm": 0.33867400884628296,
      "learning_rate": 2.483991333654309e-05,
      "loss": 1.2406,
      "step": 1449
    },
    {
      "epoch": 0.5222402305060327,
      "grad_norm": 0.3385692536830902,
      "learning_rate": 2.4836302359171884e-05,
      "loss": 1.1916,
      "step": 1450
    },
    {
      "epoch": 0.5226003961822439,
      "grad_norm": 0.34774890542030334,
      "learning_rate": 2.4832691381800673e-05,
      "loss": 1.2908,
      "step": 1451
    },
    {
      "epoch": 0.5229605618584549,
      "grad_norm": 0.36786484718322754,
      "learning_rate": 2.4829080404429465e-05,
      "loss": 1.3953,
      "step": 1452
    },
    {
      "epoch": 0.523320727534666,
      "grad_norm": 0.3312312364578247,
      "learning_rate": 2.482546942705826e-05,
      "loss": 1.3111,
      "step": 1453
    },
    {
      "epoch": 0.523680893210877,
      "grad_norm": 0.3509604036808014,
      "learning_rate": 2.482185844968705e-05,
      "loss": 1.4387,
      "step": 1454
    },
    {
      "epoch": 0.5240410588870881,
      "grad_norm": 0.34990477561950684,
      "learning_rate": 2.4818247472315842e-05,
      "loss": 1.3394,
      "step": 1455
    },
    {
      "epoch": 0.5244012245632991,
      "grad_norm": 0.3296810984611511,
      "learning_rate": 2.4814636494944634e-05,
      "loss": 1.2402,
      "step": 1456
    },
    {
      "epoch": 0.5247613902395102,
      "grad_norm": 0.34899550676345825,
      "learning_rate": 2.4811025517573423e-05,
      "loss": 1.2972,
      "step": 1457
    },
    {
      "epoch": 0.5251215559157212,
      "grad_norm": 0.34094908833503723,
      "learning_rate": 2.4807414540202215e-05,
      "loss": 1.3576,
      "step": 1458
    },
    {
      "epoch": 0.5254817215919323,
      "grad_norm": 0.35234305262565613,
      "learning_rate": 2.4803803562831004e-05,
      "loss": 1.241,
      "step": 1459
    },
    {
      "epoch": 0.5258418872681433,
      "grad_norm": 0.33400189876556396,
      "learning_rate": 2.48001925854598e-05,
      "loss": 1.1917,
      "step": 1460
    },
    {
      "epoch": 0.5262020529443544,
      "grad_norm": 0.3348073959350586,
      "learning_rate": 2.4796581608088592e-05,
      "loss": 1.2689,
      "step": 1461
    },
    {
      "epoch": 0.5265622186205655,
      "grad_norm": 0.3238757252693176,
      "learning_rate": 2.479297063071738e-05,
      "loss": 1.3031,
      "step": 1462
    },
    {
      "epoch": 0.5269223842967765,
      "grad_norm": 0.35172995924949646,
      "learning_rate": 2.4789359653346173e-05,
      "loss": 1.3261,
      "step": 1463
    },
    {
      "epoch": 0.5272825499729876,
      "grad_norm": 0.34713220596313477,
      "learning_rate": 2.4785748675974966e-05,
      "loss": 1.3537,
      "step": 1464
    },
    {
      "epoch": 0.5276427156491986,
      "grad_norm": 0.3411273658275604,
      "learning_rate": 2.4782137698603755e-05,
      "loss": 1.2294,
      "step": 1465
    },
    {
      "epoch": 0.5280028813254097,
      "grad_norm": 0.3412405550479889,
      "learning_rate": 2.4778526721232547e-05,
      "loss": 1.2686,
      "step": 1466
    },
    {
      "epoch": 0.5283630470016207,
      "grad_norm": 0.3509841561317444,
      "learning_rate": 2.477491574386134e-05,
      "loss": 1.3233,
      "step": 1467
    },
    {
      "epoch": 0.5287232126778318,
      "grad_norm": 0.34866827726364136,
      "learning_rate": 2.477130476649013e-05,
      "loss": 1.3754,
      "step": 1468
    },
    {
      "epoch": 0.5290833783540428,
      "grad_norm": 0.35196223855018616,
      "learning_rate": 2.4767693789118924e-05,
      "loss": 1.349,
      "step": 1469
    },
    {
      "epoch": 0.529443544030254,
      "grad_norm": 0.3458259403705597,
      "learning_rate": 2.4764082811747713e-05,
      "loss": 1.3105,
      "step": 1470
    },
    {
      "epoch": 0.529803709706465,
      "grad_norm": 0.3399263620376587,
      "learning_rate": 2.4760471834376505e-05,
      "loss": 1.2559,
      "step": 1471
    },
    {
      "epoch": 0.5301638753826761,
      "grad_norm": 0.35041895508766174,
      "learning_rate": 2.4756860857005297e-05,
      "loss": 1.2313,
      "step": 1472
    },
    {
      "epoch": 0.5305240410588871,
      "grad_norm": 0.34471744298934937,
      "learning_rate": 2.4753249879634086e-05,
      "loss": 1.2236,
      "step": 1473
    },
    {
      "epoch": 0.5308842067350982,
      "grad_norm": 0.34945282340049744,
      "learning_rate": 2.4749638902262882e-05,
      "loss": 1.321,
      "step": 1474
    },
    {
      "epoch": 0.5312443724113092,
      "grad_norm": 0.3778547942638397,
      "learning_rate": 2.474602792489167e-05,
      "loss": 1.2193,
      "step": 1475
    },
    {
      "epoch": 0.5316045380875203,
      "grad_norm": 0.342035174369812,
      "learning_rate": 2.4742416947520463e-05,
      "loss": 1.348,
      "step": 1476
    },
    {
      "epoch": 0.5319647037637313,
      "grad_norm": 0.3531002402305603,
      "learning_rate": 2.4738805970149255e-05,
      "loss": 1.3398,
      "step": 1477
    },
    {
      "epoch": 0.5323248694399424,
      "grad_norm": 0.36949318647384644,
      "learning_rate": 2.4735194992778044e-05,
      "loss": 1.2858,
      "step": 1478
    },
    {
      "epoch": 0.5326850351161534,
      "grad_norm": 0.3384826183319092,
      "learning_rate": 2.4731584015406837e-05,
      "loss": 1.3406,
      "step": 1479
    },
    {
      "epoch": 0.5330452007923645,
      "grad_norm": 0.35592377185821533,
      "learning_rate": 2.4727973038035632e-05,
      "loss": 1.2972,
      "step": 1480
    },
    {
      "epoch": 0.5334053664685755,
      "grad_norm": 0.34227776527404785,
      "learning_rate": 2.472436206066442e-05,
      "loss": 1.3401,
      "step": 1481
    },
    {
      "epoch": 0.5337655321447866,
      "grad_norm": 0.34531041979789734,
      "learning_rate": 2.4720751083293213e-05,
      "loss": 1.3144,
      "step": 1482
    },
    {
      "epoch": 0.5341256978209976,
      "grad_norm": 0.3508811593055725,
      "learning_rate": 2.4717140105922002e-05,
      "loss": 1.3508,
      "step": 1483
    },
    {
      "epoch": 0.5344858634972087,
      "grad_norm": 0.35103839635849,
      "learning_rate": 2.4713529128550795e-05,
      "loss": 1.2717,
      "step": 1484
    },
    {
      "epoch": 0.5348460291734197,
      "grad_norm": 0.3584863841533661,
      "learning_rate": 2.4709918151179587e-05,
      "loss": 1.25,
      "step": 1485
    },
    {
      "epoch": 0.5352061948496308,
      "grad_norm": 0.3429550528526306,
      "learning_rate": 2.4706307173808376e-05,
      "loss": 1.3309,
      "step": 1486
    },
    {
      "epoch": 0.5355663605258418,
      "grad_norm": 0.3413827121257782,
      "learning_rate": 2.470269619643717e-05,
      "loss": 1.1738,
      "step": 1487
    },
    {
      "epoch": 0.5359265262020529,
      "grad_norm": 0.3429350256919861,
      "learning_rate": 2.4699085219065964e-05,
      "loss": 1.3152,
      "step": 1488
    },
    {
      "epoch": 0.536286691878264,
      "grad_norm": 0.35629308223724365,
      "learning_rate": 2.4695474241694753e-05,
      "loss": 1.303,
      "step": 1489
    },
    {
      "epoch": 0.536646857554475,
      "grad_norm": 0.3428792953491211,
      "learning_rate": 2.4691863264323545e-05,
      "loss": 1.3243,
      "step": 1490
    },
    {
      "epoch": 0.5370070232306862,
      "grad_norm": 0.3507995009422302,
      "learning_rate": 2.4688252286952334e-05,
      "loss": 1.2746,
      "step": 1491
    },
    {
      "epoch": 0.5373671889068972,
      "grad_norm": 0.3537353575229645,
      "learning_rate": 2.4684641309581126e-05,
      "loss": 1.3009,
      "step": 1492
    },
    {
      "epoch": 0.5377273545831083,
      "grad_norm": 0.35717958211898804,
      "learning_rate": 2.468103033220992e-05,
      "loss": 1.312,
      "step": 1493
    },
    {
      "epoch": 0.5380875202593193,
      "grad_norm": 0.3713705837726593,
      "learning_rate": 2.467741935483871e-05,
      "loss": 1.3651,
      "step": 1494
    },
    {
      "epoch": 0.5384476859355304,
      "grad_norm": 0.34009966254234314,
      "learning_rate": 2.4673808377467503e-05,
      "loss": 1.2418,
      "step": 1495
    },
    {
      "epoch": 0.5388078516117414,
      "grad_norm": 0.3458018898963928,
      "learning_rate": 2.4670197400096295e-05,
      "loss": 1.3088,
      "step": 1496
    },
    {
      "epoch": 0.5391680172879525,
      "grad_norm": 0.32749423384666443,
      "learning_rate": 2.4666586422725084e-05,
      "loss": 1.2813,
      "step": 1497
    },
    {
      "epoch": 0.5395281829641635,
      "grad_norm": 0.33556535840034485,
      "learning_rate": 2.4662975445353877e-05,
      "loss": 1.2449,
      "step": 1498
    },
    {
      "epoch": 0.5398883486403746,
      "grad_norm": 0.341777503490448,
      "learning_rate": 2.4659364467982666e-05,
      "loss": 1.362,
      "step": 1499
    },
    {
      "epoch": 0.5402485143165856,
      "grad_norm": 0.34768620133399963,
      "learning_rate": 2.4655753490611458e-05,
      "loss": 1.3959,
      "step": 1500
    },
    {
      "epoch": 0.5406086799927967,
      "grad_norm": 0.3365553319454193,
      "learning_rate": 2.4652142513240253e-05,
      "loss": 1.2659,
      "step": 1501
    },
    {
      "epoch": 0.5409688456690077,
      "grad_norm": 0.3569270670413971,
      "learning_rate": 2.4648531535869042e-05,
      "loss": 1.3779,
      "step": 1502
    },
    {
      "epoch": 0.5413290113452188,
      "grad_norm": 0.33996957540512085,
      "learning_rate": 2.4644920558497835e-05,
      "loss": 1.3412,
      "step": 1503
    },
    {
      "epoch": 0.5416891770214298,
      "grad_norm": 0.3372180163860321,
      "learning_rate": 2.4641309581126627e-05,
      "loss": 1.2582,
      "step": 1504
    },
    {
      "epoch": 0.5420493426976409,
      "grad_norm": 0.33334988355636597,
      "learning_rate": 2.4637698603755416e-05,
      "loss": 1.301,
      "step": 1505
    },
    {
      "epoch": 0.5424095083738519,
      "grad_norm": 0.3364456593990326,
      "learning_rate": 2.4634087626384208e-05,
      "loss": 1.3524,
      "step": 1506
    },
    {
      "epoch": 0.542769674050063,
      "grad_norm": 0.3572959899902344,
      "learning_rate": 2.4630476649013e-05,
      "loss": 1.4692,
      "step": 1507
    },
    {
      "epoch": 0.543129839726274,
      "grad_norm": 0.3345791697502136,
      "learning_rate": 2.4626865671641793e-05,
      "loss": 1.2496,
      "step": 1508
    },
    {
      "epoch": 0.5434900054024852,
      "grad_norm": 0.3403545022010803,
      "learning_rate": 2.4623254694270585e-05,
      "loss": 1.4024,
      "step": 1509
    },
    {
      "epoch": 0.5438501710786962,
      "grad_norm": 0.34108033776283264,
      "learning_rate": 2.4619643716899374e-05,
      "loss": 1.2972,
      "step": 1510
    },
    {
      "epoch": 0.5442103367549073,
      "grad_norm": 0.3346208333969116,
      "learning_rate": 2.4616032739528166e-05,
      "loss": 1.2179,
      "step": 1511
    },
    {
      "epoch": 0.5445705024311183,
      "grad_norm": 0.3616740107536316,
      "learning_rate": 2.461242176215696e-05,
      "loss": 1.2775,
      "step": 1512
    },
    {
      "epoch": 0.5449306681073294,
      "grad_norm": 0.3524443805217743,
      "learning_rate": 2.4608810784785747e-05,
      "loss": 1.2945,
      "step": 1513
    },
    {
      "epoch": 0.5452908337835404,
      "grad_norm": 0.3449056148529053,
      "learning_rate": 2.4605199807414543e-05,
      "loss": 1.3713,
      "step": 1514
    },
    {
      "epoch": 0.5456509994597515,
      "grad_norm": 0.3382648229598999,
      "learning_rate": 2.4601588830043332e-05,
      "loss": 1.2541,
      "step": 1515
    },
    {
      "epoch": 0.5460111651359626,
      "grad_norm": 0.3407340347766876,
      "learning_rate": 2.4597977852672124e-05,
      "loss": 1.2841,
      "step": 1516
    },
    {
      "epoch": 0.5463713308121736,
      "grad_norm": 0.34418416023254395,
      "learning_rate": 2.4594366875300917e-05,
      "loss": 1.2921,
      "step": 1517
    },
    {
      "epoch": 0.5467314964883847,
      "grad_norm": 0.34912216663360596,
      "learning_rate": 2.4590755897929706e-05,
      "loss": 1.2968,
      "step": 1518
    },
    {
      "epoch": 0.5470916621645957,
      "grad_norm": 0.3350197374820709,
      "learning_rate": 2.4587144920558498e-05,
      "loss": 1.2771,
      "step": 1519
    },
    {
      "epoch": 0.5474518278408068,
      "grad_norm": 0.3686896562576294,
      "learning_rate": 2.458353394318729e-05,
      "loss": 1.3027,
      "step": 1520
    },
    {
      "epoch": 0.5478119935170178,
      "grad_norm": 0.37399184703826904,
      "learning_rate": 2.4579922965816082e-05,
      "loss": 1.3501,
      "step": 1521
    },
    {
      "epoch": 0.5481721591932289,
      "grad_norm": 0.3822803795337677,
      "learning_rate": 2.4576311988444875e-05,
      "loss": 1.3226,
      "step": 1522
    },
    {
      "epoch": 0.5485323248694399,
      "grad_norm": 0.35867777466773987,
      "learning_rate": 2.4572701011073664e-05,
      "loss": 1.2815,
      "step": 1523
    },
    {
      "epoch": 0.548892490545651,
      "grad_norm": 0.338148832321167,
      "learning_rate": 2.4569090033702456e-05,
      "loss": 1.312,
      "step": 1524
    },
    {
      "epoch": 0.549252656221862,
      "grad_norm": 0.3410825729370117,
      "learning_rate": 2.4565479056331248e-05,
      "loss": 1.3204,
      "step": 1525
    },
    {
      "epoch": 0.5496128218980731,
      "grad_norm": 0.3570464551448822,
      "learning_rate": 2.4561868078960037e-05,
      "loss": 1.3222,
      "step": 1526
    },
    {
      "epoch": 0.5499729875742841,
      "grad_norm": 0.34448227286338806,
      "learning_rate": 2.455825710158883e-05,
      "loss": 1.2199,
      "step": 1527
    },
    {
      "epoch": 0.5503331532504953,
      "grad_norm": 0.3502371609210968,
      "learning_rate": 2.4554646124217622e-05,
      "loss": 1.3139,
      "step": 1528
    },
    {
      "epoch": 0.5506933189267063,
      "grad_norm": 0.33751100301742554,
      "learning_rate": 2.4551035146846414e-05,
      "loss": 1.4162,
      "step": 1529
    },
    {
      "epoch": 0.5510534846029174,
      "grad_norm": 0.3494814932346344,
      "learning_rate": 2.4547424169475206e-05,
      "loss": 1.2722,
      "step": 1530
    },
    {
      "epoch": 0.5514136502791284,
      "grad_norm": 0.3469138741493225,
      "learning_rate": 2.4543813192103995e-05,
      "loss": 1.2048,
      "step": 1531
    },
    {
      "epoch": 0.5517738159553395,
      "grad_norm": 0.36466240882873535,
      "learning_rate": 2.4540202214732788e-05,
      "loss": 1.2356,
      "step": 1532
    },
    {
      "epoch": 0.5521339816315505,
      "grad_norm": 0.34695321321487427,
      "learning_rate": 2.453659123736158e-05,
      "loss": 1.4354,
      "step": 1533
    },
    {
      "epoch": 0.5524941473077616,
      "grad_norm": 0.3538179099559784,
      "learning_rate": 2.4532980259990372e-05,
      "loss": 1.2143,
      "step": 1534
    },
    {
      "epoch": 0.5528543129839726,
      "grad_norm": 0.3475179374217987,
      "learning_rate": 2.4529369282619164e-05,
      "loss": 1.2889,
      "step": 1535
    },
    {
      "epoch": 0.5532144786601837,
      "grad_norm": 0.35695189237594604,
      "learning_rate": 2.4525758305247953e-05,
      "loss": 1.2114,
      "step": 1536
    },
    {
      "epoch": 0.5535746443363947,
      "grad_norm": 0.3397032916545868,
      "learning_rate": 2.4522147327876746e-05,
      "loss": 1.1807,
      "step": 1537
    },
    {
      "epoch": 0.5539348100126058,
      "grad_norm": 0.35576131939888,
      "learning_rate": 2.4518536350505538e-05,
      "loss": 1.2512,
      "step": 1538
    },
    {
      "epoch": 0.5542949756888168,
      "grad_norm": 0.3335443437099457,
      "learning_rate": 2.4514925373134327e-05,
      "loss": 1.3622,
      "step": 1539
    },
    {
      "epoch": 0.5546551413650279,
      "grad_norm": 0.36440086364746094,
      "learning_rate": 2.451131439576312e-05,
      "loss": 1.2785,
      "step": 1540
    },
    {
      "epoch": 0.5550153070412389,
      "grad_norm": 0.34404006600379944,
      "learning_rate": 2.4507703418391915e-05,
      "loss": 1.3991,
      "step": 1541
    },
    {
      "epoch": 0.55537547271745,
      "grad_norm": 0.3362714946269989,
      "learning_rate": 2.4504092441020704e-05,
      "loss": 1.1404,
      "step": 1542
    },
    {
      "epoch": 0.5557356383936611,
      "grad_norm": 0.34696173667907715,
      "learning_rate": 2.4500481463649496e-05,
      "loss": 1.2791,
      "step": 1543
    },
    {
      "epoch": 0.5560958040698721,
      "grad_norm": 0.3581378757953644,
      "learning_rate": 2.4496870486278285e-05,
      "loss": 1.2235,
      "step": 1544
    },
    {
      "epoch": 0.5564559697460832,
      "grad_norm": 0.343138724565506,
      "learning_rate": 2.4493259508907077e-05,
      "loss": 1.2728,
      "step": 1545
    },
    {
      "epoch": 0.5568161354222942,
      "grad_norm": 0.34470126032829285,
      "learning_rate": 2.448964853153587e-05,
      "loss": 1.2672,
      "step": 1546
    },
    {
      "epoch": 0.5571763010985054,
      "grad_norm": 0.3519738018512726,
      "learning_rate": 2.448603755416466e-05,
      "loss": 1.2657,
      "step": 1547
    },
    {
      "epoch": 0.5575364667747164,
      "grad_norm": 0.3465662896633148,
      "learning_rate": 2.4482426576793454e-05,
      "loss": 1.3401,
      "step": 1548
    },
    {
      "epoch": 0.5578966324509275,
      "grad_norm": 0.341247022151947,
      "learning_rate": 2.4478815599422246e-05,
      "loss": 1.2582,
      "step": 1549
    },
    {
      "epoch": 0.5582567981271385,
      "grad_norm": 0.3367891013622284,
      "learning_rate": 2.4475204622051035e-05,
      "loss": 1.287,
      "step": 1550
    },
    {
      "epoch": 0.5586169638033496,
      "grad_norm": 0.3435956835746765,
      "learning_rate": 2.4471593644679828e-05,
      "loss": 1.2218,
      "step": 1551
    },
    {
      "epoch": 0.5589771294795606,
      "grad_norm": 0.34470322728157043,
      "learning_rate": 2.4467982667308617e-05,
      "loss": 1.2964,
      "step": 1552
    },
    {
      "epoch": 0.5593372951557717,
      "grad_norm": 0.3508698344230652,
      "learning_rate": 2.446437168993741e-05,
      "loss": 1.3433,
      "step": 1553
    },
    {
      "epoch": 0.5596974608319827,
      "grad_norm": 0.3346918523311615,
      "learning_rate": 2.44607607125662e-05,
      "loss": 1.2272,
      "step": 1554
    },
    {
      "epoch": 0.5600576265081938,
      "grad_norm": 0.33912035822868347,
      "learning_rate": 2.4457149735194993e-05,
      "loss": 1.4454,
      "step": 1555
    },
    {
      "epoch": 0.5604177921844048,
      "grad_norm": 0.34971609711647034,
      "learning_rate": 2.4453538757823786e-05,
      "loss": 1.4114,
      "step": 1556
    },
    {
      "epoch": 0.5607779578606159,
      "grad_norm": 0.3609512448310852,
      "learning_rate": 2.4449927780452578e-05,
      "loss": 1.3159,
      "step": 1557
    },
    {
      "epoch": 0.5611381235368269,
      "grad_norm": 0.3413808345794678,
      "learning_rate": 2.4446316803081367e-05,
      "loss": 1.2923,
      "step": 1558
    },
    {
      "epoch": 0.561498289213038,
      "grad_norm": 0.3378642797470093,
      "learning_rate": 2.444270582571016e-05,
      "loss": 1.3669,
      "step": 1559
    },
    {
      "epoch": 0.561858454889249,
      "grad_norm": 0.326430082321167,
      "learning_rate": 2.4439094848338948e-05,
      "loss": 1.1767,
      "step": 1560
    },
    {
      "epoch": 0.5622186205654601,
      "grad_norm": 0.36164426803588867,
      "learning_rate": 2.4435483870967744e-05,
      "loss": 1.2965,
      "step": 1561
    },
    {
      "epoch": 0.5625787862416711,
      "grad_norm": 0.3441455066204071,
      "learning_rate": 2.4431872893596536e-05,
      "loss": 1.3429,
      "step": 1562
    },
    {
      "epoch": 0.5629389519178822,
      "grad_norm": 0.3496795892715454,
      "learning_rate": 2.4428261916225325e-05,
      "loss": 1.3856,
      "step": 1563
    },
    {
      "epoch": 0.5632991175940932,
      "grad_norm": 0.3309950828552246,
      "learning_rate": 2.4424650938854117e-05,
      "loss": 1.2086,
      "step": 1564
    },
    {
      "epoch": 0.5636592832703043,
      "grad_norm": 0.3403387665748596,
      "learning_rate": 2.442103996148291e-05,
      "loss": 1.2166,
      "step": 1565
    },
    {
      "epoch": 0.5640194489465153,
      "grad_norm": 0.34665173292160034,
      "learning_rate": 2.44174289841117e-05,
      "loss": 1.2299,
      "step": 1566
    },
    {
      "epoch": 0.5643796146227265,
      "grad_norm": 0.3411211669445038,
      "learning_rate": 2.441381800674049e-05,
      "loss": 1.2563,
      "step": 1567
    },
    {
      "epoch": 0.5647397802989376,
      "grad_norm": 0.3503448963165283,
      "learning_rate": 2.4410207029369283e-05,
      "loss": 1.2836,
      "step": 1568
    },
    {
      "epoch": 0.5650999459751486,
      "grad_norm": 0.3451099097728729,
      "learning_rate": 2.4406596051998075e-05,
      "loss": 1.2562,
      "step": 1569
    },
    {
      "epoch": 0.5654601116513597,
      "grad_norm": 0.36424311995506287,
      "learning_rate": 2.4402985074626868e-05,
      "loss": 1.2613,
      "step": 1570
    },
    {
      "epoch": 0.5658202773275707,
      "grad_norm": 0.34368202090263367,
      "learning_rate": 2.4399374097255657e-05,
      "loss": 1.459,
      "step": 1571
    },
    {
      "epoch": 0.5661804430037818,
      "grad_norm": 0.3410407602787018,
      "learning_rate": 2.439576311988445e-05,
      "loss": 1.3581,
      "step": 1572
    },
    {
      "epoch": 0.5665406086799928,
      "grad_norm": 0.35558173060417175,
      "learning_rate": 2.439215214251324e-05,
      "loss": 1.2243,
      "step": 1573
    },
    {
      "epoch": 0.5669007743562039,
      "grad_norm": 0.34093186259269714,
      "learning_rate": 2.438854116514203e-05,
      "loss": 1.3305,
      "step": 1574
    },
    {
      "epoch": 0.5672609400324149,
      "grad_norm": 0.3489319682121277,
      "learning_rate": 2.4384930187770826e-05,
      "loss": 1.3905,
      "step": 1575
    },
    {
      "epoch": 0.567621105708626,
      "grad_norm": 0.3701693117618561,
      "learning_rate": 2.4381319210399615e-05,
      "loss": 1.2899,
      "step": 1576
    },
    {
      "epoch": 0.567981271384837,
      "grad_norm": 0.36302703619003296,
      "learning_rate": 2.4377708233028407e-05,
      "loss": 1.4215,
      "step": 1577
    },
    {
      "epoch": 0.5683414370610481,
      "grad_norm": 0.3447285294532776,
      "learning_rate": 2.43740972556572e-05,
      "loss": 1.3103,
      "step": 1578
    },
    {
      "epoch": 0.5687016027372591,
      "grad_norm": 0.3438578248023987,
      "learning_rate": 2.4370486278285988e-05,
      "loss": 1.1991,
      "step": 1579
    },
    {
      "epoch": 0.5690617684134702,
      "grad_norm": 0.3229740560054779,
      "learning_rate": 2.436687530091478e-05,
      "loss": 1.2931,
      "step": 1580
    },
    {
      "epoch": 0.5694219340896812,
      "grad_norm": 0.34623342752456665,
      "learning_rate": 2.4363264323543573e-05,
      "loss": 1.1944,
      "step": 1581
    },
    {
      "epoch": 0.5697820997658923,
      "grad_norm": 0.3461451828479767,
      "learning_rate": 2.4359653346172365e-05,
      "loss": 1.4326,
      "step": 1582
    },
    {
      "epoch": 0.5701422654421033,
      "grad_norm": 0.35152268409729004,
      "learning_rate": 2.4356042368801157e-05,
      "loss": 1.376,
      "step": 1583
    },
    {
      "epoch": 0.5705024311183144,
      "grad_norm": 0.35631585121154785,
      "learning_rate": 2.4352431391429946e-05,
      "loss": 1.4434,
      "step": 1584
    },
    {
      "epoch": 0.5708625967945254,
      "grad_norm": 0.3548583686351776,
      "learning_rate": 2.434882041405874e-05,
      "loss": 1.3997,
      "step": 1585
    },
    {
      "epoch": 0.5712227624707366,
      "grad_norm": 0.3556895852088928,
      "learning_rate": 2.434520943668753e-05,
      "loss": 1.2826,
      "step": 1586
    },
    {
      "epoch": 0.5715829281469476,
      "grad_norm": 0.3476201593875885,
      "learning_rate": 2.434159845931632e-05,
      "loss": 1.2828,
      "step": 1587
    },
    {
      "epoch": 0.5719430938231587,
      "grad_norm": 0.3585577607154846,
      "learning_rate": 2.4337987481945116e-05,
      "loss": 1.4004,
      "step": 1588
    },
    {
      "epoch": 0.5723032594993697,
      "grad_norm": 0.3239102065563202,
      "learning_rate": 2.4334376504573908e-05,
      "loss": 1.2235,
      "step": 1589
    },
    {
      "epoch": 0.5726634251755808,
      "grad_norm": 0.3637479543685913,
      "learning_rate": 2.4330765527202697e-05,
      "loss": 1.3237,
      "step": 1590
    },
    {
      "epoch": 0.5730235908517918,
      "grad_norm": 0.3438793122768402,
      "learning_rate": 2.432715454983149e-05,
      "loss": 1.2185,
      "step": 1591
    },
    {
      "epoch": 0.5733837565280029,
      "grad_norm": 0.3437191843986511,
      "learning_rate": 2.4323543572460278e-05,
      "loss": 1.2595,
      "step": 1592
    },
    {
      "epoch": 0.5737439222042139,
      "grad_norm": 0.34513890743255615,
      "learning_rate": 2.431993259508907e-05,
      "loss": 1.3785,
      "step": 1593
    },
    {
      "epoch": 0.574104087880425,
      "grad_norm": 0.36167722940444946,
      "learning_rate": 2.4316321617717863e-05,
      "loss": 1.3463,
      "step": 1594
    },
    {
      "epoch": 0.5744642535566361,
      "grad_norm": 0.351824551820755,
      "learning_rate": 2.4312710640346655e-05,
      "loss": 1.3292,
      "step": 1595
    },
    {
      "epoch": 0.5748244192328471,
      "grad_norm": 0.34313684701919556,
      "learning_rate": 2.4309099662975447e-05,
      "loss": 1.2749,
      "step": 1596
    },
    {
      "epoch": 0.5751845849090582,
      "grad_norm": 0.3426912724971771,
      "learning_rate": 2.430548868560424e-05,
      "loss": 1.3196,
      "step": 1597
    },
    {
      "epoch": 0.5755447505852692,
      "grad_norm": 0.3643777072429657,
      "learning_rate": 2.430187770823303e-05,
      "loss": 1.2757,
      "step": 1598
    },
    {
      "epoch": 0.5759049162614803,
      "grad_norm": 0.34745967388153076,
      "learning_rate": 2.429826673086182e-05,
      "loss": 1.2646,
      "step": 1599
    },
    {
      "epoch": 0.5762650819376913,
      "grad_norm": 0.398009330034256,
      "learning_rate": 2.429465575349061e-05,
      "loss": 1.2927,
      "step": 1600
    },
    {
      "epoch": 0.5766252476139024,
      "grad_norm": 0.3625989556312561,
      "learning_rate": 2.4291044776119402e-05,
      "loss": 1.3563,
      "step": 1601
    },
    {
      "epoch": 0.5769854132901134,
      "grad_norm": 0.3570897877216339,
      "learning_rate": 2.4287433798748198e-05,
      "loss": 1.239,
      "step": 1602
    },
    {
      "epoch": 0.5773455789663245,
      "grad_norm": 0.33701005578041077,
      "learning_rate": 2.4283822821376986e-05,
      "loss": 1.2193,
      "step": 1603
    },
    {
      "epoch": 0.5777057446425355,
      "grad_norm": 0.3584957420825958,
      "learning_rate": 2.428021184400578e-05,
      "loss": 1.4495,
      "step": 1604
    },
    {
      "epoch": 0.5780659103187467,
      "grad_norm": 0.346720814704895,
      "learning_rate": 2.427660086663457e-05,
      "loss": 1.3249,
      "step": 1605
    },
    {
      "epoch": 0.5784260759949577,
      "grad_norm": 0.3675999045372009,
      "learning_rate": 2.427298988926336e-05,
      "loss": 1.3302,
      "step": 1606
    },
    {
      "epoch": 0.5787862416711688,
      "grad_norm": 0.3584272861480713,
      "learning_rate": 2.4269378911892152e-05,
      "loss": 1.2274,
      "step": 1607
    },
    {
      "epoch": 0.5791464073473798,
      "grad_norm": 0.35296526551246643,
      "learning_rate": 2.426576793452094e-05,
      "loss": 1.3099,
      "step": 1608
    },
    {
      "epoch": 0.5795065730235909,
      "grad_norm": 0.3598731458187103,
      "learning_rate": 2.4262156957149737e-05,
      "loss": 1.3936,
      "step": 1609
    },
    {
      "epoch": 0.5798667386998019,
      "grad_norm": 0.3561185896396637,
      "learning_rate": 2.425854597977853e-05,
      "loss": 1.3544,
      "step": 1610
    },
    {
      "epoch": 0.580226904376013,
      "grad_norm": 0.3446803092956543,
      "learning_rate": 2.4254935002407318e-05,
      "loss": 1.2904,
      "step": 1611
    },
    {
      "epoch": 0.580587070052224,
      "grad_norm": 0.33745792508125305,
      "learning_rate": 2.425132402503611e-05,
      "loss": 1.3246,
      "step": 1612
    },
    {
      "epoch": 0.5809472357284351,
      "grad_norm": 0.3647134006023407,
      "learning_rate": 2.4247713047664903e-05,
      "loss": 1.375,
      "step": 1613
    },
    {
      "epoch": 0.5813074014046461,
      "grad_norm": 0.34896981716156006,
      "learning_rate": 2.424410207029369e-05,
      "loss": 1.3911,
      "step": 1614
    },
    {
      "epoch": 0.5816675670808572,
      "grad_norm": 0.3534805476665497,
      "learning_rate": 2.4240491092922487e-05,
      "loss": 1.2982,
      "step": 1615
    },
    {
      "epoch": 0.5820277327570682,
      "grad_norm": 0.3327779769897461,
      "learning_rate": 2.4236880115551276e-05,
      "loss": 1.263,
      "step": 1616
    },
    {
      "epoch": 0.5823878984332793,
      "grad_norm": 0.3276205360889435,
      "learning_rate": 2.423326913818007e-05,
      "loss": 1.3312,
      "step": 1617
    },
    {
      "epoch": 0.5827480641094903,
      "grad_norm": 0.3451772630214691,
      "learning_rate": 2.422965816080886e-05,
      "loss": 1.2966,
      "step": 1618
    },
    {
      "epoch": 0.5831082297857014,
      "grad_norm": 0.3386922776699066,
      "learning_rate": 2.422604718343765e-05,
      "loss": 1.1514,
      "step": 1619
    },
    {
      "epoch": 0.5834683954619124,
      "grad_norm": 0.36002033948898315,
      "learning_rate": 2.4222436206066442e-05,
      "loss": 1.4292,
      "step": 1620
    },
    {
      "epoch": 0.5838285611381235,
      "grad_norm": 0.34600400924682617,
      "learning_rate": 2.4218825228695234e-05,
      "loss": 1.2646,
      "step": 1621
    },
    {
      "epoch": 0.5841887268143346,
      "grad_norm": 0.41337043046951294,
      "learning_rate": 2.4215214251324026e-05,
      "loss": 1.5175,
      "step": 1622
    },
    {
      "epoch": 0.5845488924905456,
      "grad_norm": 0.37759003043174744,
      "learning_rate": 2.421160327395282e-05,
      "loss": 1.3844,
      "step": 1623
    },
    {
      "epoch": 0.5849090581667568,
      "grad_norm": 0.3452535569667816,
      "learning_rate": 2.4207992296581608e-05,
      "loss": 1.1305,
      "step": 1624
    },
    {
      "epoch": 0.5852692238429678,
      "grad_norm": 0.3516267240047455,
      "learning_rate": 2.42043813192104e-05,
      "loss": 1.3009,
      "step": 1625
    },
    {
      "epoch": 0.5856293895191789,
      "grad_norm": 0.34668099880218506,
      "learning_rate": 2.4200770341839192e-05,
      "loss": 1.224,
      "step": 1626
    },
    {
      "epoch": 0.5859895551953899,
      "grad_norm": 0.346225380897522,
      "learning_rate": 2.419715936446798e-05,
      "loss": 1.3175,
      "step": 1627
    },
    {
      "epoch": 0.586349720871601,
      "grad_norm": 0.35942304134368896,
      "learning_rate": 2.4193548387096773e-05,
      "loss": 1.2716,
      "step": 1628
    },
    {
      "epoch": 0.586709886547812,
      "grad_norm": 0.3585064709186554,
      "learning_rate": 2.418993740972557e-05,
      "loss": 1.2643,
      "step": 1629
    },
    {
      "epoch": 0.5870700522240231,
      "grad_norm": 0.3499119281768799,
      "learning_rate": 2.4186326432354358e-05,
      "loss": 1.2912,
      "step": 1630
    },
    {
      "epoch": 0.5874302179002341,
      "grad_norm": 0.33556780219078064,
      "learning_rate": 2.418271545498315e-05,
      "loss": 1.251,
      "step": 1631
    },
    {
      "epoch": 0.5877903835764452,
      "grad_norm": 0.3508472144603729,
      "learning_rate": 2.417910447761194e-05,
      "loss": 1.3005,
      "step": 1632
    },
    {
      "epoch": 0.5881505492526562,
      "grad_norm": 0.35247883200645447,
      "learning_rate": 2.417549350024073e-05,
      "loss": 1.3411,
      "step": 1633
    },
    {
      "epoch": 0.5885107149288673,
      "grad_norm": 0.36010050773620605,
      "learning_rate": 2.4171882522869524e-05,
      "loss": 1.3442,
      "step": 1634
    },
    {
      "epoch": 0.5888708806050783,
      "grad_norm": 0.3575507700443268,
      "learning_rate": 2.4168271545498313e-05,
      "loss": 1.2644,
      "step": 1635
    },
    {
      "epoch": 0.5892310462812894,
      "grad_norm": 0.3565182685852051,
      "learning_rate": 2.416466056812711e-05,
      "loss": 1.4473,
      "step": 1636
    },
    {
      "epoch": 0.5895912119575004,
      "grad_norm": 0.362365186214447,
      "learning_rate": 2.41610495907559e-05,
      "loss": 1.451,
      "step": 1637
    },
    {
      "epoch": 0.5899513776337115,
      "grad_norm": 0.32741862535476685,
      "learning_rate": 2.415743861338469e-05,
      "loss": 1.2783,
      "step": 1638
    },
    {
      "epoch": 0.5903115433099225,
      "grad_norm": 0.3583199381828308,
      "learning_rate": 2.4153827636013482e-05,
      "loss": 1.3961,
      "step": 1639
    },
    {
      "epoch": 0.5906717089861336,
      "grad_norm": 0.35397249460220337,
      "learning_rate": 2.415021665864227e-05,
      "loss": 1.3367,
      "step": 1640
    },
    {
      "epoch": 0.5910318746623446,
      "grad_norm": 0.345737487077713,
      "learning_rate": 2.4146605681271063e-05,
      "loss": 1.2008,
      "step": 1641
    },
    {
      "epoch": 0.5913920403385557,
      "grad_norm": 0.3486555516719818,
      "learning_rate": 2.414299470389986e-05,
      "loss": 1.2642,
      "step": 1642
    },
    {
      "epoch": 0.5917522060147667,
      "grad_norm": 0.35425928235054016,
      "learning_rate": 2.4139383726528648e-05,
      "loss": 1.2225,
      "step": 1643
    },
    {
      "epoch": 0.5921123716909779,
      "grad_norm": 0.3511297106742859,
      "learning_rate": 2.413577274915744e-05,
      "loss": 1.4341,
      "step": 1644
    },
    {
      "epoch": 0.5924725373671889,
      "grad_norm": 0.3581891655921936,
      "learning_rate": 2.4132161771786232e-05,
      "loss": 1.4367,
      "step": 1645
    },
    {
      "epoch": 0.5928327030434,
      "grad_norm": 0.33307549357414246,
      "learning_rate": 2.412855079441502e-05,
      "loss": 1.245,
      "step": 1646
    },
    {
      "epoch": 0.593192868719611,
      "grad_norm": 0.3425598740577698,
      "learning_rate": 2.4124939817043814e-05,
      "loss": 1.22,
      "step": 1647
    },
    {
      "epoch": 0.5935530343958221,
      "grad_norm": 0.3594173192977905,
      "learning_rate": 2.4121328839672602e-05,
      "loss": 1.4782,
      "step": 1648
    },
    {
      "epoch": 0.5939132000720332,
      "grad_norm": 0.34755614399909973,
      "learning_rate": 2.4117717862301398e-05,
      "loss": 1.2057,
      "step": 1649
    },
    {
      "epoch": 0.5942733657482442,
      "grad_norm": 0.338238924741745,
      "learning_rate": 2.411410688493019e-05,
      "loss": 1.2691,
      "step": 1650
    },
    {
      "epoch": 0.5946335314244553,
      "grad_norm": 0.35618358850479126,
      "learning_rate": 2.411049590755898e-05,
      "loss": 1.2346,
      "step": 1651
    },
    {
      "epoch": 0.5949936971006663,
      "grad_norm": 0.37157294154167175,
      "learning_rate": 2.410688493018777e-05,
      "loss": 1.3822,
      "step": 1652
    },
    {
      "epoch": 0.5953538627768774,
      "grad_norm": 0.3547493517398834,
      "learning_rate": 2.4103273952816564e-05,
      "loss": 1.2681,
      "step": 1653
    },
    {
      "epoch": 0.5957140284530884,
      "grad_norm": 0.3768921196460724,
      "learning_rate": 2.4099662975445353e-05,
      "loss": 1.3063,
      "step": 1654
    },
    {
      "epoch": 0.5960741941292995,
      "grad_norm": 0.364222913980484,
      "learning_rate": 2.4096051998074145e-05,
      "loss": 1.2044,
      "step": 1655
    },
    {
      "epoch": 0.5964343598055105,
      "grad_norm": 0.3602021634578705,
      "learning_rate": 2.4092441020702937e-05,
      "loss": 1.2825,
      "step": 1656
    },
    {
      "epoch": 0.5967945254817216,
      "grad_norm": 0.34487184882164,
      "learning_rate": 2.408883004333173e-05,
      "loss": 1.2689,
      "step": 1657
    },
    {
      "epoch": 0.5971546911579326,
      "grad_norm": 0.372641384601593,
      "learning_rate": 2.4085219065960522e-05,
      "loss": 1.3343,
      "step": 1658
    },
    {
      "epoch": 0.5975148568341437,
      "grad_norm": 0.3521803617477417,
      "learning_rate": 2.408160808858931e-05,
      "loss": 1.2532,
      "step": 1659
    },
    {
      "epoch": 0.5978750225103547,
      "grad_norm": 0.3468974530696869,
      "learning_rate": 2.4077997111218103e-05,
      "loss": 1.226,
      "step": 1660
    },
    {
      "epoch": 0.5982351881865658,
      "grad_norm": 0.37270310521125793,
      "learning_rate": 2.4074386133846896e-05,
      "loss": 1.3919,
      "step": 1661
    },
    {
      "epoch": 0.5985953538627768,
      "grad_norm": 0.34442809224128723,
      "learning_rate": 2.4070775156475684e-05,
      "loss": 1.2669,
      "step": 1662
    },
    {
      "epoch": 0.598955519538988,
      "grad_norm": 0.3564579486846924,
      "learning_rate": 2.406716417910448e-05,
      "loss": 1.3293,
      "step": 1663
    },
    {
      "epoch": 0.599315685215199,
      "grad_norm": 0.3579654097557068,
      "learning_rate": 2.406355320173327e-05,
      "loss": 1.2434,
      "step": 1664
    },
    {
      "epoch": 0.5996758508914101,
      "grad_norm": 0.360688179731369,
      "learning_rate": 2.405994222436206e-05,
      "loss": 1.3363,
      "step": 1665
    },
    {
      "epoch": 0.6000360165676211,
      "grad_norm": 0.356904000043869,
      "learning_rate": 2.4056331246990854e-05,
      "loss": 1.3967,
      "step": 1666
    },
    {
      "epoch": 0.6003961822438322,
      "grad_norm": 0.3501134514808655,
      "learning_rate": 2.4052720269619643e-05,
      "loss": 1.3484,
      "step": 1667
    },
    {
      "epoch": 0.6007563479200432,
      "grad_norm": 0.3463268578052521,
      "learning_rate": 2.4049109292248435e-05,
      "loss": 1.3389,
      "step": 1668
    },
    {
      "epoch": 0.6011165135962543,
      "grad_norm": 0.37606608867645264,
      "learning_rate": 2.404549831487723e-05,
      "loss": 1.3468,
      "step": 1669
    },
    {
      "epoch": 0.6014766792724653,
      "grad_norm": 0.35959118604660034,
      "learning_rate": 2.404188733750602e-05,
      "loss": 1.3834,
      "step": 1670
    },
    {
      "epoch": 0.6018368449486764,
      "grad_norm": 0.3447342813014984,
      "learning_rate": 2.4038276360134812e-05,
      "loss": 1.1992,
      "step": 1671
    },
    {
      "epoch": 0.6021970106248874,
      "grad_norm": 0.36289310455322266,
      "learning_rate": 2.40346653827636e-05,
      "loss": 1.2525,
      "step": 1672
    },
    {
      "epoch": 0.6025571763010985,
      "grad_norm": 0.35058823227882385,
      "learning_rate": 2.4031054405392393e-05,
      "loss": 1.2452,
      "step": 1673
    },
    {
      "epoch": 0.6029173419773096,
      "grad_norm": 0.3755921721458435,
      "learning_rate": 2.4027443428021185e-05,
      "loss": 1.3033,
      "step": 1674
    },
    {
      "epoch": 0.6032775076535206,
      "grad_norm": 0.3496679663658142,
      "learning_rate": 2.4023832450649974e-05,
      "loss": 1.2303,
      "step": 1675
    },
    {
      "epoch": 0.6036376733297317,
      "grad_norm": 0.34754636883735657,
      "learning_rate": 2.402022147327877e-05,
      "loss": 1.2581,
      "step": 1676
    },
    {
      "epoch": 0.6039978390059427,
      "grad_norm": 0.33644530177116394,
      "learning_rate": 2.4016610495907562e-05,
      "loss": 1.2279,
      "step": 1677
    },
    {
      "epoch": 0.6043580046821538,
      "grad_norm": 0.33965003490448,
      "learning_rate": 2.401299951853635e-05,
      "loss": 1.2103,
      "step": 1678
    },
    {
      "epoch": 0.6047181703583648,
      "grad_norm": 0.38130372762680054,
      "learning_rate": 2.4009388541165143e-05,
      "loss": 1.3172,
      "step": 1679
    },
    {
      "epoch": 0.605078336034576,
      "grad_norm": 0.3701879680156708,
      "learning_rate": 2.4005777563793932e-05,
      "loss": 1.3805,
      "step": 1680
    },
    {
      "epoch": 0.605438501710787,
      "grad_norm": 0.3574804961681366,
      "learning_rate": 2.4002166586422725e-05,
      "loss": 1.3718,
      "step": 1681
    },
    {
      "epoch": 0.6057986673869981,
      "grad_norm": 0.33249565958976746,
      "learning_rate": 2.3998555609051517e-05,
      "loss": 1.0956,
      "step": 1682
    },
    {
      "epoch": 0.6061588330632091,
      "grad_norm": 0.34602758288383484,
      "learning_rate": 2.399494463168031e-05,
      "loss": 1.2803,
      "step": 1683
    },
    {
      "epoch": 0.6065189987394202,
      "grad_norm": 0.3609922230243683,
      "learning_rate": 2.39913336543091e-05,
      "loss": 1.4669,
      "step": 1684
    },
    {
      "epoch": 0.6068791644156312,
      "grad_norm": 0.35332679748535156,
      "learning_rate": 2.3987722676937894e-05,
      "loss": 1.3827,
      "step": 1685
    },
    {
      "epoch": 0.6072393300918423,
      "grad_norm": 0.3616577088832855,
      "learning_rate": 2.3984111699566683e-05,
      "loss": 1.3346,
      "step": 1686
    },
    {
      "epoch": 0.6075994957680533,
      "grad_norm": 0.3426607549190521,
      "learning_rate": 2.3980500722195475e-05,
      "loss": 1.2623,
      "step": 1687
    },
    {
      "epoch": 0.6079596614442644,
      "grad_norm": 0.36215388774871826,
      "learning_rate": 2.3976889744824264e-05,
      "loss": 1.4985,
      "step": 1688
    },
    {
      "epoch": 0.6083198271204754,
      "grad_norm": 0.34517472982406616,
      "learning_rate": 2.3973278767453056e-05,
      "loss": 1.3311,
      "step": 1689
    },
    {
      "epoch": 0.6086799927966865,
      "grad_norm": 0.3510660231113434,
      "learning_rate": 2.3969667790081852e-05,
      "loss": 1.3158,
      "step": 1690
    },
    {
      "epoch": 0.6090401584728975,
      "grad_norm": 0.34382352232933044,
      "learning_rate": 2.396605681271064e-05,
      "loss": 1.2495,
      "step": 1691
    },
    {
      "epoch": 0.6094003241491086,
      "grad_norm": 0.352533757686615,
      "learning_rate": 2.3962445835339433e-05,
      "loss": 1.5496,
      "step": 1692
    },
    {
      "epoch": 0.6097604898253196,
      "grad_norm": 0.34197333455085754,
      "learning_rate": 2.3958834857968225e-05,
      "loss": 1.3443,
      "step": 1693
    },
    {
      "epoch": 0.6101206555015307,
      "grad_norm": 0.3528222441673279,
      "learning_rate": 2.3955223880597014e-05,
      "loss": 1.2189,
      "step": 1694
    },
    {
      "epoch": 0.6104808211777417,
      "grad_norm": 0.3615756332874298,
      "learning_rate": 2.3951612903225807e-05,
      "loss": 1.3237,
      "step": 1695
    },
    {
      "epoch": 0.6108409868539528,
      "grad_norm": 0.34230169653892517,
      "learning_rate": 2.39480019258546e-05,
      "loss": 1.3344,
      "step": 1696
    },
    {
      "epoch": 0.6112011525301638,
      "grad_norm": 0.3786204755306244,
      "learning_rate": 2.394439094848339e-05,
      "loss": 1.3467,
      "step": 1697
    },
    {
      "epoch": 0.6115613182063749,
      "grad_norm": 0.334829717874527,
      "learning_rate": 2.3940779971112183e-05,
      "loss": 1.2559,
      "step": 1698
    },
    {
      "epoch": 0.6119214838825859,
      "grad_norm": 0.34843817353248596,
      "learning_rate": 2.3937168993740972e-05,
      "loss": 1.2147,
      "step": 1699
    },
    {
      "epoch": 0.612281649558797,
      "grad_norm": 0.3498722016811371,
      "learning_rate": 2.3933558016369765e-05,
      "loss": 1.2577,
      "step": 1700
    },
    {
      "epoch": 0.6126418152350082,
      "grad_norm": 0.33589375019073486,
      "learning_rate": 2.3929947038998557e-05,
      "loss": 1.2811,
      "step": 1701
    },
    {
      "epoch": 0.6130019809112192,
      "grad_norm": 0.3428158760070801,
      "learning_rate": 2.3926336061627346e-05,
      "loss": 1.2025,
      "step": 1702
    },
    {
      "epoch": 0.6133621465874303,
      "grad_norm": 0.34210145473480225,
      "learning_rate": 2.392272508425614e-05,
      "loss": 1.1934,
      "step": 1703
    },
    {
      "epoch": 0.6137223122636413,
      "grad_norm": 0.36411353945732117,
      "learning_rate": 2.391911410688493e-05,
      "loss": 1.2072,
      "step": 1704
    },
    {
      "epoch": 0.6140824779398524,
      "grad_norm": 0.3435978293418884,
      "learning_rate": 2.3915503129513723e-05,
      "loss": 1.1467,
      "step": 1705
    },
    {
      "epoch": 0.6144426436160634,
      "grad_norm": 0.3535444438457489,
      "learning_rate": 2.3911892152142515e-05,
      "loss": 1.3021,
      "step": 1706
    },
    {
      "epoch": 0.6148028092922745,
      "grad_norm": 0.35932543873786926,
      "learning_rate": 2.3908281174771304e-05,
      "loss": 1.305,
      "step": 1707
    },
    {
      "epoch": 0.6151629749684855,
      "grad_norm": 0.39446476101875305,
      "learning_rate": 2.3904670197400096e-05,
      "loss": 1.4296,
      "step": 1708
    },
    {
      "epoch": 0.6155231406446966,
      "grad_norm": 0.3459685742855072,
      "learning_rate": 2.390105922002889e-05,
      "loss": 1.2017,
      "step": 1709
    },
    {
      "epoch": 0.6158833063209076,
      "grad_norm": 0.3533121645450592,
      "learning_rate": 2.389744824265768e-05,
      "loss": 1.4273,
      "step": 1710
    },
    {
      "epoch": 0.6162434719971187,
      "grad_norm": 0.3551533818244934,
      "learning_rate": 2.3893837265286473e-05,
      "loss": 1.3203,
      "step": 1711
    },
    {
      "epoch": 0.6166036376733297,
      "grad_norm": 0.34965524077415466,
      "learning_rate": 2.3890226287915262e-05,
      "loss": 1.2916,
      "step": 1712
    },
    {
      "epoch": 0.6169638033495408,
      "grad_norm": 0.3383173942565918,
      "learning_rate": 2.3886615310544054e-05,
      "loss": 1.3341,
      "step": 1713
    },
    {
      "epoch": 0.6173239690257518,
      "grad_norm": 0.3600321114063263,
      "learning_rate": 2.3883004333172847e-05,
      "loss": 1.2623,
      "step": 1714
    },
    {
      "epoch": 0.6176841347019629,
      "grad_norm": 0.3650067150592804,
      "learning_rate": 2.3879393355801636e-05,
      "loss": 1.3242,
      "step": 1715
    },
    {
      "epoch": 0.6180443003781739,
      "grad_norm": 0.3607775568962097,
      "learning_rate": 2.387578237843043e-05,
      "loss": 1.2382,
      "step": 1716
    },
    {
      "epoch": 0.618404466054385,
      "grad_norm": 0.3601376414299011,
      "learning_rate": 2.387217140105922e-05,
      "loss": 1.3602,
      "step": 1717
    },
    {
      "epoch": 0.618764631730596,
      "grad_norm": 0.3527119755744934,
      "learning_rate": 2.3868560423688012e-05,
      "loss": 1.1815,
      "step": 1718
    },
    {
      "epoch": 0.6191247974068071,
      "grad_norm": 0.34832215309143066,
      "learning_rate": 2.3864949446316805e-05,
      "loss": 1.2292,
      "step": 1719
    },
    {
      "epoch": 0.6194849630830181,
      "grad_norm": 0.34192895889282227,
      "learning_rate": 2.3861338468945594e-05,
      "loss": 1.2922,
      "step": 1720
    },
    {
      "epoch": 0.6198451287592293,
      "grad_norm": 0.3363712430000305,
      "learning_rate": 2.3857727491574386e-05,
      "loss": 1.2595,
      "step": 1721
    },
    {
      "epoch": 0.6202052944354403,
      "grad_norm": 0.3690275251865387,
      "learning_rate": 2.3854116514203178e-05,
      "loss": 1.3948,
      "step": 1722
    },
    {
      "epoch": 0.6205654601116514,
      "grad_norm": 0.3700424134731293,
      "learning_rate": 2.385050553683197e-05,
      "loss": 1.358,
      "step": 1723
    },
    {
      "epoch": 0.6209256257878624,
      "grad_norm": 0.3525022566318512,
      "learning_rate": 2.3846894559460763e-05,
      "loss": 1.2073,
      "step": 1724
    },
    {
      "epoch": 0.6212857914640735,
      "grad_norm": 0.3379858732223511,
      "learning_rate": 2.3843283582089552e-05,
      "loss": 1.1576,
      "step": 1725
    },
    {
      "epoch": 0.6216459571402845,
      "grad_norm": 0.3541562259197235,
      "learning_rate": 2.3839672604718344e-05,
      "loss": 1.3127,
      "step": 1726
    },
    {
      "epoch": 0.6220061228164956,
      "grad_norm": 0.3433208167552948,
      "learning_rate": 2.3836061627347136e-05,
      "loss": 1.2836,
      "step": 1727
    },
    {
      "epoch": 0.6223662884927067,
      "grad_norm": 0.3684856593608856,
      "learning_rate": 2.3832450649975925e-05,
      "loss": 1.4718,
      "step": 1728
    },
    {
      "epoch": 0.6227264541689177,
      "grad_norm": 0.35283294320106506,
      "learning_rate": 2.3828839672604718e-05,
      "loss": 1.268,
      "step": 1729
    },
    {
      "epoch": 0.6230866198451288,
      "grad_norm": 0.33757516741752625,
      "learning_rate": 2.3825228695233513e-05,
      "loss": 1.2777,
      "step": 1730
    },
    {
      "epoch": 0.6234467855213398,
      "grad_norm": 0.353023499250412,
      "learning_rate": 2.3821617717862302e-05,
      "loss": 1.3657,
      "step": 1731
    },
    {
      "epoch": 0.6238069511975509,
      "grad_norm": 0.3549005687236786,
      "learning_rate": 2.3818006740491094e-05,
      "loss": 1.1782,
      "step": 1732
    },
    {
      "epoch": 0.6241671168737619,
      "grad_norm": 0.34367895126342773,
      "learning_rate": 2.3814395763119883e-05,
      "loss": 1.3439,
      "step": 1733
    },
    {
      "epoch": 0.624527282549973,
      "grad_norm": 0.3544442355632782,
      "learning_rate": 2.3810784785748676e-05,
      "loss": 1.3931,
      "step": 1734
    },
    {
      "epoch": 0.624887448226184,
      "grad_norm": 0.3700765073299408,
      "learning_rate": 2.3807173808377468e-05,
      "loss": 1.3695,
      "step": 1735
    },
    {
      "epoch": 0.6252476139023951,
      "grad_norm": 0.3505654036998749,
      "learning_rate": 2.3803562831006257e-05,
      "loss": 1.2897,
      "step": 1736
    },
    {
      "epoch": 0.6256077795786061,
      "grad_norm": 0.3594145178794861,
      "learning_rate": 2.3799951853635052e-05,
      "loss": 1.3698,
      "step": 1737
    },
    {
      "epoch": 0.6259679452548172,
      "grad_norm": 0.3528752624988556,
      "learning_rate": 2.3796340876263845e-05,
      "loss": 1.366,
      "step": 1738
    },
    {
      "epoch": 0.6263281109310282,
      "grad_norm": 0.36710450053215027,
      "learning_rate": 2.3792729898892634e-05,
      "loss": 1.3127,
      "step": 1739
    },
    {
      "epoch": 0.6266882766072394,
      "grad_norm": 0.3564157783985138,
      "learning_rate": 2.3789118921521426e-05,
      "loss": 1.1965,
      "step": 1740
    },
    {
      "epoch": 0.6270484422834504,
      "grad_norm": 0.3518044352531433,
      "learning_rate": 2.3785507944150215e-05,
      "loss": 1.4042,
      "step": 1741
    },
    {
      "epoch": 0.6274086079596615,
      "grad_norm": 0.3495400846004486,
      "learning_rate": 2.3781896966779007e-05,
      "loss": 1.2872,
      "step": 1742
    },
    {
      "epoch": 0.6277687736358725,
      "grad_norm": 0.3503367006778717,
      "learning_rate": 2.3778285989407803e-05,
      "loss": 1.2658,
      "step": 1743
    },
    {
      "epoch": 0.6281289393120836,
      "grad_norm": 0.3576328158378601,
      "learning_rate": 2.3774675012036592e-05,
      "loss": 1.4068,
      "step": 1744
    },
    {
      "epoch": 0.6284891049882946,
      "grad_norm": 0.3596547245979309,
      "learning_rate": 2.3771064034665384e-05,
      "loss": 1.3292,
      "step": 1745
    },
    {
      "epoch": 0.6288492706645057,
      "grad_norm": 0.35322850942611694,
      "learning_rate": 2.3767453057294176e-05,
      "loss": 1.3561,
      "step": 1746
    },
    {
      "epoch": 0.6292094363407167,
      "grad_norm": 0.3562548756599426,
      "learning_rate": 2.3763842079922965e-05,
      "loss": 1.1899,
      "step": 1747
    },
    {
      "epoch": 0.6295696020169278,
      "grad_norm": 0.3378382623195648,
      "learning_rate": 2.3760231102551758e-05,
      "loss": 1.2623,
      "step": 1748
    },
    {
      "epoch": 0.6299297676931388,
      "grad_norm": 0.37034282088279724,
      "learning_rate": 2.3756620125180547e-05,
      "loss": 1.1604,
      "step": 1749
    },
    {
      "epoch": 0.6302899333693499,
      "grad_norm": 0.3609876036643982,
      "learning_rate": 2.3753009147809342e-05,
      "loss": 1.283,
      "step": 1750
    },
    {
      "epoch": 0.6306500990455609,
      "grad_norm": 0.34665942192077637,
      "learning_rate": 2.3749398170438134e-05,
      "loss": 1.2919,
      "step": 1751
    },
    {
      "epoch": 0.631010264721772,
      "grad_norm": 0.3605482578277588,
      "learning_rate": 2.3745787193066923e-05,
      "loss": 1.3151,
      "step": 1752
    },
    {
      "epoch": 0.631370430397983,
      "grad_norm": 0.3698008358478546,
      "learning_rate": 2.3742176215695716e-05,
      "loss": 1.3762,
      "step": 1753
    },
    {
      "epoch": 0.6317305960741941,
      "grad_norm": 0.3663010597229004,
      "learning_rate": 2.3738565238324508e-05,
      "loss": 1.3403,
      "step": 1754
    },
    {
      "epoch": 0.6320907617504052,
      "grad_norm": 0.36460140347480774,
      "learning_rate": 2.3734954260953297e-05,
      "loss": 1.1603,
      "step": 1755
    },
    {
      "epoch": 0.6324509274266162,
      "grad_norm": 0.3597002327442169,
      "learning_rate": 2.373134328358209e-05,
      "loss": 1.269,
      "step": 1756
    },
    {
      "epoch": 0.6328110931028273,
      "grad_norm": 0.3587965965270996,
      "learning_rate": 2.372773230621088e-05,
      "loss": 1.3302,
      "step": 1757
    },
    {
      "epoch": 0.6331712587790383,
      "grad_norm": 0.3512524366378784,
      "learning_rate": 2.3724121328839674e-05,
      "loss": 1.4053,
      "step": 1758
    },
    {
      "epoch": 0.6335314244552495,
      "grad_norm": 0.3596741855144501,
      "learning_rate": 2.3720510351468466e-05,
      "loss": 1.235,
      "step": 1759
    },
    {
      "epoch": 0.6338915901314605,
      "grad_norm": 0.3675297498703003,
      "learning_rate": 2.3716899374097255e-05,
      "loss": 1.2472,
      "step": 1760
    },
    {
      "epoch": 0.6342517558076716,
      "grad_norm": 0.360904335975647,
      "learning_rate": 2.3713288396726047e-05,
      "loss": 1.2454,
      "step": 1761
    },
    {
      "epoch": 0.6346119214838826,
      "grad_norm": 0.3457825481891632,
      "learning_rate": 2.370967741935484e-05,
      "loss": 1.2447,
      "step": 1762
    },
    {
      "epoch": 0.6349720871600937,
      "grad_norm": 0.3622892498970032,
      "learning_rate": 2.370606644198363e-05,
      "loss": 1.3099,
      "step": 1763
    },
    {
      "epoch": 0.6353322528363047,
      "grad_norm": 0.379407674074173,
      "learning_rate": 2.3702455464612424e-05,
      "loss": 1.5438,
      "step": 1764
    },
    {
      "epoch": 0.6356924185125158,
      "grad_norm": 0.3481307327747345,
      "learning_rate": 2.3698844487241213e-05,
      "loss": 1.428,
      "step": 1765
    },
    {
      "epoch": 0.6360525841887268,
      "grad_norm": 0.34990546107292175,
      "learning_rate": 2.3695233509870005e-05,
      "loss": 1.2542,
      "step": 1766
    },
    {
      "epoch": 0.6364127498649379,
      "grad_norm": 0.35408279299736023,
      "learning_rate": 2.3691622532498798e-05,
      "loss": 1.1818,
      "step": 1767
    },
    {
      "epoch": 0.6367729155411489,
      "grad_norm": 0.37296271324157715,
      "learning_rate": 2.3688011555127587e-05,
      "loss": 1.3899,
      "step": 1768
    },
    {
      "epoch": 0.63713308121736,
      "grad_norm": 0.3539544343948364,
      "learning_rate": 2.368440057775638e-05,
      "loss": 1.3165,
      "step": 1769
    },
    {
      "epoch": 0.637493246893571,
      "grad_norm": 0.3556601405143738,
      "learning_rate": 2.3680789600385175e-05,
      "loss": 1.3272,
      "step": 1770
    },
    {
      "epoch": 0.6378534125697821,
      "grad_norm": 0.3547074496746063,
      "learning_rate": 2.3677178623013963e-05,
      "loss": 1.2527,
      "step": 1771
    },
    {
      "epoch": 0.6382135782459931,
      "grad_norm": 0.37365642189979553,
      "learning_rate": 2.3673567645642756e-05,
      "loss": 1.3168,
      "step": 1772
    },
    {
      "epoch": 0.6385737439222042,
      "grad_norm": 0.3487473726272583,
      "learning_rate": 2.3669956668271545e-05,
      "loss": 1.401,
      "step": 1773
    },
    {
      "epoch": 0.6389339095984152,
      "grad_norm": 0.3717557489871979,
      "learning_rate": 2.3666345690900337e-05,
      "loss": 1.3236,
      "step": 1774
    },
    {
      "epoch": 0.6392940752746263,
      "grad_norm": 0.36785125732421875,
      "learning_rate": 2.366273471352913e-05,
      "loss": 1.1915,
      "step": 1775
    },
    {
      "epoch": 0.6396542409508373,
      "grad_norm": 0.37212398648262024,
      "learning_rate": 2.3659123736157918e-05,
      "loss": 1.3688,
      "step": 1776
    },
    {
      "epoch": 0.6400144066270484,
      "grad_norm": 0.3611752688884735,
      "learning_rate": 2.3655512758786714e-05,
      "loss": 1.2968,
      "step": 1777
    },
    {
      "epoch": 0.6403745723032594,
      "grad_norm": 0.367401659488678,
      "learning_rate": 2.3651901781415506e-05,
      "loss": 1.3712,
      "step": 1778
    },
    {
      "epoch": 0.6407347379794706,
      "grad_norm": 0.3558686673641205,
      "learning_rate": 2.3648290804044295e-05,
      "loss": 1.3827,
      "step": 1779
    },
    {
      "epoch": 0.6410949036556816,
      "grad_norm": 0.33184877038002014,
      "learning_rate": 2.3644679826673087e-05,
      "loss": 1.2438,
      "step": 1780
    },
    {
      "epoch": 0.6414550693318927,
      "grad_norm": 0.35200971364974976,
      "learning_rate": 2.3641068849301876e-05,
      "loss": 1.4097,
      "step": 1781
    },
    {
      "epoch": 0.6418152350081038,
      "grad_norm": 0.3671383559703827,
      "learning_rate": 2.363745787193067e-05,
      "loss": 1.3112,
      "step": 1782
    },
    {
      "epoch": 0.6421754006843148,
      "grad_norm": 0.3418477177619934,
      "learning_rate": 2.363384689455946e-05,
      "loss": 1.2115,
      "step": 1783
    },
    {
      "epoch": 0.6425355663605259,
      "grad_norm": 0.3667137622833252,
      "learning_rate": 2.3630235917188253e-05,
      "loss": 1.4388,
      "step": 1784
    },
    {
      "epoch": 0.6428957320367369,
      "grad_norm": 0.36176151037216187,
      "learning_rate": 2.3626624939817045e-05,
      "loss": 1.3246,
      "step": 1785
    },
    {
      "epoch": 0.643255897712948,
      "grad_norm": 0.36837640404701233,
      "learning_rate": 2.3623013962445838e-05,
      "loss": 1.3817,
      "step": 1786
    },
    {
      "epoch": 0.643616063389159,
      "grad_norm": 0.35573652386665344,
      "learning_rate": 2.3619402985074627e-05,
      "loss": 1.2317,
      "step": 1787
    },
    {
      "epoch": 0.6439762290653701,
      "grad_norm": 0.3699425160884857,
      "learning_rate": 2.361579200770342e-05,
      "loss": 1.3367,
      "step": 1788
    },
    {
      "epoch": 0.6443363947415811,
      "grad_norm": 0.35428544878959656,
      "learning_rate": 2.3612181030332208e-05,
      "loss": 1.2284,
      "step": 1789
    },
    {
      "epoch": 0.6446965604177922,
      "grad_norm": 0.36395004391670227,
      "learning_rate": 2.3608570052961e-05,
      "loss": 1.4809,
      "step": 1790
    },
    {
      "epoch": 0.6450567260940032,
      "grad_norm": 0.3745136559009552,
      "learning_rate": 2.3604959075589796e-05,
      "loss": 1.3595,
      "step": 1791
    },
    {
      "epoch": 0.6454168917702143,
      "grad_norm": 0.3716009855270386,
      "learning_rate": 2.3601348098218585e-05,
      "loss": 1.3719,
      "step": 1792
    },
    {
      "epoch": 0.6457770574464253,
      "grad_norm": 0.3504883348941803,
      "learning_rate": 2.3597737120847377e-05,
      "loss": 1.3121,
      "step": 1793
    },
    {
      "epoch": 0.6461372231226364,
      "grad_norm": 0.3528915345668793,
      "learning_rate": 2.359412614347617e-05,
      "loss": 1.4166,
      "step": 1794
    },
    {
      "epoch": 0.6464973887988474,
      "grad_norm": 0.35922694206237793,
      "learning_rate": 2.3590515166104958e-05,
      "loss": 1.3455,
      "step": 1795
    },
    {
      "epoch": 0.6468575544750585,
      "grad_norm": 0.3720978796482086,
      "learning_rate": 2.358690418873375e-05,
      "loss": 1.2736,
      "step": 1796
    },
    {
      "epoch": 0.6472177201512695,
      "grad_norm": 0.35157454013824463,
      "learning_rate": 2.3583293211362543e-05,
      "loss": 1.2576,
      "step": 1797
    },
    {
      "epoch": 0.6475778858274807,
      "grad_norm": 0.3563031554222107,
      "learning_rate": 2.3579682233991335e-05,
      "loss": 1.3107,
      "step": 1798
    },
    {
      "epoch": 0.6479380515036917,
      "grad_norm": 0.3555518388748169,
      "learning_rate": 2.3576071256620127e-05,
      "loss": 1.3086,
      "step": 1799
    },
    {
      "epoch": 0.6482982171799028,
      "grad_norm": 0.3931587338447571,
      "learning_rate": 2.3572460279248916e-05,
      "loss": 1.3995,
      "step": 1800
    },
    {
      "epoch": 0.6486583828561138,
      "grad_norm": 0.3710935115814209,
      "learning_rate": 2.356884930187771e-05,
      "loss": 1.2606,
      "step": 1801
    },
    {
      "epoch": 0.6490185485323249,
      "grad_norm": 0.3472088873386383,
      "learning_rate": 2.35652383245065e-05,
      "loss": 1.2804,
      "step": 1802
    },
    {
      "epoch": 0.6493787142085359,
      "grad_norm": 0.3369615375995636,
      "learning_rate": 2.356162734713529e-05,
      "loss": 1.2326,
      "step": 1803
    },
    {
      "epoch": 0.649738879884747,
      "grad_norm": 0.36515843868255615,
      "learning_rate": 2.3558016369764086e-05,
      "loss": 1.3541,
      "step": 1804
    },
    {
      "epoch": 0.650099045560958,
      "grad_norm": 0.3544743061065674,
      "learning_rate": 2.3554405392392874e-05,
      "loss": 1.2704,
      "step": 1805
    },
    {
      "epoch": 0.6504592112371691,
      "grad_norm": 0.37198033928871155,
      "learning_rate": 2.3550794415021667e-05,
      "loss": 1.3152,
      "step": 1806
    },
    {
      "epoch": 0.6508193769133802,
      "grad_norm": 0.3486563265323639,
      "learning_rate": 2.354718343765046e-05,
      "loss": 1.3027,
      "step": 1807
    },
    {
      "epoch": 0.6511795425895912,
      "grad_norm": 0.36740919947624207,
      "learning_rate": 2.3543572460279248e-05,
      "loss": 1.365,
      "step": 1808
    },
    {
      "epoch": 0.6515397082658023,
      "grad_norm": 0.3640877604484558,
      "learning_rate": 2.353996148290804e-05,
      "loss": 1.3448,
      "step": 1809
    },
    {
      "epoch": 0.6518998739420133,
      "grad_norm": 0.3566467761993408,
      "learning_rate": 2.3536350505536833e-05,
      "loss": 1.3255,
      "step": 1810
    },
    {
      "epoch": 0.6522600396182244,
      "grad_norm": 0.35308489203453064,
      "learning_rate": 2.3532739528165625e-05,
      "loss": 1.295,
      "step": 1811
    },
    {
      "epoch": 0.6526202052944354,
      "grad_norm": 0.35284408926963806,
      "learning_rate": 2.3529128550794417e-05,
      "loss": 1.3589,
      "step": 1812
    },
    {
      "epoch": 0.6529803709706465,
      "grad_norm": 0.36271172761917114,
      "learning_rate": 2.3525517573423206e-05,
      "loss": 1.2609,
      "step": 1813
    },
    {
      "epoch": 0.6533405366468575,
      "grad_norm": 0.3581242263317108,
      "learning_rate": 2.3521906596052e-05,
      "loss": 1.3662,
      "step": 1814
    },
    {
      "epoch": 0.6537007023230686,
      "grad_norm": 0.3482634723186493,
      "learning_rate": 2.351829561868079e-05,
      "loss": 1.2819,
      "step": 1815
    },
    {
      "epoch": 0.6540608679992796,
      "grad_norm": 0.3650665879249573,
      "learning_rate": 2.351468464130958e-05,
      "loss": 1.3313,
      "step": 1816
    },
    {
      "epoch": 0.6544210336754908,
      "grad_norm": 0.34926488995552063,
      "learning_rate": 2.3511073663938372e-05,
      "loss": 1.2666,
      "step": 1817
    },
    {
      "epoch": 0.6547811993517018,
      "grad_norm": 0.34864315390586853,
      "learning_rate": 2.3507462686567168e-05,
      "loss": 1.1712,
      "step": 1818
    },
    {
      "epoch": 0.6551413650279129,
      "grad_norm": 0.3584347069263458,
      "learning_rate": 2.3503851709195956e-05,
      "loss": 1.2042,
      "step": 1819
    },
    {
      "epoch": 0.6555015307041239,
      "grad_norm": 0.37743639945983887,
      "learning_rate": 2.350024073182475e-05,
      "loss": 1.4228,
      "step": 1820
    },
    {
      "epoch": 0.655861696380335,
      "grad_norm": 0.352649450302124,
      "learning_rate": 2.3496629754453538e-05,
      "loss": 1.2029,
      "step": 1821
    },
    {
      "epoch": 0.656221862056546,
      "grad_norm": 0.35291022062301636,
      "learning_rate": 2.349301877708233e-05,
      "loss": 1.2052,
      "step": 1822
    },
    {
      "epoch": 0.6565820277327571,
      "grad_norm": 0.3506041169166565,
      "learning_rate": 2.3489407799711122e-05,
      "loss": 1.1933,
      "step": 1823
    },
    {
      "epoch": 0.6569421934089681,
      "grad_norm": 0.3612084984779358,
      "learning_rate": 2.3485796822339915e-05,
      "loss": 1.3003,
      "step": 1824
    },
    {
      "epoch": 0.6573023590851792,
      "grad_norm": 0.35696941614151,
      "learning_rate": 2.3482185844968707e-05,
      "loss": 1.4312,
      "step": 1825
    },
    {
      "epoch": 0.6576625247613902,
      "grad_norm": 0.3505002558231354,
      "learning_rate": 2.34785748675975e-05,
      "loss": 1.3196,
      "step": 1826
    },
    {
      "epoch": 0.6580226904376013,
      "grad_norm": 0.3476603329181671,
      "learning_rate": 2.3474963890226288e-05,
      "loss": 1.2552,
      "step": 1827
    },
    {
      "epoch": 0.6583828561138123,
      "grad_norm": 0.35116031765937805,
      "learning_rate": 2.347135291285508e-05,
      "loss": 1.3129,
      "step": 1828
    },
    {
      "epoch": 0.6587430217900234,
      "grad_norm": 0.36047542095184326,
      "learning_rate": 2.346774193548387e-05,
      "loss": 1.2712,
      "step": 1829
    },
    {
      "epoch": 0.6591031874662344,
      "grad_norm": 0.37843847274780273,
      "learning_rate": 2.346413095811266e-05,
      "loss": 1.3007,
      "step": 1830
    },
    {
      "epoch": 0.6594633531424455,
      "grad_norm": 0.3539141118526459,
      "learning_rate": 2.3460519980741457e-05,
      "loss": 1.3196,
      "step": 1831
    },
    {
      "epoch": 0.6598235188186565,
      "grad_norm": 0.3622496724128723,
      "learning_rate": 2.3456909003370246e-05,
      "loss": 1.3305,
      "step": 1832
    },
    {
      "epoch": 0.6601836844948676,
      "grad_norm": 0.3794552981853485,
      "learning_rate": 2.345329802599904e-05,
      "loss": 1.3568,
      "step": 1833
    },
    {
      "epoch": 0.6605438501710788,
      "grad_norm": 0.3527123034000397,
      "learning_rate": 2.344968704862783e-05,
      "loss": 1.2481,
      "step": 1834
    },
    {
      "epoch": 0.6609040158472897,
      "grad_norm": 0.3728678524494171,
      "learning_rate": 2.344607607125662e-05,
      "loss": 1.4053,
      "step": 1835
    },
    {
      "epoch": 0.6612641815235009,
      "grad_norm": 0.3433701992034912,
      "learning_rate": 2.3442465093885412e-05,
      "loss": 1.3186,
      "step": 1836
    },
    {
      "epoch": 0.6616243471997119,
      "grad_norm": 0.352781742811203,
      "learning_rate": 2.34388541165142e-05,
      "loss": 1.1698,
      "step": 1837
    },
    {
      "epoch": 0.661984512875923,
      "grad_norm": 0.3901326656341553,
      "learning_rate": 2.3435243139142997e-05,
      "loss": 1.3295,
      "step": 1838
    },
    {
      "epoch": 0.662344678552134,
      "grad_norm": 0.3426808714866638,
      "learning_rate": 2.343163216177179e-05,
      "loss": 1.271,
      "step": 1839
    },
    {
      "epoch": 0.6627048442283451,
      "grad_norm": 0.3900443911552429,
      "learning_rate": 2.3428021184400578e-05,
      "loss": 1.4116,
      "step": 1840
    },
    {
      "epoch": 0.6630650099045561,
      "grad_norm": 0.3487034738063812,
      "learning_rate": 2.342441020702937e-05,
      "loss": 1.2473,
      "step": 1841
    },
    {
      "epoch": 0.6634251755807672,
      "grad_norm": 0.34963488578796387,
      "learning_rate": 2.3420799229658162e-05,
      "loss": 1.4047,
      "step": 1842
    },
    {
      "epoch": 0.6637853412569782,
      "grad_norm": 0.3546178936958313,
      "learning_rate": 2.341718825228695e-05,
      "loss": 1.2242,
      "step": 1843
    },
    {
      "epoch": 0.6641455069331893,
      "grad_norm": 0.3442215025424957,
      "learning_rate": 2.3413577274915744e-05,
      "loss": 1.2703,
      "step": 1844
    },
    {
      "epoch": 0.6645056726094003,
      "grad_norm": 0.34149786829948425,
      "learning_rate": 2.3409966297544536e-05,
      "loss": 1.2776,
      "step": 1845
    },
    {
      "epoch": 0.6648658382856114,
      "grad_norm": 0.34990930557250977,
      "learning_rate": 2.3406355320173328e-05,
      "loss": 1.2296,
      "step": 1846
    },
    {
      "epoch": 0.6652260039618224,
      "grad_norm": 0.3439096510410309,
      "learning_rate": 2.340274434280212e-05,
      "loss": 1.2883,
      "step": 1847
    },
    {
      "epoch": 0.6655861696380335,
      "grad_norm": 0.373256117105484,
      "learning_rate": 2.339913336543091e-05,
      "loss": 1.1977,
      "step": 1848
    },
    {
      "epoch": 0.6659463353142445,
      "grad_norm": 0.3482752740383148,
      "learning_rate": 2.33955223880597e-05,
      "loss": 1.31,
      "step": 1849
    },
    {
      "epoch": 0.6663065009904556,
      "grad_norm": 0.34437644481658936,
      "learning_rate": 2.3391911410688494e-05,
      "loss": 1.2728,
      "step": 1850
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.3547097146511078,
      "learning_rate": 2.3388300433317286e-05,
      "loss": 1.2516,
      "step": 1851
    },
    {
      "epoch": 0.6670268323428777,
      "grad_norm": 0.35640713572502136,
      "learning_rate": 2.338468945594608e-05,
      "loss": 1.3054,
      "step": 1852
    },
    {
      "epoch": 0.6673869980190887,
      "grad_norm": 0.34513604640960693,
      "learning_rate": 2.3381078478574867e-05,
      "loss": 1.2722,
      "step": 1853
    },
    {
      "epoch": 0.6677471636952999,
      "grad_norm": 0.3535730838775635,
      "learning_rate": 2.337746750120366e-05,
      "loss": 1.302,
      "step": 1854
    },
    {
      "epoch": 0.6681073293715108,
      "grad_norm": 0.35230907797813416,
      "learning_rate": 2.3373856523832452e-05,
      "loss": 1.1711,
      "step": 1855
    },
    {
      "epoch": 0.668467495047722,
      "grad_norm": 0.3560754358768463,
      "learning_rate": 2.337024554646124e-05,
      "loss": 1.431,
      "step": 1856
    },
    {
      "epoch": 0.668827660723933,
      "grad_norm": 0.35665813088417053,
      "learning_rate": 2.3366634569090033e-05,
      "loss": 1.1633,
      "step": 1857
    },
    {
      "epoch": 0.6691878264001441,
      "grad_norm": 0.357737272977829,
      "learning_rate": 2.336302359171883e-05,
      "loss": 1.2685,
      "step": 1858
    },
    {
      "epoch": 0.6695479920763551,
      "grad_norm": 0.352177232503891,
      "learning_rate": 2.3359412614347618e-05,
      "loss": 1.2289,
      "step": 1859
    },
    {
      "epoch": 0.6699081577525662,
      "grad_norm": 0.3468865156173706,
      "learning_rate": 2.335580163697641e-05,
      "loss": 1.2294,
      "step": 1860
    },
    {
      "epoch": 0.6702683234287773,
      "grad_norm": 0.3612728714942932,
      "learning_rate": 2.33521906596052e-05,
      "loss": 1.3708,
      "step": 1861
    },
    {
      "epoch": 0.6706284891049883,
      "grad_norm": 0.34555190801620483,
      "learning_rate": 2.334857968223399e-05,
      "loss": 1.2311,
      "step": 1862
    },
    {
      "epoch": 0.6709886547811994,
      "grad_norm": 0.35037198662757874,
      "learning_rate": 2.3344968704862784e-05,
      "loss": 1.2969,
      "step": 1863
    },
    {
      "epoch": 0.6713488204574104,
      "grad_norm": 0.3633771538734436,
      "learning_rate": 2.3341357727491573e-05,
      "loss": 1.3131,
      "step": 1864
    },
    {
      "epoch": 0.6717089861336215,
      "grad_norm": 0.3632413446903229,
      "learning_rate": 2.3337746750120368e-05,
      "loss": 1.3288,
      "step": 1865
    },
    {
      "epoch": 0.6720691518098325,
      "grad_norm": 0.3496871292591095,
      "learning_rate": 2.333413577274916e-05,
      "loss": 1.2758,
      "step": 1866
    },
    {
      "epoch": 0.6724293174860436,
      "grad_norm": 0.3744611144065857,
      "learning_rate": 2.333052479537795e-05,
      "loss": 1.4061,
      "step": 1867
    },
    {
      "epoch": 0.6727894831622546,
      "grad_norm": 0.3435218334197998,
      "learning_rate": 2.3326913818006742e-05,
      "loss": 1.1887,
      "step": 1868
    },
    {
      "epoch": 0.6731496488384657,
      "grad_norm": 0.38240909576416016,
      "learning_rate": 2.332330284063553e-05,
      "loss": 1.5148,
      "step": 1869
    },
    {
      "epoch": 0.6735098145146767,
      "grad_norm": 0.3538779616355896,
      "learning_rate": 2.3319691863264323e-05,
      "loss": 1.2786,
      "step": 1870
    },
    {
      "epoch": 0.6738699801908878,
      "grad_norm": 0.36962032318115234,
      "learning_rate": 2.3316080885893115e-05,
      "loss": 1.3933,
      "step": 1871
    },
    {
      "epoch": 0.6742301458670988,
      "grad_norm": 0.3702656626701355,
      "learning_rate": 2.3312469908521907e-05,
      "loss": 1.1446,
      "step": 1872
    },
    {
      "epoch": 0.67459031154331,
      "grad_norm": 0.35765185952186584,
      "learning_rate": 2.33088589311507e-05,
      "loss": 1.3078,
      "step": 1873
    },
    {
      "epoch": 0.674950477219521,
      "grad_norm": 0.3459198772907257,
      "learning_rate": 2.3305247953779492e-05,
      "loss": 1.3206,
      "step": 1874
    },
    {
      "epoch": 0.6753106428957321,
      "grad_norm": 0.3512488305568695,
      "learning_rate": 2.330163697640828e-05,
      "loss": 1.2109,
      "step": 1875
    },
    {
      "epoch": 0.6756708085719431,
      "grad_norm": 0.36803141236305237,
      "learning_rate": 2.3298025999037073e-05,
      "loss": 1.2793,
      "step": 1876
    },
    {
      "epoch": 0.6760309742481542,
      "grad_norm": 0.3615657389163971,
      "learning_rate": 2.3294415021665862e-05,
      "loss": 1.3632,
      "step": 1877
    },
    {
      "epoch": 0.6763911399243652,
      "grad_norm": 0.37343037128448486,
      "learning_rate": 2.3290804044294658e-05,
      "loss": 1.2052,
      "step": 1878
    },
    {
      "epoch": 0.6767513056005763,
      "grad_norm": 0.34878867864608765,
      "learning_rate": 2.328719306692345e-05,
      "loss": 1.3247,
      "step": 1879
    },
    {
      "epoch": 0.6771114712767873,
      "grad_norm": 0.3636738359928131,
      "learning_rate": 2.328358208955224e-05,
      "loss": 1.3004,
      "step": 1880
    },
    {
      "epoch": 0.6774716369529984,
      "grad_norm": 0.35227105021476746,
      "learning_rate": 2.327997111218103e-05,
      "loss": 1.3329,
      "step": 1881
    },
    {
      "epoch": 0.6778318026292094,
      "grad_norm": 0.359427809715271,
      "learning_rate": 2.3276360134809824e-05,
      "loss": 1.1609,
      "step": 1882
    },
    {
      "epoch": 0.6781919683054205,
      "grad_norm": 0.3617180585861206,
      "learning_rate": 2.3272749157438613e-05,
      "loss": 1.2405,
      "step": 1883
    },
    {
      "epoch": 0.6785521339816315,
      "grad_norm": 0.35931915044784546,
      "learning_rate": 2.3269138180067405e-05,
      "loss": 1.2757,
      "step": 1884
    },
    {
      "epoch": 0.6789122996578426,
      "grad_norm": 0.3662914037704468,
      "learning_rate": 2.3265527202696197e-05,
      "loss": 1.3556,
      "step": 1885
    },
    {
      "epoch": 0.6792724653340536,
      "grad_norm": 0.3673398196697235,
      "learning_rate": 2.326191622532499e-05,
      "loss": 1.2281,
      "step": 1886
    },
    {
      "epoch": 0.6796326310102647,
      "grad_norm": 0.35418546199798584,
      "learning_rate": 2.3258305247953782e-05,
      "loss": 1.2536,
      "step": 1887
    },
    {
      "epoch": 0.6799927966864758,
      "grad_norm": 0.3871496915817261,
      "learning_rate": 2.325469427058257e-05,
      "loss": 1.3593,
      "step": 1888
    },
    {
      "epoch": 0.6803529623626868,
      "grad_norm": 0.37120506167411804,
      "learning_rate": 2.3251083293211363e-05,
      "loss": 1.271,
      "step": 1889
    },
    {
      "epoch": 0.6807131280388979,
      "grad_norm": 0.37984952330589294,
      "learning_rate": 2.3247472315840155e-05,
      "loss": 1.2895,
      "step": 1890
    },
    {
      "epoch": 0.6810732937151089,
      "grad_norm": 0.36169344186782837,
      "learning_rate": 2.3243861338468944e-05,
      "loss": 1.3295,
      "step": 1891
    },
    {
      "epoch": 0.68143345939132,
      "grad_norm": 0.3741666376590729,
      "learning_rate": 2.324025036109774e-05,
      "loss": 1.2794,
      "step": 1892
    },
    {
      "epoch": 0.681793625067531,
      "grad_norm": 0.3729834258556366,
      "learning_rate": 2.323663938372653e-05,
      "loss": 1.2803,
      "step": 1893
    },
    {
      "epoch": 0.6821537907437422,
      "grad_norm": 0.36026933789253235,
      "learning_rate": 2.323302840635532e-05,
      "loss": 1.3975,
      "step": 1894
    },
    {
      "epoch": 0.6825139564199532,
      "grad_norm": 0.36135146021842957,
      "learning_rate": 2.3229417428984113e-05,
      "loss": 1.3909,
      "step": 1895
    },
    {
      "epoch": 0.6828741220961643,
      "grad_norm": 0.3547610938549042,
      "learning_rate": 2.3225806451612902e-05,
      "loss": 1.3092,
      "step": 1896
    },
    {
      "epoch": 0.6832342877723753,
      "grad_norm": 0.330928236246109,
      "learning_rate": 2.3222195474241695e-05,
      "loss": 1.207,
      "step": 1897
    },
    {
      "epoch": 0.6835944534485864,
      "grad_norm": 0.3558376133441925,
      "learning_rate": 2.3218584496870487e-05,
      "loss": 1.2902,
      "step": 1898
    },
    {
      "epoch": 0.6839546191247974,
      "grad_norm": 0.34861454367637634,
      "learning_rate": 2.321497351949928e-05,
      "loss": 1.2938,
      "step": 1899
    },
    {
      "epoch": 0.6843147848010085,
      "grad_norm": 0.34626954793930054,
      "learning_rate": 2.321136254212807e-05,
      "loss": 1.3166,
      "step": 1900
    },
    {
      "epoch": 0.6846749504772195,
      "grad_norm": 0.3595518469810486,
      "learning_rate": 2.320775156475686e-05,
      "loss": 1.2395,
      "step": 1901
    },
    {
      "epoch": 0.6850351161534306,
      "grad_norm": 0.3608231842517853,
      "learning_rate": 2.3204140587385653e-05,
      "loss": 1.3164,
      "step": 1902
    },
    {
      "epoch": 0.6853952818296416,
      "grad_norm": 0.36879071593284607,
      "learning_rate": 2.3200529610014445e-05,
      "loss": 1.355,
      "step": 1903
    },
    {
      "epoch": 0.6857554475058527,
      "grad_norm": 0.34965795278549194,
      "learning_rate": 2.3196918632643234e-05,
      "loss": 1.2238,
      "step": 1904
    },
    {
      "epoch": 0.6861156131820637,
      "grad_norm": 0.350134015083313,
      "learning_rate": 2.319330765527203e-05,
      "loss": 1.3016,
      "step": 1905
    },
    {
      "epoch": 0.6864757788582748,
      "grad_norm": 0.36458849906921387,
      "learning_rate": 2.318969667790082e-05,
      "loss": 1.3639,
      "step": 1906
    },
    {
      "epoch": 0.6868359445344858,
      "grad_norm": 0.3566454350948334,
      "learning_rate": 2.318608570052961e-05,
      "loss": 1.2669,
      "step": 1907
    },
    {
      "epoch": 0.6871961102106969,
      "grad_norm": 0.34536096453666687,
      "learning_rate": 2.3182474723158403e-05,
      "loss": 1.3031,
      "step": 1908
    },
    {
      "epoch": 0.6875562758869079,
      "grad_norm": 0.35107362270355225,
      "learning_rate": 2.3178863745787192e-05,
      "loss": 1.166,
      "step": 1909
    },
    {
      "epoch": 0.687916441563119,
      "grad_norm": 0.364541620016098,
      "learning_rate": 2.3175252768415984e-05,
      "loss": 1.3782,
      "step": 1910
    },
    {
      "epoch": 0.68827660723933,
      "grad_norm": 0.36846765875816345,
      "learning_rate": 2.3171641791044777e-05,
      "loss": 1.2912,
      "step": 1911
    },
    {
      "epoch": 0.6886367729155412,
      "grad_norm": 0.3571099638938904,
      "learning_rate": 2.316803081367357e-05,
      "loss": 1.2407,
      "step": 1912
    },
    {
      "epoch": 0.6889969385917523,
      "grad_norm": 0.35019105672836304,
      "learning_rate": 2.316441983630236e-05,
      "loss": 1.2732,
      "step": 1913
    },
    {
      "epoch": 0.6893571042679633,
      "grad_norm": 0.3704058527946472,
      "learning_rate": 2.316080885893115e-05,
      "loss": 1.2418,
      "step": 1914
    },
    {
      "epoch": 0.6897172699441744,
      "grad_norm": 0.3515303134918213,
      "learning_rate": 2.3157197881559942e-05,
      "loss": 1.2132,
      "step": 1915
    },
    {
      "epoch": 0.6900774356203854,
      "grad_norm": 0.38785427808761597,
      "learning_rate": 2.3153586904188735e-05,
      "loss": 1.2505,
      "step": 1916
    },
    {
      "epoch": 0.6904376012965965,
      "grad_norm": 0.3662811815738678,
      "learning_rate": 2.3149975926817524e-05,
      "loss": 1.3825,
      "step": 1917
    },
    {
      "epoch": 0.6907977669728075,
      "grad_norm": 0.3771283030509949,
      "learning_rate": 2.3146364949446316e-05,
      "loss": 1.241,
      "step": 1918
    },
    {
      "epoch": 0.6911579326490186,
      "grad_norm": 0.3804689049720764,
      "learning_rate": 2.314275397207511e-05,
      "loss": 1.3045,
      "step": 1919
    },
    {
      "epoch": 0.6915180983252296,
      "grad_norm": 0.37625718116760254,
      "learning_rate": 2.31391429947039e-05,
      "loss": 1.2814,
      "step": 1920
    },
    {
      "epoch": 0.6918782640014407,
      "grad_norm": 0.35073840618133545,
      "learning_rate": 2.3135532017332693e-05,
      "loss": 1.3006,
      "step": 1921
    },
    {
      "epoch": 0.6922384296776517,
      "grad_norm": 0.365023136138916,
      "learning_rate": 2.313192103996148e-05,
      "loss": 1.4386,
      "step": 1922
    },
    {
      "epoch": 0.6925985953538628,
      "grad_norm": 0.3478725254535675,
      "learning_rate": 2.3128310062590274e-05,
      "loss": 1.229,
      "step": 1923
    },
    {
      "epoch": 0.6929587610300738,
      "grad_norm": 0.3509766161441803,
      "learning_rate": 2.3124699085219066e-05,
      "loss": 1.1137,
      "step": 1924
    },
    {
      "epoch": 0.6933189267062849,
      "grad_norm": 0.36046016216278076,
      "learning_rate": 2.3121088107847855e-05,
      "loss": 1.3067,
      "step": 1925
    },
    {
      "epoch": 0.6936790923824959,
      "grad_norm": 0.36232298612594604,
      "learning_rate": 2.311747713047665e-05,
      "loss": 1.3095,
      "step": 1926
    },
    {
      "epoch": 0.694039258058707,
      "grad_norm": 0.34351035952568054,
      "learning_rate": 2.3113866153105443e-05,
      "loss": 1.2006,
      "step": 1927
    },
    {
      "epoch": 0.694399423734918,
      "grad_norm": 0.35066354274749756,
      "learning_rate": 2.3110255175734232e-05,
      "loss": 1.3079,
      "step": 1928
    },
    {
      "epoch": 0.6947595894111291,
      "grad_norm": 0.36806970834732056,
      "learning_rate": 2.3106644198363024e-05,
      "loss": 1.2981,
      "step": 1929
    },
    {
      "epoch": 0.6951197550873401,
      "grad_norm": 0.35264089703559875,
      "learning_rate": 2.3103033220991813e-05,
      "loss": 1.306,
      "step": 1930
    },
    {
      "epoch": 0.6954799207635513,
      "grad_norm": 0.34261879324913025,
      "learning_rate": 2.3099422243620606e-05,
      "loss": 1.2188,
      "step": 1931
    },
    {
      "epoch": 0.6958400864397623,
      "grad_norm": 0.34469515085220337,
      "learning_rate": 2.30958112662494e-05,
      "loss": 1.2596,
      "step": 1932
    },
    {
      "epoch": 0.6962002521159734,
      "grad_norm": 0.3692353665828705,
      "learning_rate": 2.309220028887819e-05,
      "loss": 1.2386,
      "step": 1933
    },
    {
      "epoch": 0.6965604177921844,
      "grad_norm": 0.3614419400691986,
      "learning_rate": 2.3088589311506982e-05,
      "loss": 1.3044,
      "step": 1934
    },
    {
      "epoch": 0.6969205834683955,
      "grad_norm": 0.3534889817237854,
      "learning_rate": 2.3084978334135775e-05,
      "loss": 1.4163,
      "step": 1935
    },
    {
      "epoch": 0.6972807491446065,
      "grad_norm": 0.374616414308548,
      "learning_rate": 2.3081367356764564e-05,
      "loss": 1.2303,
      "step": 1936
    },
    {
      "epoch": 0.6976409148208176,
      "grad_norm": 0.3457193672657013,
      "learning_rate": 2.3077756379393356e-05,
      "loss": 1.2176,
      "step": 1937
    },
    {
      "epoch": 0.6980010804970286,
      "grad_norm": 0.35863402485847473,
      "learning_rate": 2.3074145402022145e-05,
      "loss": 1.3251,
      "step": 1938
    },
    {
      "epoch": 0.6983612461732397,
      "grad_norm": 0.35104185342788696,
      "learning_rate": 2.307053442465094e-05,
      "loss": 1.2699,
      "step": 1939
    },
    {
      "epoch": 0.6987214118494508,
      "grad_norm": 0.3619420826435089,
      "learning_rate": 2.3066923447279733e-05,
      "loss": 1.4058,
      "step": 1940
    },
    {
      "epoch": 0.6990815775256618,
      "grad_norm": 0.34793025255203247,
      "learning_rate": 2.3063312469908522e-05,
      "loss": 1.2196,
      "step": 1941
    },
    {
      "epoch": 0.6994417432018729,
      "grad_norm": 0.3665967285633087,
      "learning_rate": 2.3059701492537314e-05,
      "loss": 1.3487,
      "step": 1942
    },
    {
      "epoch": 0.6998019088780839,
      "grad_norm": 0.3988931477069855,
      "learning_rate": 2.3056090515166106e-05,
      "loss": 1.2187,
      "step": 1943
    },
    {
      "epoch": 0.700162074554295,
      "grad_norm": 0.36451950669288635,
      "learning_rate": 2.3052479537794895e-05,
      "loss": 1.2767,
      "step": 1944
    },
    {
      "epoch": 0.700522240230506,
      "grad_norm": 0.3723020851612091,
      "learning_rate": 2.3048868560423688e-05,
      "loss": 1.3351,
      "step": 1945
    },
    {
      "epoch": 0.7008824059067171,
      "grad_norm": 0.3748205900192261,
      "learning_rate": 2.304525758305248e-05,
      "loss": 1.3219,
      "step": 1946
    },
    {
      "epoch": 0.7012425715829281,
      "grad_norm": 0.36019712686538696,
      "learning_rate": 2.3041646605681272e-05,
      "loss": 1.3737,
      "step": 1947
    },
    {
      "epoch": 0.7016027372591392,
      "grad_norm": 0.3571692109107971,
      "learning_rate": 2.3038035628310064e-05,
      "loss": 1.2716,
      "step": 1948
    },
    {
      "epoch": 0.7019629029353502,
      "grad_norm": 0.3613077998161316,
      "learning_rate": 2.3034424650938853e-05,
      "loss": 1.2769,
      "step": 1949
    },
    {
      "epoch": 0.7023230686115614,
      "grad_norm": 0.3704015910625458,
      "learning_rate": 2.3030813673567646e-05,
      "loss": 1.3037,
      "step": 1950
    },
    {
      "epoch": 0.7026832342877724,
      "grad_norm": 0.362277626991272,
      "learning_rate": 2.3027202696196438e-05,
      "loss": 1.2864,
      "step": 1951
    },
    {
      "epoch": 0.7030433999639835,
      "grad_norm": 0.39047718048095703,
      "learning_rate": 2.3023591718825227e-05,
      "loss": 1.3364,
      "step": 1952
    },
    {
      "epoch": 0.7034035656401945,
      "grad_norm": 0.35851919651031494,
      "learning_rate": 2.3019980741454023e-05,
      "loss": 1.2833,
      "step": 1953
    },
    {
      "epoch": 0.7037637313164056,
      "grad_norm": 0.3674127757549286,
      "learning_rate": 2.301636976408281e-05,
      "loss": 1.2345,
      "step": 1954
    },
    {
      "epoch": 0.7041238969926166,
      "grad_norm": 0.3581104874610901,
      "learning_rate": 2.3012758786711604e-05,
      "loss": 1.2595,
      "step": 1955
    },
    {
      "epoch": 0.7044840626688277,
      "grad_norm": 0.3794885575771332,
      "learning_rate": 2.3009147809340396e-05,
      "loss": 1.4344,
      "step": 1956
    },
    {
      "epoch": 0.7048442283450387,
      "grad_norm": 0.3860798478126526,
      "learning_rate": 2.3005536831969185e-05,
      "loss": 1.356,
      "step": 1957
    },
    {
      "epoch": 0.7052043940212498,
      "grad_norm": 0.3708766996860504,
      "learning_rate": 2.3001925854597977e-05,
      "loss": 1.3209,
      "step": 1958
    },
    {
      "epoch": 0.7055645596974608,
      "grad_norm": 0.3736042082309723,
      "learning_rate": 2.2998314877226773e-05,
      "loss": 1.2106,
      "step": 1959
    },
    {
      "epoch": 0.7059247253736719,
      "grad_norm": 0.37796202301979065,
      "learning_rate": 2.2994703899855562e-05,
      "loss": 1.3664,
      "step": 1960
    },
    {
      "epoch": 0.7062848910498829,
      "grad_norm": 0.37723836302757263,
      "learning_rate": 2.2991092922484354e-05,
      "loss": 1.2852,
      "step": 1961
    },
    {
      "epoch": 0.706645056726094,
      "grad_norm": 0.36921975016593933,
      "learning_rate": 2.2987481945113143e-05,
      "loss": 1.3469,
      "step": 1962
    },
    {
      "epoch": 0.707005222402305,
      "grad_norm": 0.3725905418395996,
      "learning_rate": 2.2983870967741935e-05,
      "loss": 1.2737,
      "step": 1963
    },
    {
      "epoch": 0.7073653880785161,
      "grad_norm": 0.3635367155075073,
      "learning_rate": 2.2980259990370728e-05,
      "loss": 1.2958,
      "step": 1964
    },
    {
      "epoch": 0.7077255537547271,
      "grad_norm": 0.369520902633667,
      "learning_rate": 2.2976649012999517e-05,
      "loss": 1.3896,
      "step": 1965
    },
    {
      "epoch": 0.7080857194309382,
      "grad_norm": 0.3554054796695709,
      "learning_rate": 2.2973038035628312e-05,
      "loss": 1.2192,
      "step": 1966
    },
    {
      "epoch": 0.7084458851071493,
      "grad_norm": 0.36098888516426086,
      "learning_rate": 2.2969427058257105e-05,
      "loss": 1.4086,
      "step": 1967
    },
    {
      "epoch": 0.7088060507833603,
      "grad_norm": 0.360727995634079,
      "learning_rate": 2.2965816080885893e-05,
      "loss": 1.3031,
      "step": 1968
    },
    {
      "epoch": 0.7091662164595715,
      "grad_norm": 0.3406851291656494,
      "learning_rate": 2.2962205103514686e-05,
      "loss": 1.2901,
      "step": 1969
    },
    {
      "epoch": 0.7095263821357825,
      "grad_norm": 0.37452685832977295,
      "learning_rate": 2.2958594126143475e-05,
      "loss": 1.2628,
      "step": 1970
    },
    {
      "epoch": 0.7098865478119936,
      "grad_norm": 0.3782329857349396,
      "learning_rate": 2.2954983148772267e-05,
      "loss": 1.4653,
      "step": 1971
    },
    {
      "epoch": 0.7102467134882046,
      "grad_norm": 0.36261704564094543,
      "learning_rate": 2.295137217140106e-05,
      "loss": 1.2618,
      "step": 1972
    },
    {
      "epoch": 0.7106068791644157,
      "grad_norm": 0.363870769739151,
      "learning_rate": 2.294776119402985e-05,
      "loss": 1.3536,
      "step": 1973
    },
    {
      "epoch": 0.7109670448406267,
      "grad_norm": 0.3595064878463745,
      "learning_rate": 2.2944150216658644e-05,
      "loss": 1.323,
      "step": 1974
    },
    {
      "epoch": 0.7113272105168378,
      "grad_norm": 0.3698173761367798,
      "learning_rate": 2.2940539239287436e-05,
      "loss": 1.3947,
      "step": 1975
    },
    {
      "epoch": 0.7116873761930488,
      "grad_norm": 0.3551672697067261,
      "learning_rate": 2.2936928261916225e-05,
      "loss": 1.2825,
      "step": 1976
    },
    {
      "epoch": 0.7120475418692599,
      "grad_norm": 0.3725747764110565,
      "learning_rate": 2.2933317284545017e-05,
      "loss": 1.2641,
      "step": 1977
    },
    {
      "epoch": 0.7124077075454709,
      "grad_norm": 0.385951966047287,
      "learning_rate": 2.2929706307173806e-05,
      "loss": 1.2678,
      "step": 1978
    },
    {
      "epoch": 0.712767873221682,
      "grad_norm": 0.3558403551578522,
      "learning_rate": 2.2926095329802602e-05,
      "loss": 1.2179,
      "step": 1979
    },
    {
      "epoch": 0.713128038897893,
      "grad_norm": 0.366982102394104,
      "learning_rate": 2.2922484352431394e-05,
      "loss": 1.2403,
      "step": 1980
    },
    {
      "epoch": 0.7134882045741041,
      "grad_norm": 0.3794853389263153,
      "learning_rate": 2.2918873375060183e-05,
      "loss": 1.4505,
      "step": 1981
    },
    {
      "epoch": 0.7138483702503151,
      "grad_norm": 0.3886154294013977,
      "learning_rate": 2.2915262397688975e-05,
      "loss": 1.243,
      "step": 1982
    },
    {
      "epoch": 0.7142085359265262,
      "grad_norm": 0.38494330644607544,
      "learning_rate": 2.2911651420317768e-05,
      "loss": 1.3496,
      "step": 1983
    },
    {
      "epoch": 0.7145687016027372,
      "grad_norm": 0.36252033710479736,
      "learning_rate": 2.2908040442946557e-05,
      "loss": 1.2627,
      "step": 1984
    },
    {
      "epoch": 0.7149288672789483,
      "grad_norm": 0.3611757159233093,
      "learning_rate": 2.290442946557535e-05,
      "loss": 1.3315,
      "step": 1985
    },
    {
      "epoch": 0.7152890329551593,
      "grad_norm": 0.35645830631256104,
      "learning_rate": 2.290081848820414e-05,
      "loss": 1.335,
      "step": 1986
    },
    {
      "epoch": 0.7156491986313704,
      "grad_norm": 0.37377843260765076,
      "learning_rate": 2.2897207510832934e-05,
      "loss": 1.4448,
      "step": 1987
    },
    {
      "epoch": 0.7160093643075814,
      "grad_norm": 0.35387641191482544,
      "learning_rate": 2.2893596533461726e-05,
      "loss": 1.4242,
      "step": 1988
    },
    {
      "epoch": 0.7163695299837926,
      "grad_norm": 0.39259451627731323,
      "learning_rate": 2.2889985556090515e-05,
      "loss": 1.478,
      "step": 1989
    },
    {
      "epoch": 0.7167296956600036,
      "grad_norm": 0.3596358001232147,
      "learning_rate": 2.2886374578719307e-05,
      "loss": 1.2183,
      "step": 1990
    },
    {
      "epoch": 0.7170898613362147,
      "grad_norm": 0.3752477169036865,
      "learning_rate": 2.28827636013481e-05,
      "loss": 1.3125,
      "step": 1991
    },
    {
      "epoch": 0.7174500270124257,
      "grad_norm": 0.3578742742538452,
      "learning_rate": 2.2879152623976888e-05,
      "loss": 1.2215,
      "step": 1992
    },
    {
      "epoch": 0.7178101926886368,
      "grad_norm": 0.40011024475097656,
      "learning_rate": 2.2875541646605684e-05,
      "loss": 1.3299,
      "step": 1993
    },
    {
      "epoch": 0.7181703583648479,
      "grad_norm": 0.3569742441177368,
      "learning_rate": 2.2871930669234473e-05,
      "loss": 1.2072,
      "step": 1994
    },
    {
      "epoch": 0.7185305240410589,
      "grad_norm": 0.3811100423336029,
      "learning_rate": 2.2868319691863265e-05,
      "loss": 1.2248,
      "step": 1995
    },
    {
      "epoch": 0.71889068971727,
      "grad_norm": 0.3806251287460327,
      "learning_rate": 2.2864708714492057e-05,
      "loss": 1.2663,
      "step": 1996
    },
    {
      "epoch": 0.719250855393481,
      "grad_norm": 0.3736475706100464,
      "learning_rate": 2.2861097737120846e-05,
      "loss": 1.312,
      "step": 1997
    },
    {
      "epoch": 0.7196110210696921,
      "grad_norm": 0.3550131618976593,
      "learning_rate": 2.285748675974964e-05,
      "loss": 1.2572,
      "step": 1998
    },
    {
      "epoch": 0.7199711867459031,
      "grad_norm": 0.3692483603954315,
      "learning_rate": 2.285387578237843e-05,
      "loss": 1.2647,
      "step": 1999
    },
    {
      "epoch": 0.7203313524221142,
      "grad_norm": 0.35627469420433044,
      "learning_rate": 2.2850264805007223e-05,
      "loss": 1.3114,
      "step": 2000
    },
    {
      "epoch": 0.7206915180983252,
      "grad_norm": 0.3638963997364044,
      "learning_rate": 2.2846653827636015e-05,
      "loss": 1.3237,
      "step": 2001
    },
    {
      "epoch": 0.7210516837745363,
      "grad_norm": 0.3748044967651367,
      "learning_rate": 2.2843042850264804e-05,
      "loss": 1.2621,
      "step": 2002
    },
    {
      "epoch": 0.7214118494507473,
      "grad_norm": 0.3850874602794647,
      "learning_rate": 2.2839431872893597e-05,
      "loss": 1.3418,
      "step": 2003
    },
    {
      "epoch": 0.7217720151269584,
      "grad_norm": 0.37378793954849243,
      "learning_rate": 2.283582089552239e-05,
      "loss": 1.3169,
      "step": 2004
    },
    {
      "epoch": 0.7221321808031694,
      "grad_norm": 0.36366695165634155,
      "learning_rate": 2.2832209918151178e-05,
      "loss": 1.2964,
      "step": 2005
    },
    {
      "epoch": 0.7224923464793805,
      "grad_norm": 0.3518160581588745,
      "learning_rate": 2.2828598940779974e-05,
      "loss": 1.2743,
      "step": 2006
    },
    {
      "epoch": 0.7228525121555915,
      "grad_norm": 0.3596735894680023,
      "learning_rate": 2.2824987963408766e-05,
      "loss": 1.2288,
      "step": 2007
    },
    {
      "epoch": 0.7232126778318027,
      "grad_norm": 0.34444156289100647,
      "learning_rate": 2.2821376986037555e-05,
      "loss": 1.207,
      "step": 2008
    },
    {
      "epoch": 0.7235728435080137,
      "grad_norm": 0.35315561294555664,
      "learning_rate": 2.2817766008666347e-05,
      "loss": 1.3101,
      "step": 2009
    },
    {
      "epoch": 0.7239330091842248,
      "grad_norm": 0.3672962486743927,
      "learning_rate": 2.2814155031295136e-05,
      "loss": 1.2718,
      "step": 2010
    },
    {
      "epoch": 0.7242931748604358,
      "grad_norm": 0.3612373173236847,
      "learning_rate": 2.2810544053923928e-05,
      "loss": 1.3439,
      "step": 2011
    },
    {
      "epoch": 0.7246533405366469,
      "grad_norm": 0.3765842914581299,
      "learning_rate": 2.280693307655272e-05,
      "loss": 1.4003,
      "step": 2012
    },
    {
      "epoch": 0.7250135062128579,
      "grad_norm": 0.35882043838500977,
      "learning_rate": 2.2803322099181513e-05,
      "loss": 1.301,
      "step": 2013
    },
    {
      "epoch": 0.725373671889069,
      "grad_norm": 0.3713721036911011,
      "learning_rate": 2.2799711121810305e-05,
      "loss": 1.3885,
      "step": 2014
    },
    {
      "epoch": 0.72573383756528,
      "grad_norm": 0.352586567401886,
      "learning_rate": 2.2796100144439097e-05,
      "loss": 1.4261,
      "step": 2015
    },
    {
      "epoch": 0.7260940032414911,
      "grad_norm": 0.37343066930770874,
      "learning_rate": 2.2792489167067886e-05,
      "loss": 1.1969,
      "step": 2016
    },
    {
      "epoch": 0.7264541689177021,
      "grad_norm": 0.38735637068748474,
      "learning_rate": 2.278887818969668e-05,
      "loss": 1.2971,
      "step": 2017
    },
    {
      "epoch": 0.7268143345939132,
      "grad_norm": 0.39455917477607727,
      "learning_rate": 2.2785267212325468e-05,
      "loss": 1.4502,
      "step": 2018
    },
    {
      "epoch": 0.7271745002701242,
      "grad_norm": 0.36989250779151917,
      "learning_rate": 2.278165623495426e-05,
      "loss": 1.2669,
      "step": 2019
    },
    {
      "epoch": 0.7275346659463353,
      "grad_norm": 0.371808260679245,
      "learning_rate": 2.2778045257583056e-05,
      "loss": 1.3426,
      "step": 2020
    },
    {
      "epoch": 0.7278948316225464,
      "grad_norm": 0.3711003363132477,
      "learning_rate": 2.2774434280211844e-05,
      "loss": 1.3559,
      "step": 2021
    },
    {
      "epoch": 0.7282549972987574,
      "grad_norm": 0.3956638276576996,
      "learning_rate": 2.2770823302840637e-05,
      "loss": 1.3257,
      "step": 2022
    },
    {
      "epoch": 0.7286151629749685,
      "grad_norm": 0.4044959545135498,
      "learning_rate": 2.276721232546943e-05,
      "loss": 1.3823,
      "step": 2023
    },
    {
      "epoch": 0.7289753286511795,
      "grad_norm": 0.37026283144950867,
      "learning_rate": 2.2763601348098218e-05,
      "loss": 1.2622,
      "step": 2024
    },
    {
      "epoch": 0.7293354943273906,
      "grad_norm": 0.359539270401001,
      "learning_rate": 2.275999037072701e-05,
      "loss": 1.2728,
      "step": 2025
    },
    {
      "epoch": 0.7296956600036016,
      "grad_norm": 0.36497193574905396,
      "learning_rate": 2.27563793933558e-05,
      "loss": 1.18,
      "step": 2026
    },
    {
      "epoch": 0.7300558256798128,
      "grad_norm": 0.3717595636844635,
      "learning_rate": 2.2752768415984595e-05,
      "loss": 1.2695,
      "step": 2027
    },
    {
      "epoch": 0.7304159913560238,
      "grad_norm": 0.3686262369155884,
      "learning_rate": 2.2749157438613387e-05,
      "loss": 1.3408,
      "step": 2028
    },
    {
      "epoch": 0.7307761570322349,
      "grad_norm": 0.3753564655780792,
      "learning_rate": 2.2745546461242176e-05,
      "loss": 1.2693,
      "step": 2029
    },
    {
      "epoch": 0.7311363227084459,
      "grad_norm": 0.3597225248813629,
      "learning_rate": 2.274193548387097e-05,
      "loss": 1.3106,
      "step": 2030
    },
    {
      "epoch": 0.731496488384657,
      "grad_norm": 0.35523301362991333,
      "learning_rate": 2.273832450649976e-05,
      "loss": 1.1989,
      "step": 2031
    },
    {
      "epoch": 0.731856654060868,
      "grad_norm": 0.36294394731521606,
      "learning_rate": 2.273471352912855e-05,
      "loss": 1.2024,
      "step": 2032
    },
    {
      "epoch": 0.7322168197370791,
      "grad_norm": 0.3727664649486542,
      "learning_rate": 2.2731102551757345e-05,
      "loss": 1.3395,
      "step": 2033
    },
    {
      "epoch": 0.7325769854132901,
      "grad_norm": 0.35625213384628296,
      "learning_rate": 2.2727491574386134e-05,
      "loss": 1.4129,
      "step": 2034
    },
    {
      "epoch": 0.7329371510895012,
      "grad_norm": 0.36631810665130615,
      "learning_rate": 2.2723880597014926e-05,
      "loss": 1.3053,
      "step": 2035
    },
    {
      "epoch": 0.7332973167657122,
      "grad_norm": 0.3658807575702667,
      "learning_rate": 2.272026961964372e-05,
      "loss": 1.177,
      "step": 2036
    },
    {
      "epoch": 0.7336574824419233,
      "grad_norm": 0.36868828535079956,
      "learning_rate": 2.2716658642272508e-05,
      "loss": 1.4022,
      "step": 2037
    },
    {
      "epoch": 0.7340176481181343,
      "grad_norm": 0.3780550956726074,
      "learning_rate": 2.27130476649013e-05,
      "loss": 1.1938,
      "step": 2038
    },
    {
      "epoch": 0.7343778137943454,
      "grad_norm": 0.35532480478286743,
      "learning_rate": 2.2709436687530092e-05,
      "loss": 1.3486,
      "step": 2039
    },
    {
      "epoch": 0.7347379794705564,
      "grad_norm": 0.36501598358154297,
      "learning_rate": 2.2705825710158885e-05,
      "loss": 1.3478,
      "step": 2040
    },
    {
      "epoch": 0.7350981451467675,
      "grad_norm": 0.3428046405315399,
      "learning_rate": 2.2702214732787677e-05,
      "loss": 1.168,
      "step": 2041
    },
    {
      "epoch": 0.7354583108229785,
      "grad_norm": 0.39774230122566223,
      "learning_rate": 2.2698603755416466e-05,
      "loss": 1.4201,
      "step": 2042
    },
    {
      "epoch": 0.7358184764991896,
      "grad_norm": 0.39773795008659363,
      "learning_rate": 2.2694992778045258e-05,
      "loss": 1.2916,
      "step": 2043
    },
    {
      "epoch": 0.7361786421754006,
      "grad_norm": 0.3643186390399933,
      "learning_rate": 2.269138180067405e-05,
      "loss": 1.3141,
      "step": 2044
    },
    {
      "epoch": 0.7365388078516117,
      "grad_norm": 0.3663318157196045,
      "learning_rate": 2.268777082330284e-05,
      "loss": 1.3073,
      "step": 2045
    },
    {
      "epoch": 0.7368989735278229,
      "grad_norm": 0.36276334524154663,
      "learning_rate": 2.268415984593163e-05,
      "loss": 1.3293,
      "step": 2046
    },
    {
      "epoch": 0.7372591392040339,
      "grad_norm": 0.37543827295303345,
      "learning_rate": 2.2680548868560427e-05,
      "loss": 1.2184,
      "step": 2047
    },
    {
      "epoch": 0.737619304880245,
      "grad_norm": 0.36694058775901794,
      "learning_rate": 2.2676937891189216e-05,
      "loss": 1.3163,
      "step": 2048
    },
    {
      "epoch": 0.737979470556456,
      "grad_norm": 0.3812171518802643,
      "learning_rate": 2.267332691381801e-05,
      "loss": 1.3111,
      "step": 2049
    },
    {
      "epoch": 0.7383396362326671,
      "grad_norm": 0.35876068472862244,
      "learning_rate": 2.2669715936446797e-05,
      "loss": 1.1919,
      "step": 2050
    },
    {
      "epoch": 0.7386998019088781,
      "grad_norm": 0.35319390892982483,
      "learning_rate": 2.266610495907559e-05,
      "loss": 1.2026,
      "step": 2051
    },
    {
      "epoch": 0.7390599675850892,
      "grad_norm": 0.3644377291202545,
      "learning_rate": 2.2662493981704382e-05,
      "loss": 1.3092,
      "step": 2052
    },
    {
      "epoch": 0.7394201332613002,
      "grad_norm": 0.3750034272670746,
      "learning_rate": 2.265888300433317e-05,
      "loss": 1.3508,
      "step": 2053
    },
    {
      "epoch": 0.7397802989375113,
      "grad_norm": 0.37027138471603394,
      "learning_rate": 2.2655272026961967e-05,
      "loss": 1.3137,
      "step": 2054
    },
    {
      "epoch": 0.7401404646137223,
      "grad_norm": 0.35351404547691345,
      "learning_rate": 2.265166104959076e-05,
      "loss": 1.2183,
      "step": 2055
    },
    {
      "epoch": 0.7405006302899334,
      "grad_norm": 0.3596063554286957,
      "learning_rate": 2.2648050072219548e-05,
      "loss": 1.1831,
      "step": 2056
    },
    {
      "epoch": 0.7408607959661444,
      "grad_norm": 0.3655427098274231,
      "learning_rate": 2.264443909484834e-05,
      "loss": 1.4125,
      "step": 2057
    },
    {
      "epoch": 0.7412209616423555,
      "grad_norm": 0.39467817544937134,
      "learning_rate": 2.264082811747713e-05,
      "loss": 1.3891,
      "step": 2058
    },
    {
      "epoch": 0.7415811273185665,
      "grad_norm": 0.384066641330719,
      "learning_rate": 2.263721714010592e-05,
      "loss": 1.3154,
      "step": 2059
    },
    {
      "epoch": 0.7419412929947776,
      "grad_norm": 0.3755982220172882,
      "learning_rate": 2.2633606162734717e-05,
      "loss": 1.4702,
      "step": 2060
    },
    {
      "epoch": 0.7423014586709886,
      "grad_norm": 0.37195467948913574,
      "learning_rate": 2.2629995185363506e-05,
      "loss": 1.3815,
      "step": 2061
    },
    {
      "epoch": 0.7426616243471997,
      "grad_norm": 0.3516935110092163,
      "learning_rate": 2.2626384207992298e-05,
      "loss": 1.265,
      "step": 2062
    },
    {
      "epoch": 0.7430217900234107,
      "grad_norm": 0.3420506417751312,
      "learning_rate": 2.262277323062109e-05,
      "loss": 1.151,
      "step": 2063
    },
    {
      "epoch": 0.7433819556996218,
      "grad_norm": 0.3545733392238617,
      "learning_rate": 2.261916225324988e-05,
      "loss": 1.4494,
      "step": 2064
    },
    {
      "epoch": 0.7437421213758328,
      "grad_norm": 0.351092666387558,
      "learning_rate": 2.261555127587867e-05,
      "loss": 1.2276,
      "step": 2065
    },
    {
      "epoch": 0.744102287052044,
      "grad_norm": 0.38531121611595154,
      "learning_rate": 2.261194029850746e-05,
      "loss": 1.3986,
      "step": 2066
    },
    {
      "epoch": 0.744462452728255,
      "grad_norm": 0.378286749124527,
      "learning_rate": 2.2608329321136256e-05,
      "loss": 1.3605,
      "step": 2067
    },
    {
      "epoch": 0.7448226184044661,
      "grad_norm": 0.37529611587524414,
      "learning_rate": 2.260471834376505e-05,
      "loss": 1.2672,
      "step": 2068
    },
    {
      "epoch": 0.7451827840806771,
      "grad_norm": 0.35295793414115906,
      "learning_rate": 2.2601107366393837e-05,
      "loss": 1.1397,
      "step": 2069
    },
    {
      "epoch": 0.7455429497568882,
      "grad_norm": 0.3577638566493988,
      "learning_rate": 2.259749638902263e-05,
      "loss": 1.3954,
      "step": 2070
    },
    {
      "epoch": 0.7459031154330992,
      "grad_norm": 0.3463366627693176,
      "learning_rate": 2.2593885411651422e-05,
      "loss": 1.2287,
      "step": 2071
    },
    {
      "epoch": 0.7462632811093103,
      "grad_norm": 0.3636908233165741,
      "learning_rate": 2.259027443428021e-05,
      "loss": 1.342,
      "step": 2072
    },
    {
      "epoch": 0.7466234467855214,
      "grad_norm": 0.35993143916130066,
      "learning_rate": 2.2586663456909003e-05,
      "loss": 1.1856,
      "step": 2073
    },
    {
      "epoch": 0.7469836124617324,
      "grad_norm": 0.37262454628944397,
      "learning_rate": 2.2583052479537796e-05,
      "loss": 1.3865,
      "step": 2074
    },
    {
      "epoch": 0.7473437781379435,
      "grad_norm": 0.36050838232040405,
      "learning_rate": 2.2579441502166588e-05,
      "loss": 1.2375,
      "step": 2075
    },
    {
      "epoch": 0.7477039438141545,
      "grad_norm": 0.3751996159553528,
      "learning_rate": 2.257583052479538e-05,
      "loss": 1.2423,
      "step": 2076
    },
    {
      "epoch": 0.7480641094903656,
      "grad_norm": 0.38312670588493347,
      "learning_rate": 2.257221954742417e-05,
      "loss": 1.3097,
      "step": 2077
    },
    {
      "epoch": 0.7484242751665766,
      "grad_norm": 0.3760310411453247,
      "learning_rate": 2.256860857005296e-05,
      "loss": 1.2876,
      "step": 2078
    },
    {
      "epoch": 0.7487844408427877,
      "grad_norm": 0.36081963777542114,
      "learning_rate": 2.2564997592681754e-05,
      "loss": 1.2565,
      "step": 2079
    },
    {
      "epoch": 0.7491446065189987,
      "grad_norm": 0.36876606941223145,
      "learning_rate": 2.2561386615310543e-05,
      "loss": 1.2055,
      "step": 2080
    },
    {
      "epoch": 0.7495047721952098,
      "grad_norm": 0.3622570335865021,
      "learning_rate": 2.2557775637939338e-05,
      "loss": 1.1758,
      "step": 2081
    },
    {
      "epoch": 0.7498649378714208,
      "grad_norm": 0.6103416085243225,
      "learning_rate": 2.2554164660568127e-05,
      "loss": 1.3062,
      "step": 2082
    },
    {
      "epoch": 0.7502251035476319,
      "grad_norm": 0.38664716482162476,
      "learning_rate": 2.255055368319692e-05,
      "loss": 1.4496,
      "step": 2083
    },
    {
      "epoch": 0.7505852692238429,
      "grad_norm": 0.3560032844543457,
      "learning_rate": 2.2546942705825712e-05,
      "loss": 1.2985,
      "step": 2084
    },
    {
      "epoch": 0.750945434900054,
      "grad_norm": 0.3710518181324005,
      "learning_rate": 2.25433317284545e-05,
      "loss": 1.2669,
      "step": 2085
    },
    {
      "epoch": 0.751305600576265,
      "grad_norm": 0.3606862723827362,
      "learning_rate": 2.2539720751083293e-05,
      "loss": 1.2277,
      "step": 2086
    },
    {
      "epoch": 0.7516657662524762,
      "grad_norm": 0.37022721767425537,
      "learning_rate": 2.2536109773712085e-05,
      "loss": 1.2793,
      "step": 2087
    },
    {
      "epoch": 0.7520259319286872,
      "grad_norm": 0.35593631863594055,
      "learning_rate": 2.2532498796340878e-05,
      "loss": 1.1314,
      "step": 2088
    },
    {
      "epoch": 0.7523860976048983,
      "grad_norm": 0.374895304441452,
      "learning_rate": 2.252888781896967e-05,
      "loss": 1.3461,
      "step": 2089
    },
    {
      "epoch": 0.7527462632811093,
      "grad_norm": 0.3577080965042114,
      "learning_rate": 2.252527684159846e-05,
      "loss": 1.2893,
      "step": 2090
    },
    {
      "epoch": 0.7531064289573204,
      "grad_norm": 0.36890366673469543,
      "learning_rate": 2.252166586422725e-05,
      "loss": 1.1948,
      "step": 2091
    },
    {
      "epoch": 0.7534665946335314,
      "grad_norm": 0.35244253277778625,
      "learning_rate": 2.2518054886856043e-05,
      "loss": 1.2556,
      "step": 2092
    },
    {
      "epoch": 0.7538267603097425,
      "grad_norm": 0.3803718090057373,
      "learning_rate": 2.2514443909484832e-05,
      "loss": 1.3735,
      "step": 2093
    },
    {
      "epoch": 0.7541869259859535,
      "grad_norm": 0.34710514545440674,
      "learning_rate": 2.2510832932113628e-05,
      "loss": 1.2332,
      "step": 2094
    },
    {
      "epoch": 0.7545470916621646,
      "grad_norm": 0.3725474178791046,
      "learning_rate": 2.2507221954742417e-05,
      "loss": 1.3957,
      "step": 2095
    },
    {
      "epoch": 0.7549072573383756,
      "grad_norm": 0.37169045209884644,
      "learning_rate": 2.250361097737121e-05,
      "loss": 1.1418,
      "step": 2096
    },
    {
      "epoch": 0.7552674230145867,
      "grad_norm": 0.36796754598617554,
      "learning_rate": 2.25e-05,
      "loss": 1.3333,
      "step": 2097
    },
    {
      "epoch": 0.7556275886907977,
      "grad_norm": 0.3638421893119812,
      "learning_rate": 2.249638902262879e-05,
      "loss": 1.2437,
      "step": 2098
    },
    {
      "epoch": 0.7559877543670088,
      "grad_norm": 0.3801572918891907,
      "learning_rate": 2.2492778045257583e-05,
      "loss": 1.2797,
      "step": 2099
    },
    {
      "epoch": 0.7563479200432199,
      "grad_norm": 0.37246906757354736,
      "learning_rate": 2.2489167067886375e-05,
      "loss": 1.2354,
      "step": 2100
    },
    {
      "epoch": 0.7567080857194309,
      "grad_norm": 0.36413300037384033,
      "learning_rate": 2.2485556090515167e-05,
      "loss": 1.2954,
      "step": 2101
    },
    {
      "epoch": 0.757068251395642,
      "grad_norm": 0.40806594491004944,
      "learning_rate": 2.248194511314396e-05,
      "loss": 1.3726,
      "step": 2102
    },
    {
      "epoch": 0.757428417071853,
      "grad_norm": 0.3750029504299164,
      "learning_rate": 2.247833413577275e-05,
      "loss": 1.2843,
      "step": 2103
    },
    {
      "epoch": 0.7577885827480642,
      "grad_norm": 0.3630611300468445,
      "learning_rate": 2.247472315840154e-05,
      "loss": 1.3435,
      "step": 2104
    },
    {
      "epoch": 0.7581487484242752,
      "grad_norm": 0.36097925901412964,
      "learning_rate": 2.2471112181030333e-05,
      "loss": 1.1736,
      "step": 2105
    },
    {
      "epoch": 0.7585089141004863,
      "grad_norm": 0.35727033019065857,
      "learning_rate": 2.2467501203659122e-05,
      "loss": 1.322,
      "step": 2106
    },
    {
      "epoch": 0.7588690797766973,
      "grad_norm": 0.35737237334251404,
      "learning_rate": 2.2463890226287914e-05,
      "loss": 1.2384,
      "step": 2107
    },
    {
      "epoch": 0.7592292454529084,
      "grad_norm": 0.3867720663547516,
      "learning_rate": 2.246027924891671e-05,
      "loss": 1.262,
      "step": 2108
    },
    {
      "epoch": 0.7595894111291194,
      "grad_norm": 0.3604031503200531,
      "learning_rate": 2.24566682715455e-05,
      "loss": 1.2561,
      "step": 2109
    },
    {
      "epoch": 0.7599495768053305,
      "grad_norm": 0.3751315474510193,
      "learning_rate": 2.245305729417429e-05,
      "loss": 1.3228,
      "step": 2110
    },
    {
      "epoch": 0.7603097424815415,
      "grad_norm": 0.3562324047088623,
      "learning_rate": 2.244944631680308e-05,
      "loss": 1.2708,
      "step": 2111
    },
    {
      "epoch": 0.7606699081577526,
      "grad_norm": 0.36080142855644226,
      "learning_rate": 2.2445835339431872e-05,
      "loss": 1.1623,
      "step": 2112
    },
    {
      "epoch": 0.7610300738339636,
      "grad_norm": 0.3486359715461731,
      "learning_rate": 2.2442224362060665e-05,
      "loss": 1.2549,
      "step": 2113
    },
    {
      "epoch": 0.7613902395101747,
      "grad_norm": 0.39812415838241577,
      "learning_rate": 2.2438613384689457e-05,
      "loss": 1.3519,
      "step": 2114
    },
    {
      "epoch": 0.7617504051863857,
      "grad_norm": 0.35943666100502014,
      "learning_rate": 2.243500240731825e-05,
      "loss": 1.1224,
      "step": 2115
    },
    {
      "epoch": 0.7621105708625968,
      "grad_norm": 0.36846083402633667,
      "learning_rate": 2.243139142994704e-05,
      "loss": 1.3953,
      "step": 2116
    },
    {
      "epoch": 0.7624707365388078,
      "grad_norm": 0.37696394324302673,
      "learning_rate": 2.242778045257583e-05,
      "loss": 1.3332,
      "step": 2117
    },
    {
      "epoch": 0.7628309022150189,
      "grad_norm": 0.3624876141548157,
      "learning_rate": 2.2424169475204623e-05,
      "loss": 1.2448,
      "step": 2118
    },
    {
      "epoch": 0.7631910678912299,
      "grad_norm": 0.35467663407325745,
      "learning_rate": 2.242055849783341e-05,
      "loss": 1.1928,
      "step": 2119
    },
    {
      "epoch": 0.763551233567441,
      "grad_norm": 0.3654315769672394,
      "learning_rate": 2.2416947520462204e-05,
      "loss": 1.214,
      "step": 2120
    },
    {
      "epoch": 0.763911399243652,
      "grad_norm": 0.36989885568618774,
      "learning_rate": 2.2413336543091e-05,
      "loss": 1.2981,
      "step": 2121
    },
    {
      "epoch": 0.7642715649198631,
      "grad_norm": 0.36614322662353516,
      "learning_rate": 2.240972556571979e-05,
      "loss": 1.3165,
      "step": 2122
    },
    {
      "epoch": 0.7646317305960741,
      "grad_norm": 0.3705850839614868,
      "learning_rate": 2.240611458834858e-05,
      "loss": 1.2859,
      "step": 2123
    },
    {
      "epoch": 0.7649918962722853,
      "grad_norm": 0.38205453753471375,
      "learning_rate": 2.2402503610977373e-05,
      "loss": 1.3729,
      "step": 2124
    },
    {
      "epoch": 0.7653520619484963,
      "grad_norm": 0.3716875910758972,
      "learning_rate": 2.2398892633606162e-05,
      "loss": 1.3274,
      "step": 2125
    },
    {
      "epoch": 0.7657122276247074,
      "grad_norm": 0.35994192957878113,
      "learning_rate": 2.2395281656234954e-05,
      "loss": 1.3446,
      "step": 2126
    },
    {
      "epoch": 0.7660723933009185,
      "grad_norm": 0.36445531249046326,
      "learning_rate": 2.2391670678863743e-05,
      "loss": 1.223,
      "step": 2127
    },
    {
      "epoch": 0.7664325589771295,
      "grad_norm": 0.36296752095222473,
      "learning_rate": 2.238805970149254e-05,
      "loss": 1.3592,
      "step": 2128
    },
    {
      "epoch": 0.7667927246533406,
      "grad_norm": 0.36340585350990295,
      "learning_rate": 2.238444872412133e-05,
      "loss": 1.2769,
      "step": 2129
    },
    {
      "epoch": 0.7671528903295516,
      "grad_norm": 0.3570233881473541,
      "learning_rate": 2.238083774675012e-05,
      "loss": 1.3442,
      "step": 2130
    },
    {
      "epoch": 0.7675130560057627,
      "grad_norm": 0.36034879088401794,
      "learning_rate": 2.2377226769378912e-05,
      "loss": 1.1966,
      "step": 2131
    },
    {
      "epoch": 0.7678732216819737,
      "grad_norm": 0.3562391400337219,
      "learning_rate": 2.2373615792007705e-05,
      "loss": 1.2439,
      "step": 2132
    },
    {
      "epoch": 0.7682333873581848,
      "grad_norm": 0.35972997546195984,
      "learning_rate": 2.2370004814636494e-05,
      "loss": 1.4059,
      "step": 2133
    },
    {
      "epoch": 0.7685935530343958,
      "grad_norm": 0.3653584122657776,
      "learning_rate": 2.2366393837265286e-05,
      "loss": 1.2791,
      "step": 2134
    },
    {
      "epoch": 0.7689537187106069,
      "grad_norm": 0.34948477149009705,
      "learning_rate": 2.2362782859894078e-05,
      "loss": 1.2889,
      "step": 2135
    },
    {
      "epoch": 0.7693138843868179,
      "grad_norm": 0.37857675552368164,
      "learning_rate": 2.235917188252287e-05,
      "loss": 1.2946,
      "step": 2136
    },
    {
      "epoch": 0.769674050063029,
      "grad_norm": 0.36902475357055664,
      "learning_rate": 2.2355560905151663e-05,
      "loss": 1.2721,
      "step": 2137
    },
    {
      "epoch": 0.77003421573924,
      "grad_norm": 0.356791228055954,
      "learning_rate": 2.235194992778045e-05,
      "loss": 1.2017,
      "step": 2138
    },
    {
      "epoch": 0.7703943814154511,
      "grad_norm": 0.3645646870136261,
      "learning_rate": 2.2348338950409244e-05,
      "loss": 1.3797,
      "step": 2139
    },
    {
      "epoch": 0.7707545470916621,
      "grad_norm": 0.36662983894348145,
      "learning_rate": 2.2344727973038036e-05,
      "loss": 1.3662,
      "step": 2140
    },
    {
      "epoch": 0.7711147127678732,
      "grad_norm": 0.34242093563079834,
      "learning_rate": 2.234111699566683e-05,
      "loss": 1.2893,
      "step": 2141
    },
    {
      "epoch": 0.7714748784440842,
      "grad_norm": 0.3761211931705475,
      "learning_rate": 2.233750601829562e-05,
      "loss": 1.4043,
      "step": 2142
    },
    {
      "epoch": 0.7718350441202954,
      "grad_norm": 0.35718557238578796,
      "learning_rate": 2.233389504092441e-05,
      "loss": 1.2583,
      "step": 2143
    },
    {
      "epoch": 0.7721952097965064,
      "grad_norm": 0.3793759346008301,
      "learning_rate": 2.2330284063553202e-05,
      "loss": 1.261,
      "step": 2144
    },
    {
      "epoch": 0.7725553754727175,
      "grad_norm": 0.36642393469810486,
      "learning_rate": 2.2326673086181994e-05,
      "loss": 1.2579,
      "step": 2145
    },
    {
      "epoch": 0.7729155411489285,
      "grad_norm": 0.36155804991722107,
      "learning_rate": 2.2323062108810783e-05,
      "loss": 1.3623,
      "step": 2146
    },
    {
      "epoch": 0.7732757068251396,
      "grad_norm": 0.38721275329589844,
      "learning_rate": 2.2319451131439576e-05,
      "loss": 1.3749,
      "step": 2147
    },
    {
      "epoch": 0.7736358725013506,
      "grad_norm": 0.3618333637714386,
      "learning_rate": 2.231584015406837e-05,
      "loss": 1.3749,
      "step": 2148
    },
    {
      "epoch": 0.7739960381775617,
      "grad_norm": 0.36555004119873047,
      "learning_rate": 2.231222917669716e-05,
      "loss": 1.3812,
      "step": 2149
    },
    {
      "epoch": 0.7743562038537727,
      "grad_norm": 0.3515966832637787,
      "learning_rate": 2.2308618199325952e-05,
      "loss": 1.1664,
      "step": 2150
    },
    {
      "epoch": 0.7747163695299838,
      "grad_norm": 0.38138508796691895,
      "learning_rate": 2.230500722195474e-05,
      "loss": 1.25,
      "step": 2151
    },
    {
      "epoch": 0.7750765352061948,
      "grad_norm": 0.3672455847263336,
      "learning_rate": 2.2301396244583534e-05,
      "loss": 1.3634,
      "step": 2152
    },
    {
      "epoch": 0.7754367008824059,
      "grad_norm": 0.3709114193916321,
      "learning_rate": 2.2297785267212326e-05,
      "loss": 1.2036,
      "step": 2153
    },
    {
      "epoch": 0.775796866558617,
      "grad_norm": 0.37332478165626526,
      "learning_rate": 2.2294174289841115e-05,
      "loss": 1.2387,
      "step": 2154
    },
    {
      "epoch": 0.776157032234828,
      "grad_norm": 0.37806445360183716,
      "learning_rate": 2.229056331246991e-05,
      "loss": 1.2923,
      "step": 2155
    },
    {
      "epoch": 0.7765171979110391,
      "grad_norm": 0.3933856189250946,
      "learning_rate": 2.2286952335098703e-05,
      "loss": 1.2689,
      "step": 2156
    },
    {
      "epoch": 0.7768773635872501,
      "grad_norm": 0.3672351837158203,
      "learning_rate": 2.2283341357727492e-05,
      "loss": 1.3133,
      "step": 2157
    },
    {
      "epoch": 0.7772375292634612,
      "grad_norm": 0.37156927585601807,
      "learning_rate": 2.2279730380356284e-05,
      "loss": 1.3128,
      "step": 2158
    },
    {
      "epoch": 0.7775976949396722,
      "grad_norm": 0.3634641766548157,
      "learning_rate": 2.2276119402985073e-05,
      "loss": 1.2414,
      "step": 2159
    },
    {
      "epoch": 0.7779578606158833,
      "grad_norm": 0.3591384291648865,
      "learning_rate": 2.2272508425613865e-05,
      "loss": 1.3488,
      "step": 2160
    },
    {
      "epoch": 0.7783180262920943,
      "grad_norm": 0.3807620108127594,
      "learning_rate": 2.2268897448242658e-05,
      "loss": 1.1943,
      "step": 2161
    },
    {
      "epoch": 0.7786781919683055,
      "grad_norm": 0.39102599024772644,
      "learning_rate": 2.226528647087145e-05,
      "loss": 1.4193,
      "step": 2162
    },
    {
      "epoch": 0.7790383576445165,
      "grad_norm": 0.37643060088157654,
      "learning_rate": 2.2261675493500242e-05,
      "loss": 1.3504,
      "step": 2163
    },
    {
      "epoch": 0.7793985233207276,
      "grad_norm": 0.3608745336532593,
      "learning_rate": 2.2258064516129034e-05,
      "loss": 1.3489,
      "step": 2164
    },
    {
      "epoch": 0.7797586889969386,
      "grad_norm": 0.36587241291999817,
      "learning_rate": 2.2254453538757823e-05,
      "loss": 1.1899,
      "step": 2165
    },
    {
      "epoch": 0.7801188546731497,
      "grad_norm": 0.3909503221511841,
      "learning_rate": 2.2250842561386616e-05,
      "loss": 1.3803,
      "step": 2166
    },
    {
      "epoch": 0.7804790203493607,
      "grad_norm": 0.34941065311431885,
      "learning_rate": 2.2247231584015405e-05,
      "loss": 1.2217,
      "step": 2167
    },
    {
      "epoch": 0.7808391860255718,
      "grad_norm": 0.36322662234306335,
      "learning_rate": 2.22436206066442e-05,
      "loss": 1.2504,
      "step": 2168
    },
    {
      "epoch": 0.7811993517017828,
      "grad_norm": 0.37165889143943787,
      "learning_rate": 2.2240009629272993e-05,
      "loss": 1.2724,
      "step": 2169
    },
    {
      "epoch": 0.7815595173779939,
      "grad_norm": 0.36850491166114807,
      "learning_rate": 2.223639865190178e-05,
      "loss": 1.3362,
      "step": 2170
    },
    {
      "epoch": 0.7819196830542049,
      "grad_norm": 0.3527967035770416,
      "learning_rate": 2.2232787674530574e-05,
      "loss": 1.1577,
      "step": 2171
    },
    {
      "epoch": 0.782279848730416,
      "grad_norm": 0.3672482967376709,
      "learning_rate": 2.2229176697159366e-05,
      "loss": 1.4391,
      "step": 2172
    },
    {
      "epoch": 0.782640014406627,
      "grad_norm": 0.37075090408325195,
      "learning_rate": 2.2225565719788155e-05,
      "loss": 1.3003,
      "step": 2173
    },
    {
      "epoch": 0.7830001800828381,
      "grad_norm": 0.37014827132225037,
      "learning_rate": 2.2221954742416947e-05,
      "loss": 1.1947,
      "step": 2174
    },
    {
      "epoch": 0.7833603457590491,
      "grad_norm": 0.3482401371002197,
      "learning_rate": 2.221834376504574e-05,
      "loss": 1.1891,
      "step": 2175
    },
    {
      "epoch": 0.7837205114352602,
      "grad_norm": 0.3723392188549042,
      "learning_rate": 2.2214732787674532e-05,
      "loss": 1.2624,
      "step": 2176
    },
    {
      "epoch": 0.7840806771114712,
      "grad_norm": 0.3754938244819641,
      "learning_rate": 2.2211121810303324e-05,
      "loss": 1.3794,
      "step": 2177
    },
    {
      "epoch": 0.7844408427876823,
      "grad_norm": 0.3799172639846802,
      "learning_rate": 2.2207510832932113e-05,
      "loss": 1.249,
      "step": 2178
    },
    {
      "epoch": 0.7848010084638934,
      "grad_norm": 0.38073134422302246,
      "learning_rate": 2.2203899855560905e-05,
      "loss": 1.4304,
      "step": 2179
    },
    {
      "epoch": 0.7851611741401044,
      "grad_norm": 0.3707897961139679,
      "learning_rate": 2.2200288878189698e-05,
      "loss": 1.4111,
      "step": 2180
    },
    {
      "epoch": 0.7855213398163156,
      "grad_norm": 0.3744734525680542,
      "learning_rate": 2.2196677900818487e-05,
      "loss": 1.3504,
      "step": 2181
    },
    {
      "epoch": 0.7858815054925266,
      "grad_norm": 0.36122778058052063,
      "learning_rate": 2.2193066923447282e-05,
      "loss": 1.1914,
      "step": 2182
    },
    {
      "epoch": 0.7862416711687377,
      "grad_norm": 0.36603111028671265,
      "learning_rate": 2.218945594607607e-05,
      "loss": 1.2198,
      "step": 2183
    },
    {
      "epoch": 0.7866018368449487,
      "grad_norm": 0.34913164377212524,
      "learning_rate": 2.2185844968704863e-05,
      "loss": 1.3187,
      "step": 2184
    },
    {
      "epoch": 0.7869620025211598,
      "grad_norm": 0.36692532896995544,
      "learning_rate": 2.2182233991333656e-05,
      "loss": 1.3337,
      "step": 2185
    },
    {
      "epoch": 0.7873221681973708,
      "grad_norm": 0.36389458179473877,
      "learning_rate": 2.2178623013962445e-05,
      "loss": 1.2489,
      "step": 2186
    },
    {
      "epoch": 0.7876823338735819,
      "grad_norm": 0.3685361444950104,
      "learning_rate": 2.2175012036591237e-05,
      "loss": 1.3654,
      "step": 2187
    },
    {
      "epoch": 0.7880424995497929,
      "grad_norm": 0.3694969415664673,
      "learning_rate": 2.217140105922003e-05,
      "loss": 1.2335,
      "step": 2188
    },
    {
      "epoch": 0.788402665226004,
      "grad_norm": 0.37219834327697754,
      "learning_rate": 2.216779008184882e-05,
      "loss": 1.2179,
      "step": 2189
    },
    {
      "epoch": 0.788762830902215,
      "grad_norm": 0.36473187804222107,
      "learning_rate": 2.2164179104477614e-05,
      "loss": 1.3317,
      "step": 2190
    },
    {
      "epoch": 0.7891229965784261,
      "grad_norm": 0.37716424465179443,
      "learning_rate": 2.2160568127106403e-05,
      "loss": 1.346,
      "step": 2191
    },
    {
      "epoch": 0.7894831622546371,
      "grad_norm": 0.3578391671180725,
      "learning_rate": 2.2156957149735195e-05,
      "loss": 1.2609,
      "step": 2192
    },
    {
      "epoch": 0.7898433279308482,
      "grad_norm": 0.381804883480072,
      "learning_rate": 2.2153346172363987e-05,
      "loss": 1.4359,
      "step": 2193
    },
    {
      "epoch": 0.7902034936070592,
      "grad_norm": 0.3905530571937561,
      "learning_rate": 2.2149735194992776e-05,
      "loss": 1.3118,
      "step": 2194
    },
    {
      "epoch": 0.7905636592832703,
      "grad_norm": 0.3688696622848511,
      "learning_rate": 2.2146124217621572e-05,
      "loss": 1.2883,
      "step": 2195
    },
    {
      "epoch": 0.7909238249594813,
      "grad_norm": 0.36577028036117554,
      "learning_rate": 2.2142513240250364e-05,
      "loss": 1.29,
      "step": 2196
    },
    {
      "epoch": 0.7912839906356924,
      "grad_norm": 0.37215280532836914,
      "learning_rate": 2.2138902262879153e-05,
      "loss": 1.2352,
      "step": 2197
    },
    {
      "epoch": 0.7916441563119034,
      "grad_norm": 0.35576131939888,
      "learning_rate": 2.2135291285507945e-05,
      "loss": 1.2924,
      "step": 2198
    },
    {
      "epoch": 0.7920043219881145,
      "grad_norm": 0.3863578140735626,
      "learning_rate": 2.2131680308136734e-05,
      "loss": 1.2974,
      "step": 2199
    },
    {
      "epoch": 0.7923644876643255,
      "grad_norm": 0.353942334651947,
      "learning_rate": 2.2128069330765527e-05,
      "loss": 1.2333,
      "step": 2200
    },
    {
      "epoch": 0.7927246533405367,
      "grad_norm": 0.377506822347641,
      "learning_rate": 2.212445835339432e-05,
      "loss": 1.5064,
      "step": 2201
    },
    {
      "epoch": 0.7930848190167477,
      "grad_norm": 0.3635999262332916,
      "learning_rate": 2.212084737602311e-05,
      "loss": 1.4523,
      "step": 2202
    },
    {
      "epoch": 0.7934449846929588,
      "grad_norm": 0.36650145053863525,
      "learning_rate": 2.2117236398651904e-05,
      "loss": 1.3203,
      "step": 2203
    },
    {
      "epoch": 0.7938051503691698,
      "grad_norm": 0.3803369700908661,
      "learning_rate": 2.2113625421280696e-05,
      "loss": 1.4856,
      "step": 2204
    },
    {
      "epoch": 0.7941653160453809,
      "grad_norm": 0.3600769340991974,
      "learning_rate": 2.2110014443909485e-05,
      "loss": 1.2282,
      "step": 2205
    },
    {
      "epoch": 0.794525481721592,
      "grad_norm": 0.37432894110679626,
      "learning_rate": 2.2106403466538277e-05,
      "loss": 1.2841,
      "step": 2206
    },
    {
      "epoch": 0.794885647397803,
      "grad_norm": 0.38379475474357605,
      "learning_rate": 2.2102792489167066e-05,
      "loss": 1.3105,
      "step": 2207
    },
    {
      "epoch": 0.7952458130740141,
      "grad_norm": 0.3683529794216156,
      "learning_rate": 2.2099181511795858e-05,
      "loss": 1.3604,
      "step": 2208
    },
    {
      "epoch": 0.7956059787502251,
      "grad_norm": 0.38881567120552063,
      "learning_rate": 2.2095570534424654e-05,
      "loss": 1.2032,
      "step": 2209
    },
    {
      "epoch": 0.7959661444264362,
      "grad_norm": 0.3634547293186188,
      "learning_rate": 2.2091959557053443e-05,
      "loss": 1.1909,
      "step": 2210
    },
    {
      "epoch": 0.7963263101026472,
      "grad_norm": 0.3674564063549042,
      "learning_rate": 2.2088348579682235e-05,
      "loss": 1.2836,
      "step": 2211
    },
    {
      "epoch": 0.7966864757788583,
      "grad_norm": 0.3830896019935608,
      "learning_rate": 2.2084737602311027e-05,
      "loss": 1.3184,
      "step": 2212
    },
    {
      "epoch": 0.7970466414550693,
      "grad_norm": 0.3893260359764099,
      "learning_rate": 2.2081126624939816e-05,
      "loss": 1.4182,
      "step": 2213
    },
    {
      "epoch": 0.7974068071312804,
      "grad_norm": 0.37390851974487305,
      "learning_rate": 2.207751564756861e-05,
      "loss": 1.3161,
      "step": 2214
    },
    {
      "epoch": 0.7977669728074914,
      "grad_norm": 0.3866131603717804,
      "learning_rate": 2.2073904670197398e-05,
      "loss": 1.1796,
      "step": 2215
    },
    {
      "epoch": 0.7981271384837025,
      "grad_norm": 0.3683534860610962,
      "learning_rate": 2.2070293692826193e-05,
      "loss": 1.3602,
      "step": 2216
    },
    {
      "epoch": 0.7984873041599135,
      "grad_norm": 0.38273078203201294,
      "learning_rate": 2.2066682715454986e-05,
      "loss": 1.3516,
      "step": 2217
    },
    {
      "epoch": 0.7988474698361246,
      "grad_norm": 0.37401679158210754,
      "learning_rate": 2.2063071738083774e-05,
      "loss": 1.3677,
      "step": 2218
    },
    {
      "epoch": 0.7992076355123356,
      "grad_norm": 0.357864648103714,
      "learning_rate": 2.2059460760712567e-05,
      "loss": 1.2274,
      "step": 2219
    },
    {
      "epoch": 0.7995678011885468,
      "grad_norm": 0.3567257225513458,
      "learning_rate": 2.205584978334136e-05,
      "loss": 1.2417,
      "step": 2220
    },
    {
      "epoch": 0.7999279668647578,
      "grad_norm": 0.3827500343322754,
      "learning_rate": 2.2052238805970148e-05,
      "loss": 1.2328,
      "step": 2221
    },
    {
      "epoch": 0.8002881325409689,
      "grad_norm": 0.3736054599285126,
      "learning_rate": 2.2048627828598944e-05,
      "loss": 1.271,
      "step": 2222
    },
    {
      "epoch": 0.8006482982171799,
      "grad_norm": 0.35585081577301025,
      "learning_rate": 2.2045016851227733e-05,
      "loss": 1.1495,
      "step": 2223
    },
    {
      "epoch": 0.801008463893391,
      "grad_norm": 0.36480745673179626,
      "learning_rate": 2.2041405873856525e-05,
      "loss": 1.1859,
      "step": 2224
    },
    {
      "epoch": 0.801368629569602,
      "grad_norm": 0.35870593786239624,
      "learning_rate": 2.2037794896485317e-05,
      "loss": 1.3066,
      "step": 2225
    },
    {
      "epoch": 0.8017287952458131,
      "grad_norm": 0.35550162196159363,
      "learning_rate": 2.2034183919114106e-05,
      "loss": 1.3007,
      "step": 2226
    },
    {
      "epoch": 0.8020889609220241,
      "grad_norm": 0.3648146092891693,
      "learning_rate": 2.20305729417429e-05,
      "loss": 1.2113,
      "step": 2227
    },
    {
      "epoch": 0.8024491265982352,
      "grad_norm": 0.3657155930995941,
      "learning_rate": 2.202696196437169e-05,
      "loss": 1.2424,
      "step": 2228
    },
    {
      "epoch": 0.8028092922744462,
      "grad_norm": 0.36446282267570496,
      "learning_rate": 2.2023350987000483e-05,
      "loss": 1.3559,
      "step": 2229
    },
    {
      "epoch": 0.8031694579506573,
      "grad_norm": 0.37151041626930237,
      "learning_rate": 2.2019740009629275e-05,
      "loss": 1.3054,
      "step": 2230
    },
    {
      "epoch": 0.8035296236268683,
      "grad_norm": 0.3736301362514496,
      "learning_rate": 2.2016129032258064e-05,
      "loss": 1.2394,
      "step": 2231
    },
    {
      "epoch": 0.8038897893030794,
      "grad_norm": 0.3705499470233917,
      "learning_rate": 2.2012518054886856e-05,
      "loss": 1.1578,
      "step": 2232
    },
    {
      "epoch": 0.8042499549792905,
      "grad_norm": 0.368081659078598,
      "learning_rate": 2.200890707751565e-05,
      "loss": 1.2618,
      "step": 2233
    },
    {
      "epoch": 0.8046101206555015,
      "grad_norm": 0.359274297952652,
      "learning_rate": 2.2005296100144438e-05,
      "loss": 1.1558,
      "step": 2234
    },
    {
      "epoch": 0.8049702863317126,
      "grad_norm": 0.3641323745250702,
      "learning_rate": 2.200168512277323e-05,
      "loss": 1.2154,
      "step": 2235
    },
    {
      "epoch": 0.8053304520079236,
      "grad_norm": 0.3705698847770691,
      "learning_rate": 2.1998074145402026e-05,
      "loss": 1.3966,
      "step": 2236
    },
    {
      "epoch": 0.8056906176841347,
      "grad_norm": 0.3652694821357727,
      "learning_rate": 2.1994463168030815e-05,
      "loss": 1.246,
      "step": 2237
    },
    {
      "epoch": 0.8060507833603457,
      "grad_norm": 0.370334655046463,
      "learning_rate": 2.1990852190659607e-05,
      "loss": 1.2286,
      "step": 2238
    },
    {
      "epoch": 0.8064109490365569,
      "grad_norm": 0.39295583963394165,
      "learning_rate": 2.1987241213288396e-05,
      "loss": 1.195,
      "step": 2239
    },
    {
      "epoch": 0.8067711147127679,
      "grad_norm": 0.3551843762397766,
      "learning_rate": 2.1983630235917188e-05,
      "loss": 1.228,
      "step": 2240
    },
    {
      "epoch": 0.807131280388979,
      "grad_norm": 0.37325647473335266,
      "learning_rate": 2.198001925854598e-05,
      "loss": 1.3721,
      "step": 2241
    },
    {
      "epoch": 0.80749144606519,
      "grad_norm": 0.38173356652259827,
      "learning_rate": 2.1976408281174773e-05,
      "loss": 1.3806,
      "step": 2242
    },
    {
      "epoch": 0.8078516117414011,
      "grad_norm": 0.3660289943218231,
      "learning_rate": 2.1972797303803565e-05,
      "loss": 1.2636,
      "step": 2243
    },
    {
      "epoch": 0.8082117774176121,
      "grad_norm": 0.35802116990089417,
      "learning_rate": 2.1969186326432357e-05,
      "loss": 1.3576,
      "step": 2244
    },
    {
      "epoch": 0.8085719430938232,
      "grad_norm": 0.39657023549079895,
      "learning_rate": 2.1965575349061146e-05,
      "loss": 1.3441,
      "step": 2245
    },
    {
      "epoch": 0.8089321087700342,
      "grad_norm": 0.39472123980522156,
      "learning_rate": 2.196196437168994e-05,
      "loss": 1.4367,
      "step": 2246
    },
    {
      "epoch": 0.8092922744462453,
      "grad_norm": 0.3555290400981903,
      "learning_rate": 2.1958353394318727e-05,
      "loss": 1.2131,
      "step": 2247
    },
    {
      "epoch": 0.8096524401224563,
      "grad_norm": 0.36984774470329285,
      "learning_rate": 2.195474241694752e-05,
      "loss": 1.3001,
      "step": 2248
    },
    {
      "epoch": 0.8100126057986674,
      "grad_norm": 0.371025413274765,
      "learning_rate": 2.1951131439576315e-05,
      "loss": 1.384,
      "step": 2249
    },
    {
      "epoch": 0.8103727714748784,
      "grad_norm": 0.3724697530269623,
      "learning_rate": 2.1947520462205104e-05,
      "loss": 1.2316,
      "step": 2250
    },
    {
      "epoch": 0.8107329371510895,
      "grad_norm": 0.3889002203941345,
      "learning_rate": 2.1943909484833896e-05,
      "loss": 1.3906,
      "step": 2251
    },
    {
      "epoch": 0.8110931028273005,
      "grad_norm": 0.4022026062011719,
      "learning_rate": 2.194029850746269e-05,
      "loss": 1.2459,
      "step": 2252
    },
    {
      "epoch": 0.8114532685035116,
      "grad_norm": 0.36947256326675415,
      "learning_rate": 2.1936687530091478e-05,
      "loss": 1.3123,
      "step": 2253
    },
    {
      "epoch": 0.8118134341797226,
      "grad_norm": 0.4014294147491455,
      "learning_rate": 2.193307655272027e-05,
      "loss": 1.3658,
      "step": 2254
    },
    {
      "epoch": 0.8121735998559337,
      "grad_norm": 0.3738139271736145,
      "learning_rate": 2.192946557534906e-05,
      "loss": 1.2513,
      "step": 2255
    },
    {
      "epoch": 0.8125337655321447,
      "grad_norm": 0.37730130553245544,
      "learning_rate": 2.1925854597977855e-05,
      "loss": 1.4034,
      "step": 2256
    },
    {
      "epoch": 0.8128939312083558,
      "grad_norm": 0.37135401368141174,
      "learning_rate": 2.1922243620606647e-05,
      "loss": 1.3484,
      "step": 2257
    },
    {
      "epoch": 0.8132540968845668,
      "grad_norm": 0.368752658367157,
      "learning_rate": 2.1918632643235436e-05,
      "loss": 1.1626,
      "step": 2258
    },
    {
      "epoch": 0.813614262560778,
      "grad_norm": 0.36422982811927795,
      "learning_rate": 2.1915021665864228e-05,
      "loss": 1.2508,
      "step": 2259
    },
    {
      "epoch": 0.8139744282369891,
      "grad_norm": 0.38391172885894775,
      "learning_rate": 2.191141068849302e-05,
      "loss": 1.225,
      "step": 2260
    },
    {
      "epoch": 0.8143345939132001,
      "grad_norm": 0.3861698508262634,
      "learning_rate": 2.190779971112181e-05,
      "loss": 1.2196,
      "step": 2261
    },
    {
      "epoch": 0.8146947595894112,
      "grad_norm": 0.39517319202423096,
      "learning_rate": 2.19041887337506e-05,
      "loss": 1.2348,
      "step": 2262
    },
    {
      "epoch": 0.8150549252656222,
      "grad_norm": 0.37673458456993103,
      "learning_rate": 2.1900577756379394e-05,
      "loss": 1.3163,
      "step": 2263
    },
    {
      "epoch": 0.8154150909418333,
      "grad_norm": 0.35795167088508606,
      "learning_rate": 2.1896966779008186e-05,
      "loss": 1.1678,
      "step": 2264
    },
    {
      "epoch": 0.8157752566180443,
      "grad_norm": 0.3899802267551422,
      "learning_rate": 2.189335580163698e-05,
      "loss": 1.3562,
      "step": 2265
    },
    {
      "epoch": 0.8161354222942554,
      "grad_norm": 0.3606242835521698,
      "learning_rate": 2.1889744824265767e-05,
      "loss": 1.301,
      "step": 2266
    },
    {
      "epoch": 0.8164955879704664,
      "grad_norm": 0.37026605010032654,
      "learning_rate": 2.188613384689456e-05,
      "loss": 1.2294,
      "step": 2267
    },
    {
      "epoch": 0.8168557536466775,
      "grad_norm": 0.38124263286590576,
      "learning_rate": 2.1882522869523352e-05,
      "loss": 1.3343,
      "step": 2268
    },
    {
      "epoch": 0.8172159193228885,
      "grad_norm": 0.3866482973098755,
      "learning_rate": 2.1878911892152144e-05,
      "loss": 1.2262,
      "step": 2269
    },
    {
      "epoch": 0.8175760849990996,
      "grad_norm": 0.3617386817932129,
      "learning_rate": 2.1875300914780937e-05,
      "loss": 1.276,
      "step": 2270
    },
    {
      "epoch": 0.8179362506753106,
      "grad_norm": 0.3734467923641205,
      "learning_rate": 2.1871689937409725e-05,
      "loss": 1.2009,
      "step": 2271
    },
    {
      "epoch": 0.8182964163515217,
      "grad_norm": 0.39375385642051697,
      "learning_rate": 2.1868078960038518e-05,
      "loss": 1.3732,
      "step": 2272
    },
    {
      "epoch": 0.8186565820277327,
      "grad_norm": 0.3611752688884735,
      "learning_rate": 2.186446798266731e-05,
      "loss": 1.2145,
      "step": 2273
    },
    {
      "epoch": 0.8190167477039438,
      "grad_norm": 0.36395201086997986,
      "learning_rate": 2.18608570052961e-05,
      "loss": 1.2635,
      "step": 2274
    },
    {
      "epoch": 0.8193769133801548,
      "grad_norm": 0.3601526618003845,
      "learning_rate": 2.185724602792489e-05,
      "loss": 1.328,
      "step": 2275
    },
    {
      "epoch": 0.819737079056366,
      "grad_norm": 0.3700712025165558,
      "learning_rate": 2.1853635050553684e-05,
      "loss": 1.2864,
      "step": 2276
    },
    {
      "epoch": 0.820097244732577,
      "grad_norm": 0.3548726737499237,
      "learning_rate": 2.1850024073182476e-05,
      "loss": 1.4028,
      "step": 2277
    },
    {
      "epoch": 0.8204574104087881,
      "grad_norm": 0.38026028871536255,
      "learning_rate": 2.1846413095811268e-05,
      "loss": 1.304,
      "step": 2278
    },
    {
      "epoch": 0.8208175760849991,
      "grad_norm": 0.3601439297199249,
      "learning_rate": 2.1842802118440057e-05,
      "loss": 1.3379,
      "step": 2279
    },
    {
      "epoch": 0.8211777417612102,
      "grad_norm": 0.36425021290779114,
      "learning_rate": 2.183919114106885e-05,
      "loss": 1.2421,
      "step": 2280
    },
    {
      "epoch": 0.8215379074374212,
      "grad_norm": 0.36481714248657227,
      "learning_rate": 2.183558016369764e-05,
      "loss": 1.3068,
      "step": 2281
    },
    {
      "epoch": 0.8218980731136323,
      "grad_norm": 0.35902753472328186,
      "learning_rate": 2.183196918632643e-05,
      "loss": 1.1875,
      "step": 2282
    },
    {
      "epoch": 0.8222582387898433,
      "grad_norm": 0.36101579666137695,
      "learning_rate": 2.1828358208955226e-05,
      "loss": 1.3872,
      "step": 2283
    },
    {
      "epoch": 0.8226184044660544,
      "grad_norm": 0.3711017668247223,
      "learning_rate": 2.1824747231584015e-05,
      "loss": 1.3273,
      "step": 2284
    },
    {
      "epoch": 0.8229785701422655,
      "grad_norm": 0.38763517141342163,
      "learning_rate": 2.1821136254212807e-05,
      "loss": 1.3768,
      "step": 2285
    },
    {
      "epoch": 0.8233387358184765,
      "grad_norm": 0.36560532450675964,
      "learning_rate": 2.18175252768416e-05,
      "loss": 1.2762,
      "step": 2286
    },
    {
      "epoch": 0.8236989014946876,
      "grad_norm": 0.37715622782707214,
      "learning_rate": 2.181391429947039e-05,
      "loss": 1.2347,
      "step": 2287
    },
    {
      "epoch": 0.8240590671708986,
      "grad_norm": 0.36827927827835083,
      "learning_rate": 2.181030332209918e-05,
      "loss": 1.2697,
      "step": 2288
    },
    {
      "epoch": 0.8244192328471097,
      "grad_norm": 0.3674640357494354,
      "learning_rate": 2.1806692344727973e-05,
      "loss": 1.2731,
      "step": 2289
    },
    {
      "epoch": 0.8247793985233207,
      "grad_norm": 0.38053110241889954,
      "learning_rate": 2.1803081367356766e-05,
      "loss": 1.3364,
      "step": 2290
    },
    {
      "epoch": 0.8251395641995318,
      "grad_norm": 0.39603090286254883,
      "learning_rate": 2.1799470389985558e-05,
      "loss": 1.382,
      "step": 2291
    },
    {
      "epoch": 0.8254997298757428,
      "grad_norm": 0.3743137717247009,
      "learning_rate": 2.1795859412614347e-05,
      "loss": 1.298,
      "step": 2292
    },
    {
      "epoch": 0.8258598955519539,
      "grad_norm": 0.3688012659549713,
      "learning_rate": 2.179224843524314e-05,
      "loss": 1.3383,
      "step": 2293
    },
    {
      "epoch": 0.8262200612281649,
      "grad_norm": 0.35791584849357605,
      "learning_rate": 2.178863745787193e-05,
      "loss": 1.3686,
      "step": 2294
    },
    {
      "epoch": 0.826580226904376,
      "grad_norm": 0.38227930665016174,
      "learning_rate": 2.178502648050072e-05,
      "loss": 1.2577,
      "step": 2295
    },
    {
      "epoch": 0.826940392580587,
      "grad_norm": 0.3825077414512634,
      "learning_rate": 2.1781415503129516e-05,
      "loss": 1.2401,
      "step": 2296
    },
    {
      "epoch": 0.8273005582567982,
      "grad_norm": 0.38083699345588684,
      "learning_rate": 2.1777804525758308e-05,
      "loss": 1.4426,
      "step": 2297
    },
    {
      "epoch": 0.8276607239330092,
      "grad_norm": 0.36218810081481934,
      "learning_rate": 2.1774193548387097e-05,
      "loss": 1.2856,
      "step": 2298
    },
    {
      "epoch": 0.8280208896092203,
      "grad_norm": 0.3845292627811432,
      "learning_rate": 2.177058257101589e-05,
      "loss": 1.3625,
      "step": 2299
    },
    {
      "epoch": 0.8283810552854313,
      "grad_norm": 0.37167736887931824,
      "learning_rate": 2.176697159364468e-05,
      "loss": 1.312,
      "step": 2300
    },
    {
      "epoch": 0.8287412209616424,
      "grad_norm": 0.3573421835899353,
      "learning_rate": 2.176336061627347e-05,
      "loss": 1.3638,
      "step": 2301
    },
    {
      "epoch": 0.8291013866378534,
      "grad_norm": 0.381544291973114,
      "learning_rate": 2.1759749638902263e-05,
      "loss": 1.2481,
      "step": 2302
    },
    {
      "epoch": 0.8294615523140645,
      "grad_norm": 0.3783189356327057,
      "learning_rate": 2.1756138661531055e-05,
      "loss": 1.3079,
      "step": 2303
    },
    {
      "epoch": 0.8298217179902755,
      "grad_norm": 0.3886899948120117,
      "learning_rate": 2.1752527684159848e-05,
      "loss": 1.3337,
      "step": 2304
    },
    {
      "epoch": 0.8301818836664866,
      "grad_norm": 0.37314796447753906,
      "learning_rate": 2.174891670678864e-05,
      "loss": 1.1982,
      "step": 2305
    },
    {
      "epoch": 0.8305420493426976,
      "grad_norm": 0.39671778678894043,
      "learning_rate": 2.174530572941743e-05,
      "loss": 1.4652,
      "step": 2306
    },
    {
      "epoch": 0.8309022150189087,
      "grad_norm": 0.35716310143470764,
      "learning_rate": 2.174169475204622e-05,
      "loss": 1.3135,
      "step": 2307
    },
    {
      "epoch": 0.8312623806951197,
      "grad_norm": 0.34662431478500366,
      "learning_rate": 2.173808377467501e-05,
      "loss": 1.2138,
      "step": 2308
    },
    {
      "epoch": 0.8316225463713308,
      "grad_norm": 0.37645068764686584,
      "learning_rate": 2.1734472797303802e-05,
      "loss": 1.3688,
      "step": 2309
    },
    {
      "epoch": 0.8319827120475418,
      "grad_norm": 0.4024280607700348,
      "learning_rate": 2.1730861819932598e-05,
      "loss": 1.2799,
      "step": 2310
    },
    {
      "epoch": 0.8323428777237529,
      "grad_norm": 0.3745925724506378,
      "learning_rate": 2.1727250842561387e-05,
      "loss": 1.3667,
      "step": 2311
    },
    {
      "epoch": 0.832703043399964,
      "grad_norm": 0.38201379776000977,
      "learning_rate": 2.172363986519018e-05,
      "loss": 1.2522,
      "step": 2312
    },
    {
      "epoch": 0.833063209076175,
      "grad_norm": 0.37580251693725586,
      "learning_rate": 2.172002888781897e-05,
      "loss": 1.2963,
      "step": 2313
    },
    {
      "epoch": 0.8334233747523861,
      "grad_norm": 0.3782270550727844,
      "learning_rate": 2.171641791044776e-05,
      "loss": 1.2483,
      "step": 2314
    },
    {
      "epoch": 0.8337835404285971,
      "grad_norm": 0.3516583740711212,
      "learning_rate": 2.1712806933076553e-05,
      "loss": 1.2744,
      "step": 2315
    },
    {
      "epoch": 0.8341437061048083,
      "grad_norm": 0.3827676475048065,
      "learning_rate": 2.170919595570534e-05,
      "loss": 1.3593,
      "step": 2316
    },
    {
      "epoch": 0.8345038717810193,
      "grad_norm": 0.3807932436466217,
      "learning_rate": 2.1705584978334137e-05,
      "loss": 1.2755,
      "step": 2317
    },
    {
      "epoch": 0.8348640374572304,
      "grad_norm": 0.3535647988319397,
      "learning_rate": 2.170197400096293e-05,
      "loss": 1.1353,
      "step": 2318
    },
    {
      "epoch": 0.8352242031334414,
      "grad_norm": 0.37024325132369995,
      "learning_rate": 2.169836302359172e-05,
      "loss": 1.1931,
      "step": 2319
    },
    {
      "epoch": 0.8355843688096525,
      "grad_norm": 0.3672648072242737,
      "learning_rate": 2.169475204622051e-05,
      "loss": 1.3169,
      "step": 2320
    },
    {
      "epoch": 0.8359445344858635,
      "grad_norm": 0.37034833431243896,
      "learning_rate": 2.1691141068849303e-05,
      "loss": 1.1596,
      "step": 2321
    },
    {
      "epoch": 0.8363047001620746,
      "grad_norm": 0.3762761354446411,
      "learning_rate": 2.1687530091478092e-05,
      "loss": 1.1864,
      "step": 2322
    },
    {
      "epoch": 0.8366648658382856,
      "grad_norm": 0.40517663955688477,
      "learning_rate": 2.1683919114106888e-05,
      "loss": 1.3617,
      "step": 2323
    },
    {
      "epoch": 0.8370250315144967,
      "grad_norm": 0.3698565661907196,
      "learning_rate": 2.1680308136735677e-05,
      "loss": 1.215,
      "step": 2324
    },
    {
      "epoch": 0.8373851971907077,
      "grad_norm": 0.379871666431427,
      "learning_rate": 2.167669715936447e-05,
      "loss": 1.3724,
      "step": 2325
    },
    {
      "epoch": 0.8377453628669188,
      "grad_norm": 0.3941853940486908,
      "learning_rate": 2.167308618199326e-05,
      "loss": 1.4555,
      "step": 2326
    },
    {
      "epoch": 0.8381055285431298,
      "grad_norm": 0.37663161754608154,
      "learning_rate": 2.166947520462205e-05,
      "loss": 1.2502,
      "step": 2327
    },
    {
      "epoch": 0.8384656942193409,
      "grad_norm": 0.36983710527420044,
      "learning_rate": 2.1665864227250842e-05,
      "loss": 1.236,
      "step": 2328
    },
    {
      "epoch": 0.8388258598955519,
      "grad_norm": 0.37300077080726624,
      "learning_rate": 2.1662253249879635e-05,
      "loss": 1.2977,
      "step": 2329
    },
    {
      "epoch": 0.839186025571763,
      "grad_norm": 0.3695535957813263,
      "learning_rate": 2.1658642272508427e-05,
      "loss": 1.3101,
      "step": 2330
    },
    {
      "epoch": 0.839546191247974,
      "grad_norm": 0.3522234857082367,
      "learning_rate": 2.165503129513722e-05,
      "loss": 1.1517,
      "step": 2331
    },
    {
      "epoch": 0.8399063569241851,
      "grad_norm": 0.3886401653289795,
      "learning_rate": 2.1651420317766008e-05,
      "loss": 1.4318,
      "step": 2332
    },
    {
      "epoch": 0.8402665226003961,
      "grad_norm": 0.37266337871551514,
      "learning_rate": 2.16478093403948e-05,
      "loss": 1.3818,
      "step": 2333
    },
    {
      "epoch": 0.8406266882766072,
      "grad_norm": 0.35725802183151245,
      "learning_rate": 2.1644198363023593e-05,
      "loss": 1.2593,
      "step": 2334
    },
    {
      "epoch": 0.8409868539528182,
      "grad_norm": 0.35522428154945374,
      "learning_rate": 2.164058738565238e-05,
      "loss": 1.2665,
      "step": 2335
    },
    {
      "epoch": 0.8413470196290294,
      "grad_norm": 0.3800489902496338,
      "learning_rate": 2.1636976408281174e-05,
      "loss": 1.3662,
      "step": 2336
    },
    {
      "epoch": 0.8417071853052404,
      "grad_norm": 0.39780253171920776,
      "learning_rate": 2.163336543090997e-05,
      "loss": 1.316,
      "step": 2337
    },
    {
      "epoch": 0.8420673509814515,
      "grad_norm": 0.3754405677318573,
      "learning_rate": 2.162975445353876e-05,
      "loss": 1.3835,
      "step": 2338
    },
    {
      "epoch": 0.8424275166576626,
      "grad_norm": 0.38055744767189026,
      "learning_rate": 2.162614347616755e-05,
      "loss": 1.2999,
      "step": 2339
    },
    {
      "epoch": 0.8427876823338736,
      "grad_norm": 0.3671315610408783,
      "learning_rate": 2.162253249879634e-05,
      "loss": 1.2873,
      "step": 2340
    },
    {
      "epoch": 0.8431478480100847,
      "grad_norm": 0.35160696506500244,
      "learning_rate": 2.1618921521425132e-05,
      "loss": 1.2543,
      "step": 2341
    },
    {
      "epoch": 0.8435080136862957,
      "grad_norm": 0.3712841272354126,
      "learning_rate": 2.1615310544053924e-05,
      "loss": 1.3385,
      "step": 2342
    },
    {
      "epoch": 0.8438681793625068,
      "grad_norm": 0.3905957341194153,
      "learning_rate": 2.1611699566682713e-05,
      "loss": 1.2733,
      "step": 2343
    },
    {
      "epoch": 0.8442283450387178,
      "grad_norm": 0.38211214542388916,
      "learning_rate": 2.160808858931151e-05,
      "loss": 1.3057,
      "step": 2344
    },
    {
      "epoch": 0.8445885107149289,
      "grad_norm": 0.39933666586875916,
      "learning_rate": 2.16044776119403e-05,
      "loss": 1.4043,
      "step": 2345
    },
    {
      "epoch": 0.8449486763911399,
      "grad_norm": 0.3900972306728363,
      "learning_rate": 2.160086663456909e-05,
      "loss": 1.4458,
      "step": 2346
    },
    {
      "epoch": 0.845308842067351,
      "grad_norm": 0.368060439825058,
      "learning_rate": 2.1597255657197882e-05,
      "loss": 1.2505,
      "step": 2347
    },
    {
      "epoch": 0.845669007743562,
      "grad_norm": 0.3892357647418976,
      "learning_rate": 2.159364467982667e-05,
      "loss": 1.2039,
      "step": 2348
    },
    {
      "epoch": 0.8460291734197731,
      "grad_norm": 0.373684823513031,
      "learning_rate": 2.1590033702455464e-05,
      "loss": 1.2302,
      "step": 2349
    },
    {
      "epoch": 0.8463893390959841,
      "grad_norm": 0.3783613443374634,
      "learning_rate": 2.158642272508426e-05,
      "loss": 1.3196,
      "step": 2350
    },
    {
      "epoch": 0.8467495047721952,
      "grad_norm": 0.40067198872566223,
      "learning_rate": 2.1582811747713048e-05,
      "loss": 1.3273,
      "step": 2351
    },
    {
      "epoch": 0.8471096704484062,
      "grad_norm": 0.3862384855747223,
      "learning_rate": 2.157920077034184e-05,
      "loss": 1.3108,
      "step": 2352
    },
    {
      "epoch": 0.8474698361246173,
      "grad_norm": 0.37804660201072693,
      "learning_rate": 2.1575589792970633e-05,
      "loss": 1.3454,
      "step": 2353
    },
    {
      "epoch": 0.8478300018008283,
      "grad_norm": 0.37622490525245667,
      "learning_rate": 2.1571978815599422e-05,
      "loss": 1.3457,
      "step": 2354
    },
    {
      "epoch": 0.8481901674770395,
      "grad_norm": 0.37663769721984863,
      "learning_rate": 2.1568367838228214e-05,
      "loss": 1.3395,
      "step": 2355
    },
    {
      "epoch": 0.8485503331532505,
      "grad_norm": 0.3839831054210663,
      "learning_rate": 2.1564756860857003e-05,
      "loss": 1.3388,
      "step": 2356
    },
    {
      "epoch": 0.8489104988294616,
      "grad_norm": 0.37726029753685,
      "learning_rate": 2.15611458834858e-05,
      "loss": 1.4284,
      "step": 2357
    },
    {
      "epoch": 0.8492706645056726,
      "grad_norm": 0.3750568628311157,
      "learning_rate": 2.155753490611459e-05,
      "loss": 1.2406,
      "step": 2358
    },
    {
      "epoch": 0.8496308301818837,
      "grad_norm": 0.36798956990242004,
      "learning_rate": 2.155392392874338e-05,
      "loss": 1.2409,
      "step": 2359
    },
    {
      "epoch": 0.8499909958580947,
      "grad_norm": 0.3795701563358307,
      "learning_rate": 2.1550312951372172e-05,
      "loss": 1.3071,
      "step": 2360
    },
    {
      "epoch": 0.8503511615343058,
      "grad_norm": 0.35676106810569763,
      "learning_rate": 2.1546701974000964e-05,
      "loss": 1.26,
      "step": 2361
    },
    {
      "epoch": 0.8507113272105168,
      "grad_norm": 0.379175066947937,
      "learning_rate": 2.1543090996629753e-05,
      "loss": 1.2664,
      "step": 2362
    },
    {
      "epoch": 0.8510714928867279,
      "grad_norm": 0.3825923800468445,
      "learning_rate": 2.1539480019258546e-05,
      "loss": 1.2048,
      "step": 2363
    },
    {
      "epoch": 0.8514316585629389,
      "grad_norm": 0.3684300184249878,
      "learning_rate": 2.1535869041887338e-05,
      "loss": 1.3888,
      "step": 2364
    },
    {
      "epoch": 0.85179182423915,
      "grad_norm": 0.37950578331947327,
      "learning_rate": 2.153225806451613e-05,
      "loss": 1.3593,
      "step": 2365
    },
    {
      "epoch": 0.8521519899153611,
      "grad_norm": 0.36020922660827637,
      "learning_rate": 2.1528647087144923e-05,
      "loss": 1.2267,
      "step": 2366
    },
    {
      "epoch": 0.8525121555915721,
      "grad_norm": 0.3882014751434326,
      "learning_rate": 2.152503610977371e-05,
      "loss": 1.3238,
      "step": 2367
    },
    {
      "epoch": 0.8528723212677832,
      "grad_norm": 0.3869967758655548,
      "learning_rate": 2.1521425132402504e-05,
      "loss": 1.3175,
      "step": 2368
    },
    {
      "epoch": 0.8532324869439942,
      "grad_norm": 0.3715696632862091,
      "learning_rate": 2.1517814155031296e-05,
      "loss": 1.3147,
      "step": 2369
    },
    {
      "epoch": 0.8535926526202053,
      "grad_norm": 0.3836880624294281,
      "learning_rate": 2.1514203177660085e-05,
      "loss": 1.3374,
      "step": 2370
    },
    {
      "epoch": 0.8539528182964163,
      "grad_norm": 0.3908604383468628,
      "learning_rate": 2.151059220028888e-05,
      "loss": 1.3157,
      "step": 2371
    },
    {
      "epoch": 0.8543129839726274,
      "grad_norm": 0.3724358379840851,
      "learning_rate": 2.150698122291767e-05,
      "loss": 1.3553,
      "step": 2372
    },
    {
      "epoch": 0.8546731496488384,
      "grad_norm": 0.36178717017173767,
      "learning_rate": 2.1503370245546462e-05,
      "loss": 1.2483,
      "step": 2373
    },
    {
      "epoch": 0.8550333153250496,
      "grad_norm": 0.38576242327690125,
      "learning_rate": 2.1499759268175254e-05,
      "loss": 1.2025,
      "step": 2374
    },
    {
      "epoch": 0.8553934810012606,
      "grad_norm": 0.3746567964553833,
      "learning_rate": 2.1496148290804043e-05,
      "loss": 1.1747,
      "step": 2375
    },
    {
      "epoch": 0.8557536466774717,
      "grad_norm": 0.37267613410949707,
      "learning_rate": 2.1492537313432835e-05,
      "loss": 1.3433,
      "step": 2376
    },
    {
      "epoch": 0.8561138123536827,
      "grad_norm": 0.36659160256385803,
      "learning_rate": 2.148892633606163e-05,
      "loss": 1.1919,
      "step": 2377
    },
    {
      "epoch": 0.8564739780298938,
      "grad_norm": 0.36953654885292053,
      "learning_rate": 2.148531535869042e-05,
      "loss": 1.1759,
      "step": 2378
    },
    {
      "epoch": 0.8568341437061048,
      "grad_norm": 0.367747962474823,
      "learning_rate": 2.1481704381319212e-05,
      "loss": 1.1858,
      "step": 2379
    },
    {
      "epoch": 0.8571943093823159,
      "grad_norm": 0.3596985936164856,
      "learning_rate": 2.1478093403948e-05,
      "loss": 1.2041,
      "step": 2380
    },
    {
      "epoch": 0.8575544750585269,
      "grad_norm": 0.3750886023044586,
      "learning_rate": 2.1474482426576793e-05,
      "loss": 1.3266,
      "step": 2381
    },
    {
      "epoch": 0.857914640734738,
      "grad_norm": 0.3748522996902466,
      "learning_rate": 2.1470871449205586e-05,
      "loss": 1.2388,
      "step": 2382
    },
    {
      "epoch": 0.858274806410949,
      "grad_norm": 0.36444738507270813,
      "learning_rate": 2.1467260471834375e-05,
      "loss": 1.1895,
      "step": 2383
    },
    {
      "epoch": 0.8586349720871601,
      "grad_norm": 0.3746960461139679,
      "learning_rate": 2.146364949446317e-05,
      "loss": 1.2123,
      "step": 2384
    },
    {
      "epoch": 0.8589951377633711,
      "grad_norm": 0.3646398186683655,
      "learning_rate": 2.1460038517091963e-05,
      "loss": 1.3227,
      "step": 2385
    },
    {
      "epoch": 0.8593553034395822,
      "grad_norm": 0.3754465579986572,
      "learning_rate": 2.145642753972075e-05,
      "loss": 1.3069,
      "step": 2386
    },
    {
      "epoch": 0.8597154691157932,
      "grad_norm": 0.3837900757789612,
      "learning_rate": 2.1452816562349544e-05,
      "loss": 1.3244,
      "step": 2387
    },
    {
      "epoch": 0.8600756347920043,
      "grad_norm": 0.36981329321861267,
      "learning_rate": 2.1449205584978333e-05,
      "loss": 1.3002,
      "step": 2388
    },
    {
      "epoch": 0.8604358004682153,
      "grad_norm": 0.3665522336959839,
      "learning_rate": 2.1445594607607125e-05,
      "loss": 1.1787,
      "step": 2389
    },
    {
      "epoch": 0.8607959661444264,
      "grad_norm": 0.3870159387588501,
      "learning_rate": 2.1441983630235917e-05,
      "loss": 1.4026,
      "step": 2390
    },
    {
      "epoch": 0.8611561318206374,
      "grad_norm": 0.3855651915073395,
      "learning_rate": 2.143837265286471e-05,
      "loss": 1.4081,
      "step": 2391
    },
    {
      "epoch": 0.8615162974968485,
      "grad_norm": 0.37372273206710815,
      "learning_rate": 2.1434761675493502e-05,
      "loss": 1.2167,
      "step": 2392
    },
    {
      "epoch": 0.8618764631730597,
      "grad_norm": 0.3811395466327667,
      "learning_rate": 2.1431150698122294e-05,
      "loss": 1.3612,
      "step": 2393
    },
    {
      "epoch": 0.8622366288492707,
      "grad_norm": 0.3679240942001343,
      "learning_rate": 2.1427539720751083e-05,
      "loss": 1.2885,
      "step": 2394
    },
    {
      "epoch": 0.8625967945254818,
      "grad_norm": 0.3648967742919922,
      "learning_rate": 2.1423928743379875e-05,
      "loss": 1.2619,
      "step": 2395
    },
    {
      "epoch": 0.8629569602016928,
      "grad_norm": 0.3659045994281769,
      "learning_rate": 2.1420317766008664e-05,
      "loss": 1.2933,
      "step": 2396
    },
    {
      "epoch": 0.8633171258779039,
      "grad_norm": 0.383353054523468,
      "learning_rate": 2.1416706788637457e-05,
      "loss": 1.3933,
      "step": 2397
    },
    {
      "epoch": 0.8636772915541149,
      "grad_norm": 0.3711913824081421,
      "learning_rate": 2.1413095811266252e-05,
      "loss": 1.2231,
      "step": 2398
    },
    {
      "epoch": 0.864037457230326,
      "grad_norm": 0.38266289234161377,
      "learning_rate": 2.140948483389504e-05,
      "loss": 1.2685,
      "step": 2399
    },
    {
      "epoch": 0.864397622906537,
      "grad_norm": 0.34695640206336975,
      "learning_rate": 2.1405873856523833e-05,
      "loss": 1.2347,
      "step": 2400
    },
    {
      "epoch": 0.8647577885827481,
      "grad_norm": 0.36367955803871155,
      "learning_rate": 2.1402262879152626e-05,
      "loss": 1.2829,
      "step": 2401
    },
    {
      "epoch": 0.8651179542589591,
      "grad_norm": 0.37925979495048523,
      "learning_rate": 2.1398651901781415e-05,
      "loss": 1.1785,
      "step": 2402
    },
    {
      "epoch": 0.8654781199351702,
      "grad_norm": 0.36659494042396545,
      "learning_rate": 2.1395040924410207e-05,
      "loss": 1.2115,
      "step": 2403
    },
    {
      "epoch": 0.8658382856113812,
      "grad_norm": 0.38331153988838196,
      "learning_rate": 2.1391429947039e-05,
      "loss": 1.3242,
      "step": 2404
    },
    {
      "epoch": 0.8661984512875923,
      "grad_norm": 0.3677811026573181,
      "learning_rate": 2.138781896966779e-05,
      "loss": 1.1807,
      "step": 2405
    },
    {
      "epoch": 0.8665586169638033,
      "grad_norm": 0.41826000809669495,
      "learning_rate": 2.1384207992296584e-05,
      "loss": 1.394,
      "step": 2406
    },
    {
      "epoch": 0.8669187826400144,
      "grad_norm": 0.36188259720802307,
      "learning_rate": 2.1380597014925373e-05,
      "loss": 1.3235,
      "step": 2407
    },
    {
      "epoch": 0.8672789483162254,
      "grad_norm": 0.39739274978637695,
      "learning_rate": 2.1376986037554165e-05,
      "loss": 1.3385,
      "step": 2408
    },
    {
      "epoch": 0.8676391139924365,
      "grad_norm": 0.3989400565624237,
      "learning_rate": 2.1373375060182957e-05,
      "loss": 1.2989,
      "step": 2409
    },
    {
      "epoch": 0.8679992796686475,
      "grad_norm": 0.4071342945098877,
      "learning_rate": 2.1369764082811746e-05,
      "loss": 1.1898,
      "step": 2410
    },
    {
      "epoch": 0.8683594453448586,
      "grad_norm": 0.3748694360256195,
      "learning_rate": 2.1366153105440542e-05,
      "loss": 1.2576,
      "step": 2411
    },
    {
      "epoch": 0.8687196110210696,
      "grad_norm": 0.3765827417373657,
      "learning_rate": 2.136254212806933e-05,
      "loss": 1.2772,
      "step": 2412
    },
    {
      "epoch": 0.8690797766972808,
      "grad_norm": 0.37497392296791077,
      "learning_rate": 2.1358931150698123e-05,
      "loss": 1.1977,
      "step": 2413
    },
    {
      "epoch": 0.8694399423734918,
      "grad_norm": 0.3793264627456665,
      "learning_rate": 2.1355320173326915e-05,
      "loss": 1.2545,
      "step": 2414
    },
    {
      "epoch": 0.8698001080497029,
      "grad_norm": 0.37322893738746643,
      "learning_rate": 2.1351709195955704e-05,
      "loss": 1.2554,
      "step": 2415
    },
    {
      "epoch": 0.8701602737259139,
      "grad_norm": 0.3546983003616333,
      "learning_rate": 2.1348098218584497e-05,
      "loss": 1.1909,
      "step": 2416
    },
    {
      "epoch": 0.870520439402125,
      "grad_norm": 0.3794264495372772,
      "learning_rate": 2.134448724121329e-05,
      "loss": 1.3186,
      "step": 2417
    },
    {
      "epoch": 0.8708806050783361,
      "grad_norm": 0.3718070983886719,
      "learning_rate": 2.134087626384208e-05,
      "loss": 1.4085,
      "step": 2418
    },
    {
      "epoch": 0.8712407707545471,
      "grad_norm": 0.37221288681030273,
      "learning_rate": 2.1337265286470874e-05,
      "loss": 1.288,
      "step": 2419
    },
    {
      "epoch": 0.8716009364307582,
      "grad_norm": 0.3728746473789215,
      "learning_rate": 2.1333654309099662e-05,
      "loss": 1.2211,
      "step": 2420
    },
    {
      "epoch": 0.8719611021069692,
      "grad_norm": 0.38578394055366516,
      "learning_rate": 2.1330043331728455e-05,
      "loss": 1.2734,
      "step": 2421
    },
    {
      "epoch": 0.8723212677831803,
      "grad_norm": 0.38130417466163635,
      "learning_rate": 2.1326432354357247e-05,
      "loss": 1.2551,
      "step": 2422
    },
    {
      "epoch": 0.8726814334593913,
      "grad_norm": 0.3874102234840393,
      "learning_rate": 2.1322821376986036e-05,
      "loss": 1.4002,
      "step": 2423
    },
    {
      "epoch": 0.8730415991356024,
      "grad_norm": 0.3653203845024109,
      "learning_rate": 2.1319210399614828e-05,
      "loss": 1.2768,
      "step": 2424
    },
    {
      "epoch": 0.8734017648118134,
      "grad_norm": 0.391083300113678,
      "learning_rate": 2.1315599422243624e-05,
      "loss": 1.2974,
      "step": 2425
    },
    {
      "epoch": 0.8737619304880245,
      "grad_norm": 0.3774362802505493,
      "learning_rate": 2.1311988444872413e-05,
      "loss": 1.2979,
      "step": 2426
    },
    {
      "epoch": 0.8741220961642355,
      "grad_norm": 0.3599017560482025,
      "learning_rate": 2.1308377467501205e-05,
      "loss": 1.3338,
      "step": 2427
    },
    {
      "epoch": 0.8744822618404466,
      "grad_norm": 0.3794565498828888,
      "learning_rate": 2.1304766490129994e-05,
      "loss": 1.4049,
      "step": 2428
    },
    {
      "epoch": 0.8748424275166576,
      "grad_norm": 0.3450320065021515,
      "learning_rate": 2.1301155512758786e-05,
      "loss": 1.1593,
      "step": 2429
    },
    {
      "epoch": 0.8752025931928687,
      "grad_norm": 0.38386431336402893,
      "learning_rate": 2.129754453538758e-05,
      "loss": 1.3028,
      "step": 2430
    },
    {
      "epoch": 0.8755627588690797,
      "grad_norm": 0.38049131631851196,
      "learning_rate": 2.129393355801637e-05,
      "loss": 1.2903,
      "step": 2431
    },
    {
      "epoch": 0.8759229245452909,
      "grad_norm": 0.3896607458591461,
      "learning_rate": 2.1290322580645163e-05,
      "loss": 1.2149,
      "step": 2432
    },
    {
      "epoch": 0.8762830902215019,
      "grad_norm": 0.3849461078643799,
      "learning_rate": 2.1286711603273956e-05,
      "loss": 1.2995,
      "step": 2433
    },
    {
      "epoch": 0.876643255897713,
      "grad_norm": 0.4090080261230469,
      "learning_rate": 2.1283100625902744e-05,
      "loss": 1.4396,
      "step": 2434
    },
    {
      "epoch": 0.877003421573924,
      "grad_norm": 0.3800683319568634,
      "learning_rate": 2.1279489648531537e-05,
      "loss": 1.3636,
      "step": 2435
    },
    {
      "epoch": 0.8773635872501351,
      "grad_norm": 0.38770654797554016,
      "learning_rate": 2.1275878671160326e-05,
      "loss": 1.3171,
      "step": 2436
    },
    {
      "epoch": 0.8777237529263461,
      "grad_norm": 0.3685084283351898,
      "learning_rate": 2.1272267693789118e-05,
      "loss": 1.2614,
      "step": 2437
    },
    {
      "epoch": 0.8780839186025572,
      "grad_norm": 0.3681773543357849,
      "learning_rate": 2.1268656716417914e-05,
      "loss": 1.2955,
      "step": 2438
    },
    {
      "epoch": 0.8784440842787682,
      "grad_norm": 0.36253640055656433,
      "learning_rate": 2.1265045739046703e-05,
      "loss": 1.214,
      "step": 2439
    },
    {
      "epoch": 0.8788042499549793,
      "grad_norm": 0.37424805760383606,
      "learning_rate": 2.1261434761675495e-05,
      "loss": 1.2879,
      "step": 2440
    },
    {
      "epoch": 0.8791644156311903,
      "grad_norm": 0.36809241771698,
      "learning_rate": 2.1257823784304287e-05,
      "loss": 1.2367,
      "step": 2441
    },
    {
      "epoch": 0.8795245813074014,
      "grad_norm": 0.3874896168708801,
      "learning_rate": 2.1254212806933076e-05,
      "loss": 1.3743,
      "step": 2442
    },
    {
      "epoch": 0.8798847469836124,
      "grad_norm": 0.3614785969257355,
      "learning_rate": 2.125060182956187e-05,
      "loss": 1.2584,
      "step": 2443
    },
    {
      "epoch": 0.8802449126598235,
      "grad_norm": 0.3561270236968994,
      "learning_rate": 2.1246990852190657e-05,
      "loss": 1.2316,
      "step": 2444
    },
    {
      "epoch": 0.8806050783360346,
      "grad_norm": 0.36368128657341003,
      "learning_rate": 2.1243379874819453e-05,
      "loss": 1.2996,
      "step": 2445
    },
    {
      "epoch": 0.8809652440122456,
      "grad_norm": 0.3873281478881836,
      "learning_rate": 2.1239768897448245e-05,
      "loss": 1.2744,
      "step": 2446
    },
    {
      "epoch": 0.8813254096884567,
      "grad_norm": 0.365546315908432,
      "learning_rate": 2.1236157920077034e-05,
      "loss": 1.2926,
      "step": 2447
    },
    {
      "epoch": 0.8816855753646677,
      "grad_norm": 0.3926421105861664,
      "learning_rate": 2.1232546942705826e-05,
      "loss": 1.1969,
      "step": 2448
    },
    {
      "epoch": 0.8820457410408789,
      "grad_norm": 0.3665010929107666,
      "learning_rate": 2.122893596533462e-05,
      "loss": 1.2273,
      "step": 2449
    },
    {
      "epoch": 0.8824059067170898,
      "grad_norm": 0.3896024525165558,
      "learning_rate": 2.1225324987963408e-05,
      "loss": 1.2829,
      "step": 2450
    },
    {
      "epoch": 0.882766072393301,
      "grad_norm": 0.38419756293296814,
      "learning_rate": 2.12217140105922e-05,
      "loss": 1.323,
      "step": 2451
    },
    {
      "epoch": 0.883126238069512,
      "grad_norm": 0.385374516248703,
      "learning_rate": 2.1218103033220992e-05,
      "loss": 1.3214,
      "step": 2452
    },
    {
      "epoch": 0.8834864037457231,
      "grad_norm": 0.3789670765399933,
      "learning_rate": 2.1214492055849785e-05,
      "loss": 1.1812,
      "step": 2453
    },
    {
      "epoch": 0.8838465694219341,
      "grad_norm": 0.3807432949542999,
      "learning_rate": 2.1210881078478577e-05,
      "loss": 1.3205,
      "step": 2454
    },
    {
      "epoch": 0.8842067350981452,
      "grad_norm": 0.38595160841941833,
      "learning_rate": 2.1207270101107366e-05,
      "loss": 1.2789,
      "step": 2455
    },
    {
      "epoch": 0.8845669007743562,
      "grad_norm": 0.3904310464859009,
      "learning_rate": 2.1203659123736158e-05,
      "loss": 1.3579,
      "step": 2456
    },
    {
      "epoch": 0.8849270664505673,
      "grad_norm": 0.3750244379043579,
      "learning_rate": 2.120004814636495e-05,
      "loss": 1.2479,
      "step": 2457
    },
    {
      "epoch": 0.8852872321267783,
      "grad_norm": 0.38312315940856934,
      "learning_rate": 2.1196437168993743e-05,
      "loss": 1.2276,
      "step": 2458
    },
    {
      "epoch": 0.8856473978029894,
      "grad_norm": 0.3809633255004883,
      "learning_rate": 2.1192826191622535e-05,
      "loss": 1.2821,
      "step": 2459
    },
    {
      "epoch": 0.8860075634792004,
      "grad_norm": 0.3841896951198578,
      "learning_rate": 2.1189215214251324e-05,
      "loss": 1.3772,
      "step": 2460
    },
    {
      "epoch": 0.8863677291554115,
      "grad_norm": 0.39951053261756897,
      "learning_rate": 2.1185604236880116e-05,
      "loss": 1.3064,
      "step": 2461
    },
    {
      "epoch": 0.8867278948316225,
      "grad_norm": 0.39086368680000305,
      "learning_rate": 2.118199325950891e-05,
      "loss": 1.3038,
      "step": 2462
    },
    {
      "epoch": 0.8870880605078336,
      "grad_norm": 0.35593774914741516,
      "learning_rate": 2.1178382282137697e-05,
      "loss": 1.1555,
      "step": 2463
    },
    {
      "epoch": 0.8874482261840446,
      "grad_norm": 0.3791128098964691,
      "learning_rate": 2.117477130476649e-05,
      "loss": 1.1967,
      "step": 2464
    },
    {
      "epoch": 0.8878083918602557,
      "grad_norm": 0.3831796646118164,
      "learning_rate": 2.1171160327395282e-05,
      "loss": 1.3047,
      "step": 2465
    },
    {
      "epoch": 0.8881685575364667,
      "grad_norm": 0.3888207674026489,
      "learning_rate": 2.1167549350024074e-05,
      "loss": 1.2424,
      "step": 2466
    },
    {
      "epoch": 0.8885287232126778,
      "grad_norm": 0.3840530812740326,
      "learning_rate": 2.1163938372652867e-05,
      "loss": 1.2686,
      "step": 2467
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.37449342012405396,
      "learning_rate": 2.1160327395281655e-05,
      "loss": 1.2676,
      "step": 2468
    },
    {
      "epoch": 0.8892490545651,
      "grad_norm": 0.37064045667648315,
      "learning_rate": 2.1156716417910448e-05,
      "loss": 1.2569,
      "step": 2469
    },
    {
      "epoch": 0.889609220241311,
      "grad_norm": 0.37729862332344055,
      "learning_rate": 2.115310544053924e-05,
      "loss": 1.3562,
      "step": 2470
    },
    {
      "epoch": 0.8899693859175221,
      "grad_norm": 0.3748426139354706,
      "learning_rate": 2.114949446316803e-05,
      "loss": 1.2343,
      "step": 2471
    },
    {
      "epoch": 0.8903295515937332,
      "grad_norm": 0.3824480473995209,
      "learning_rate": 2.1145883485796825e-05,
      "loss": 1.2935,
      "step": 2472
    },
    {
      "epoch": 0.8906897172699442,
      "grad_norm": 0.3716363310813904,
      "learning_rate": 2.1142272508425614e-05,
      "loss": 1.1883,
      "step": 2473
    },
    {
      "epoch": 0.8910498829461553,
      "grad_norm": 0.3774353265762329,
      "learning_rate": 2.1138661531054406e-05,
      "loss": 1.3757,
      "step": 2474
    },
    {
      "epoch": 0.8914100486223663,
      "grad_norm": 0.38757219910621643,
      "learning_rate": 2.1135050553683198e-05,
      "loss": 1.2818,
      "step": 2475
    },
    {
      "epoch": 0.8917702142985774,
      "grad_norm": 0.3811042606830597,
      "learning_rate": 2.1131439576311987e-05,
      "loss": 1.3592,
      "step": 2476
    },
    {
      "epoch": 0.8921303799747884,
      "grad_norm": 0.3840468227863312,
      "learning_rate": 2.112782859894078e-05,
      "loss": 1.2941,
      "step": 2477
    },
    {
      "epoch": 0.8924905456509995,
      "grad_norm": 0.3898272216320038,
      "learning_rate": 2.112421762156957e-05,
      "loss": 1.2692,
      "step": 2478
    },
    {
      "epoch": 0.8928507113272105,
      "grad_norm": 0.3847314715385437,
      "learning_rate": 2.1120606644198364e-05,
      "loss": 1.2131,
      "step": 2479
    },
    {
      "epoch": 0.8932108770034216,
      "grad_norm": 0.37480390071868896,
      "learning_rate": 2.1116995666827156e-05,
      "loss": 1.3835,
      "step": 2480
    },
    {
      "epoch": 0.8935710426796326,
      "grad_norm": 0.38177332282066345,
      "learning_rate": 2.1113384689455945e-05,
      "loss": 1.1994,
      "step": 2481
    },
    {
      "epoch": 0.8939312083558437,
      "grad_norm": 0.395071804523468,
      "learning_rate": 2.1109773712084737e-05,
      "loss": 1.3128,
      "step": 2482
    },
    {
      "epoch": 0.8942913740320547,
      "grad_norm": 0.39472711086273193,
      "learning_rate": 2.110616273471353e-05,
      "loss": 1.2589,
      "step": 2483
    },
    {
      "epoch": 0.8946515397082658,
      "grad_norm": 0.35431575775146484,
      "learning_rate": 2.110255175734232e-05,
      "loss": 1.3461,
      "step": 2484
    },
    {
      "epoch": 0.8950117053844768,
      "grad_norm": 0.3783680200576782,
      "learning_rate": 2.1098940779971114e-05,
      "loss": 1.2134,
      "step": 2485
    },
    {
      "epoch": 0.8953718710606879,
      "grad_norm": 0.3808882534503937,
      "learning_rate": 2.1095329802599907e-05,
      "loss": 1.2998,
      "step": 2486
    },
    {
      "epoch": 0.8957320367368989,
      "grad_norm": 0.3809218108654022,
      "learning_rate": 2.1091718825228696e-05,
      "loss": 1.2531,
      "step": 2487
    },
    {
      "epoch": 0.89609220241311,
      "grad_norm": 0.38256415724754333,
      "learning_rate": 2.1088107847857488e-05,
      "loss": 1.4158,
      "step": 2488
    },
    {
      "epoch": 0.896452368089321,
      "grad_norm": 0.3819747567176819,
      "learning_rate": 2.1084496870486277e-05,
      "loss": 1.294,
      "step": 2489
    },
    {
      "epoch": 0.8968125337655322,
      "grad_norm": 0.36904260516166687,
      "learning_rate": 2.108088589311507e-05,
      "loss": 1.1586,
      "step": 2490
    },
    {
      "epoch": 0.8971726994417432,
      "grad_norm": 0.36050543189048767,
      "learning_rate": 2.107727491574386e-05,
      "loss": 1.247,
      "step": 2491
    },
    {
      "epoch": 0.8975328651179543,
      "grad_norm": 0.3847252428531647,
      "learning_rate": 2.1073663938372654e-05,
      "loss": 1.354,
      "step": 2492
    },
    {
      "epoch": 0.8978930307941653,
      "grad_norm": 0.38899990916252136,
      "learning_rate": 2.1070052961001446e-05,
      "loss": 1.4326,
      "step": 2493
    },
    {
      "epoch": 0.8982531964703764,
      "grad_norm": 0.3648395836353302,
      "learning_rate": 2.1066441983630238e-05,
      "loss": 1.1065,
      "step": 2494
    },
    {
      "epoch": 0.8986133621465874,
      "grad_norm": 0.384432315826416,
      "learning_rate": 2.1062831006259027e-05,
      "loss": 1.2395,
      "step": 2495
    },
    {
      "epoch": 0.8989735278227985,
      "grad_norm": 0.380195289850235,
      "learning_rate": 2.105922002888782e-05,
      "loss": 1.3209,
      "step": 2496
    },
    {
      "epoch": 0.8993336934990095,
      "grad_norm": 0.38074228167533875,
      "learning_rate": 2.105560905151661e-05,
      "loss": 1.2543,
      "step": 2497
    },
    {
      "epoch": 0.8996938591752206,
      "grad_norm": 0.3845572769641876,
      "learning_rate": 2.10519980741454e-05,
      "loss": 1.2928,
      "step": 2498
    },
    {
      "epoch": 0.9000540248514317,
      "grad_norm": 0.3697815537452698,
      "learning_rate": 2.1048387096774196e-05,
      "loss": 1.2305,
      "step": 2499
    },
    {
      "epoch": 0.9004141905276427,
      "grad_norm": 0.3684404492378235,
      "learning_rate": 2.1044776119402985e-05,
      "loss": 1.2249,
      "step": 2500
    },
    {
      "epoch": 0.9007743562038538,
      "grad_norm": 0.3885374367237091,
      "learning_rate": 2.1041165142031777e-05,
      "loss": 1.3158,
      "step": 2501
    },
    {
      "epoch": 0.9011345218800648,
      "grad_norm": 0.3850365877151489,
      "learning_rate": 2.103755416466057e-05,
      "loss": 1.2861,
      "step": 2502
    },
    {
      "epoch": 0.9014946875562759,
      "grad_norm": 0.3746418356895447,
      "learning_rate": 2.103394318728936e-05,
      "loss": 1.243,
      "step": 2503
    },
    {
      "epoch": 0.9018548532324869,
      "grad_norm": 0.37565115094184875,
      "learning_rate": 2.103033220991815e-05,
      "loss": 1.2727,
      "step": 2504
    },
    {
      "epoch": 0.902215018908698,
      "grad_norm": 0.3944304287433624,
      "learning_rate": 2.1026721232546943e-05,
      "loss": 1.1976,
      "step": 2505
    },
    {
      "epoch": 0.902575184584909,
      "grad_norm": 0.3724825978279114,
      "learning_rate": 2.1023110255175736e-05,
      "loss": 1.2871,
      "step": 2506
    },
    {
      "epoch": 0.9029353502611202,
      "grad_norm": 0.4295048117637634,
      "learning_rate": 2.1019499277804528e-05,
      "loss": 1.4418,
      "step": 2507
    },
    {
      "epoch": 0.9032955159373312,
      "grad_norm": 0.4012524485588074,
      "learning_rate": 2.1015888300433317e-05,
      "loss": 1.2654,
      "step": 2508
    },
    {
      "epoch": 0.9036556816135423,
      "grad_norm": 0.38343825936317444,
      "learning_rate": 2.101227732306211e-05,
      "loss": 1.2711,
      "step": 2509
    },
    {
      "epoch": 0.9040158472897533,
      "grad_norm": 0.38889458775520325,
      "learning_rate": 2.10086663456909e-05,
      "loss": 1.3787,
      "step": 2510
    },
    {
      "epoch": 0.9043760129659644,
      "grad_norm": 0.3790474534034729,
      "learning_rate": 2.100505536831969e-05,
      "loss": 1.2588,
      "step": 2511
    },
    {
      "epoch": 0.9047361786421754,
      "grad_norm": 0.38840654492378235,
      "learning_rate": 2.1001444390948486e-05,
      "loss": 1.2346,
      "step": 2512
    },
    {
      "epoch": 0.9050963443183865,
      "grad_norm": 0.3886341154575348,
      "learning_rate": 2.0997833413577275e-05,
      "loss": 1.2294,
      "step": 2513
    },
    {
      "epoch": 0.9054565099945975,
      "grad_norm": 0.3802155554294586,
      "learning_rate": 2.0994222436206067e-05,
      "loss": 1.2833,
      "step": 2514
    },
    {
      "epoch": 0.9058166756708086,
      "grad_norm": 0.3720662593841553,
      "learning_rate": 2.099061145883486e-05,
      "loss": 1.1851,
      "step": 2515
    },
    {
      "epoch": 0.9061768413470196,
      "grad_norm": 0.4039655327796936,
      "learning_rate": 2.098700048146365e-05,
      "loss": 1.3191,
      "step": 2516
    },
    {
      "epoch": 0.9065370070232307,
      "grad_norm": 0.38189104199409485,
      "learning_rate": 2.098338950409244e-05,
      "loss": 1.267,
      "step": 2517
    },
    {
      "epoch": 0.9068971726994417,
      "grad_norm": 0.38057613372802734,
      "learning_rate": 2.0979778526721233e-05,
      "loss": 1.1276,
      "step": 2518
    },
    {
      "epoch": 0.9072573383756528,
      "grad_norm": 0.3825472891330719,
      "learning_rate": 2.0976167549350025e-05,
      "loss": 1.2236,
      "step": 2519
    },
    {
      "epoch": 0.9076175040518638,
      "grad_norm": 0.3781844675540924,
      "learning_rate": 2.0972556571978818e-05,
      "loss": 1.3165,
      "step": 2520
    },
    {
      "epoch": 0.9079776697280749,
      "grad_norm": 0.3777589201927185,
      "learning_rate": 2.0968945594607606e-05,
      "loss": 1.2667,
      "step": 2521
    },
    {
      "epoch": 0.9083378354042859,
      "grad_norm": 0.39551684260368347,
      "learning_rate": 2.09653346172364e-05,
      "loss": 1.3113,
      "step": 2522
    },
    {
      "epoch": 0.908698001080497,
      "grad_norm": 0.4130224287509918,
      "learning_rate": 2.096172363986519e-05,
      "loss": 1.2337,
      "step": 2523
    },
    {
      "epoch": 0.909058166756708,
      "grad_norm": 0.3831638693809509,
      "learning_rate": 2.095811266249398e-05,
      "loss": 1.2447,
      "step": 2524
    },
    {
      "epoch": 0.9094183324329191,
      "grad_norm": 0.39537152647972107,
      "learning_rate": 2.0954501685122772e-05,
      "loss": 1.3267,
      "step": 2525
    },
    {
      "epoch": 0.9097784981091303,
      "grad_norm": 0.37274742126464844,
      "learning_rate": 2.0950890707751568e-05,
      "loss": 1.2437,
      "step": 2526
    },
    {
      "epoch": 0.9101386637853413,
      "grad_norm": 0.37220627069473267,
      "learning_rate": 2.0947279730380357e-05,
      "loss": 1.1962,
      "step": 2527
    },
    {
      "epoch": 0.9104988294615524,
      "grad_norm": 0.40228575468063354,
      "learning_rate": 2.094366875300915e-05,
      "loss": 1.3803,
      "step": 2528
    },
    {
      "epoch": 0.9108589951377634,
      "grad_norm": 0.38263973593711853,
      "learning_rate": 2.0940057775637938e-05,
      "loss": 1.2118,
      "step": 2529
    },
    {
      "epoch": 0.9112191608139745,
      "grad_norm": 0.4099358320236206,
      "learning_rate": 2.093644679826673e-05,
      "loss": 1.4158,
      "step": 2530
    },
    {
      "epoch": 0.9115793264901855,
      "grad_norm": 0.37280622124671936,
      "learning_rate": 2.0932835820895523e-05,
      "loss": 1.2912,
      "step": 2531
    },
    {
      "epoch": 0.9119394921663966,
      "grad_norm": 0.37616100907325745,
      "learning_rate": 2.0929224843524315e-05,
      "loss": 1.3682,
      "step": 2532
    },
    {
      "epoch": 0.9122996578426076,
      "grad_norm": 0.37689098715782166,
      "learning_rate": 2.0925613866153107e-05,
      "loss": 1.2495,
      "step": 2533
    },
    {
      "epoch": 0.9126598235188187,
      "grad_norm": 0.40088343620300293,
      "learning_rate": 2.09220028887819e-05,
      "loss": 1.3386,
      "step": 2534
    },
    {
      "epoch": 0.9130199891950297,
      "grad_norm": 0.3579390048980713,
      "learning_rate": 2.091839191141069e-05,
      "loss": 1.2499,
      "step": 2535
    },
    {
      "epoch": 0.9133801548712408,
      "grad_norm": 0.37759068608283997,
      "learning_rate": 2.091478093403948e-05,
      "loss": 1.217,
      "step": 2536
    },
    {
      "epoch": 0.9137403205474518,
      "grad_norm": 0.36605191230773926,
      "learning_rate": 2.091116995666827e-05,
      "loss": 1.2462,
      "step": 2537
    },
    {
      "epoch": 0.9141004862236629,
      "grad_norm": 0.3699539601802826,
      "learning_rate": 2.0907558979297062e-05,
      "loss": 1.3056,
      "step": 2538
    },
    {
      "epoch": 0.9144606518998739,
      "grad_norm": 0.40006381273269653,
      "learning_rate": 2.0903948001925858e-05,
      "loss": 1.2951,
      "step": 2539
    },
    {
      "epoch": 0.914820817576085,
      "grad_norm": 0.38241341710090637,
      "learning_rate": 2.0900337024554647e-05,
      "loss": 1.2813,
      "step": 2540
    },
    {
      "epoch": 0.915180983252296,
      "grad_norm": 0.3884911835193634,
      "learning_rate": 2.089672604718344e-05,
      "loss": 1.3786,
      "step": 2541
    },
    {
      "epoch": 0.9155411489285071,
      "grad_norm": 0.4003871977329254,
      "learning_rate": 2.089311506981223e-05,
      "loss": 1.2696,
      "step": 2542
    },
    {
      "epoch": 0.9159013146047181,
      "grad_norm": 0.37038159370422363,
      "learning_rate": 2.088950409244102e-05,
      "loss": 1.1769,
      "step": 2543
    },
    {
      "epoch": 0.9162614802809292,
      "grad_norm": 0.39133256673812866,
      "learning_rate": 2.0885893115069812e-05,
      "loss": 1.256,
      "step": 2544
    },
    {
      "epoch": 0.9166216459571402,
      "grad_norm": 0.3961624801158905,
      "learning_rate": 2.08822821376986e-05,
      "loss": 1.2779,
      "step": 2545
    },
    {
      "epoch": 0.9169818116333514,
      "grad_norm": 0.3992998003959656,
      "learning_rate": 2.0878671160327397e-05,
      "loss": 1.3455,
      "step": 2546
    },
    {
      "epoch": 0.9173419773095624,
      "grad_norm": 0.39900657534599304,
      "learning_rate": 2.087506018295619e-05,
      "loss": 1.2932,
      "step": 2547
    },
    {
      "epoch": 0.9177021429857735,
      "grad_norm": 0.36464348435401917,
      "learning_rate": 2.0871449205584978e-05,
      "loss": 1.1934,
      "step": 2548
    },
    {
      "epoch": 0.9180623086619845,
      "grad_norm": 0.38010892271995544,
      "learning_rate": 2.086783822821377e-05,
      "loss": 1.2231,
      "step": 2549
    },
    {
      "epoch": 0.9184224743381956,
      "grad_norm": 0.35913440585136414,
      "learning_rate": 2.0864227250842563e-05,
      "loss": 1.147,
      "step": 2550
    },
    {
      "epoch": 0.9187826400144067,
      "grad_norm": 0.38365688920021057,
      "learning_rate": 2.086061627347135e-05,
      "loss": 1.4292,
      "step": 2551
    },
    {
      "epoch": 0.9191428056906177,
      "grad_norm": 0.37368500232696533,
      "learning_rate": 2.0857005296100144e-05,
      "loss": 1.2522,
      "step": 2552
    },
    {
      "epoch": 0.9195029713668288,
      "grad_norm": 0.37813663482666016,
      "learning_rate": 2.0853394318728936e-05,
      "loss": 1.2732,
      "step": 2553
    },
    {
      "epoch": 0.9198631370430398,
      "grad_norm": 0.3922577500343323,
      "learning_rate": 2.084978334135773e-05,
      "loss": 1.3081,
      "step": 2554
    },
    {
      "epoch": 0.9202233027192509,
      "grad_norm": 0.36912786960601807,
      "learning_rate": 2.084617236398652e-05,
      "loss": 1.2482,
      "step": 2555
    },
    {
      "epoch": 0.9205834683954619,
      "grad_norm": 0.38663211464881897,
      "learning_rate": 2.084256138661531e-05,
      "loss": 1.3606,
      "step": 2556
    },
    {
      "epoch": 0.920943634071673,
      "grad_norm": 0.41152554750442505,
      "learning_rate": 2.0838950409244102e-05,
      "loss": 1.3627,
      "step": 2557
    },
    {
      "epoch": 0.921303799747884,
      "grad_norm": 0.394876092672348,
      "learning_rate": 2.0835339431872894e-05,
      "loss": 1.4176,
      "step": 2558
    },
    {
      "epoch": 0.9216639654240951,
      "grad_norm": 0.39574721455574036,
      "learning_rate": 2.0831728454501687e-05,
      "loss": 1.1702,
      "step": 2559
    },
    {
      "epoch": 0.9220241311003061,
      "grad_norm": 0.38554564118385315,
      "learning_rate": 2.082811747713048e-05,
      "loss": 1.2449,
      "step": 2560
    },
    {
      "epoch": 0.9223842967765172,
      "grad_norm": 0.3759433627128601,
      "learning_rate": 2.0824506499759268e-05,
      "loss": 1.3688,
      "step": 2561
    },
    {
      "epoch": 0.9227444624527282,
      "grad_norm": 0.3880748152732849,
      "learning_rate": 2.082089552238806e-05,
      "loss": 1.3502,
      "step": 2562
    },
    {
      "epoch": 0.9231046281289393,
      "grad_norm": 0.3737730383872986,
      "learning_rate": 2.0817284545016852e-05,
      "loss": 1.2773,
      "step": 2563
    },
    {
      "epoch": 0.9234647938051503,
      "grad_norm": 0.3758143186569214,
      "learning_rate": 2.081367356764564e-05,
      "loss": 1.2484,
      "step": 2564
    },
    {
      "epoch": 0.9238249594813615,
      "grad_norm": 0.38483041524887085,
      "learning_rate": 2.0810062590274434e-05,
      "loss": 1.2309,
      "step": 2565
    },
    {
      "epoch": 0.9241851251575725,
      "grad_norm": 0.3755874037742615,
      "learning_rate": 2.080645161290323e-05,
      "loss": 1.283,
      "step": 2566
    },
    {
      "epoch": 0.9245452908337836,
      "grad_norm": 0.38602524995803833,
      "learning_rate": 2.0802840635532018e-05,
      "loss": 1.2841,
      "step": 2567
    },
    {
      "epoch": 0.9249054565099946,
      "grad_norm": 0.3976586163043976,
      "learning_rate": 2.079922965816081e-05,
      "loss": 1.28,
      "step": 2568
    },
    {
      "epoch": 0.9252656221862057,
      "grad_norm": 0.3721759021282196,
      "learning_rate": 2.07956186807896e-05,
      "loss": 1.1721,
      "step": 2569
    },
    {
      "epoch": 0.9256257878624167,
      "grad_norm": 0.38076093792915344,
      "learning_rate": 2.0792007703418392e-05,
      "loss": 1.2823,
      "step": 2570
    },
    {
      "epoch": 0.9259859535386278,
      "grad_norm": 0.3827034533023834,
      "learning_rate": 2.0788396726047184e-05,
      "loss": 1.3541,
      "step": 2571
    },
    {
      "epoch": 0.9263461192148388,
      "grad_norm": 0.3593982756137848,
      "learning_rate": 2.0784785748675973e-05,
      "loss": 1.2093,
      "step": 2572
    },
    {
      "epoch": 0.9267062848910499,
      "grad_norm": 0.38448795676231384,
      "learning_rate": 2.078117477130477e-05,
      "loss": 1.3021,
      "step": 2573
    },
    {
      "epoch": 0.9270664505672609,
      "grad_norm": 0.380109965801239,
      "learning_rate": 2.077756379393356e-05,
      "loss": 1.2927,
      "step": 2574
    },
    {
      "epoch": 0.927426616243472,
      "grad_norm": 0.38715627789497375,
      "learning_rate": 2.077395281656235e-05,
      "loss": 1.2881,
      "step": 2575
    },
    {
      "epoch": 0.927786781919683,
      "grad_norm": 0.3708227574825287,
      "learning_rate": 2.0770341839191142e-05,
      "loss": 1.2742,
      "step": 2576
    },
    {
      "epoch": 0.9281469475958941,
      "grad_norm": 0.3874693512916565,
      "learning_rate": 2.076673086181993e-05,
      "loss": 1.2669,
      "step": 2577
    },
    {
      "epoch": 0.9285071132721052,
      "grad_norm": 0.3831802010536194,
      "learning_rate": 2.0763119884448723e-05,
      "loss": 1.2181,
      "step": 2578
    },
    {
      "epoch": 0.9288672789483162,
      "grad_norm": 0.38503819704055786,
      "learning_rate": 2.0759508907077516e-05,
      "loss": 1.2925,
      "step": 2579
    },
    {
      "epoch": 0.9292274446245273,
      "grad_norm": 0.3722899556159973,
      "learning_rate": 2.0755897929706308e-05,
      "loss": 1.2422,
      "step": 2580
    },
    {
      "epoch": 0.9295876103007383,
      "grad_norm": 0.39376258850097656,
      "learning_rate": 2.07522869523351e-05,
      "loss": 1.3706,
      "step": 2581
    },
    {
      "epoch": 0.9299477759769494,
      "grad_norm": 0.3912201225757599,
      "learning_rate": 2.0748675974963893e-05,
      "loss": 1.3089,
      "step": 2582
    },
    {
      "epoch": 0.9303079416531604,
      "grad_norm": 0.3792441785335541,
      "learning_rate": 2.074506499759268e-05,
      "loss": 1.3042,
      "step": 2583
    },
    {
      "epoch": 0.9306681073293716,
      "grad_norm": 0.3848753273487091,
      "learning_rate": 2.0741454020221474e-05,
      "loss": 1.3089,
      "step": 2584
    },
    {
      "epoch": 0.9310282730055826,
      "grad_norm": 0.3860947787761688,
      "learning_rate": 2.0737843042850263e-05,
      "loss": 1.3048,
      "step": 2585
    },
    {
      "epoch": 0.9313884386817937,
      "grad_norm": 0.37587881088256836,
      "learning_rate": 2.073423206547906e-05,
      "loss": 1.1624,
      "step": 2586
    },
    {
      "epoch": 0.9317486043580047,
      "grad_norm": 0.379452109336853,
      "learning_rate": 2.073062108810785e-05,
      "loss": 1.3134,
      "step": 2587
    },
    {
      "epoch": 0.9321087700342158,
      "grad_norm": 0.3836628198623657,
      "learning_rate": 2.072701011073664e-05,
      "loss": 1.2771,
      "step": 2588
    },
    {
      "epoch": 0.9324689357104268,
      "grad_norm": 0.38946399092674255,
      "learning_rate": 2.0723399133365432e-05,
      "loss": 1.3851,
      "step": 2589
    },
    {
      "epoch": 0.9328291013866379,
      "grad_norm": 0.3765968680381775,
      "learning_rate": 2.0719788155994224e-05,
      "loss": 1.3071,
      "step": 2590
    },
    {
      "epoch": 0.9331892670628489,
      "grad_norm": 0.3906387388706207,
      "learning_rate": 2.0716177178623013e-05,
      "loss": 1.2789,
      "step": 2591
    },
    {
      "epoch": 0.93354943273906,
      "grad_norm": 0.3902977406978607,
      "learning_rate": 2.0712566201251805e-05,
      "loss": 1.3239,
      "step": 2592
    },
    {
      "epoch": 0.933909598415271,
      "grad_norm": 0.37983548641204834,
      "learning_rate": 2.0708955223880598e-05,
      "loss": 1.2926,
      "step": 2593
    },
    {
      "epoch": 0.9342697640914821,
      "grad_norm": 0.3878517746925354,
      "learning_rate": 2.070534424650939e-05,
      "loss": 1.2736,
      "step": 2594
    },
    {
      "epoch": 0.9346299297676931,
      "grad_norm": 0.3863178491592407,
      "learning_rate": 2.0701733269138182e-05,
      "loss": 1.2342,
      "step": 2595
    },
    {
      "epoch": 0.9349900954439042,
      "grad_norm": 0.3801212012767792,
      "learning_rate": 2.069812229176697e-05,
      "loss": 1.3009,
      "step": 2596
    },
    {
      "epoch": 0.9353502611201152,
      "grad_norm": 0.38720813393592834,
      "learning_rate": 2.0694511314395763e-05,
      "loss": 1.1859,
      "step": 2597
    },
    {
      "epoch": 0.9357104267963263,
      "grad_norm": 0.3803914785385132,
      "learning_rate": 2.0690900337024556e-05,
      "loss": 1.2014,
      "step": 2598
    },
    {
      "epoch": 0.9360705924725373,
      "grad_norm": 0.387798011302948,
      "learning_rate": 2.0687289359653345e-05,
      "loss": 1.3465,
      "step": 2599
    },
    {
      "epoch": 0.9364307581487484,
      "grad_norm": 0.37652647495269775,
      "learning_rate": 2.068367838228214e-05,
      "loss": 1.2521,
      "step": 2600
    },
    {
      "epoch": 0.9367909238249594,
      "grad_norm": 0.3804391324520111,
      "learning_rate": 2.068006740491093e-05,
      "loss": 1.2645,
      "step": 2601
    },
    {
      "epoch": 0.9371510895011705,
      "grad_norm": 0.3978751599788666,
      "learning_rate": 2.067645642753972e-05,
      "loss": 1.325,
      "step": 2602
    },
    {
      "epoch": 0.9375112551773815,
      "grad_norm": 0.37498965859413147,
      "learning_rate": 2.0672845450168514e-05,
      "loss": 1.1585,
      "step": 2603
    },
    {
      "epoch": 0.9378714208535927,
      "grad_norm": 0.3761340379714966,
      "learning_rate": 2.0669234472797303e-05,
      "loss": 1.2601,
      "step": 2604
    },
    {
      "epoch": 0.9382315865298038,
      "grad_norm": 0.38700199127197266,
      "learning_rate": 2.0665623495426095e-05,
      "loss": 1.2891,
      "step": 2605
    },
    {
      "epoch": 0.9385917522060148,
      "grad_norm": 0.3937910795211792,
      "learning_rate": 2.0662012518054887e-05,
      "loss": 1.1728,
      "step": 2606
    },
    {
      "epoch": 0.9389519178822259,
      "grad_norm": 0.3786785304546356,
      "learning_rate": 2.065840154068368e-05,
      "loss": 1.2289,
      "step": 2607
    },
    {
      "epoch": 0.9393120835584369,
      "grad_norm": 0.3793857991695404,
      "learning_rate": 2.0654790563312472e-05,
      "loss": 1.4261,
      "step": 2608
    },
    {
      "epoch": 0.939672249234648,
      "grad_norm": 0.38400542736053467,
      "learning_rate": 2.065117958594126e-05,
      "loss": 1.2597,
      "step": 2609
    },
    {
      "epoch": 0.940032414910859,
      "grad_norm": 0.390196293592453,
      "learning_rate": 2.0647568608570053e-05,
      "loss": 1.2651,
      "step": 2610
    },
    {
      "epoch": 0.9403925805870701,
      "grad_norm": 0.37856191396713257,
      "learning_rate": 2.0643957631198845e-05,
      "loss": 1.3234,
      "step": 2611
    },
    {
      "epoch": 0.9407527462632811,
      "grad_norm": 0.3830544352531433,
      "learning_rate": 2.0640346653827634e-05,
      "loss": 1.2906,
      "step": 2612
    },
    {
      "epoch": 0.9411129119394922,
      "grad_norm": 0.37880173325538635,
      "learning_rate": 2.063673567645643e-05,
      "loss": 1.3332,
      "step": 2613
    },
    {
      "epoch": 0.9414730776157032,
      "grad_norm": 0.37773504853248596,
      "learning_rate": 2.0633124699085222e-05,
      "loss": 1.3013,
      "step": 2614
    },
    {
      "epoch": 0.9418332432919143,
      "grad_norm": 0.3760470449924469,
      "learning_rate": 2.062951372171401e-05,
      "loss": 1.2857,
      "step": 2615
    },
    {
      "epoch": 0.9421934089681253,
      "grad_norm": 0.3875674903392792,
      "learning_rate": 2.0625902744342804e-05,
      "loss": 1.2872,
      "step": 2616
    },
    {
      "epoch": 0.9425535746443364,
      "grad_norm": 0.3778577148914337,
      "learning_rate": 2.0622291766971592e-05,
      "loss": 1.2456,
      "step": 2617
    },
    {
      "epoch": 0.9429137403205474,
      "grad_norm": 0.392686128616333,
      "learning_rate": 2.0618680789600385e-05,
      "loss": 1.2531,
      "step": 2618
    },
    {
      "epoch": 0.9432739059967585,
      "grad_norm": 0.3762684166431427,
      "learning_rate": 2.0615069812229177e-05,
      "loss": 1.2141,
      "step": 2619
    },
    {
      "epoch": 0.9436340716729695,
      "grad_norm": 0.37032780051231384,
      "learning_rate": 2.061145883485797e-05,
      "loss": 1.2558,
      "step": 2620
    },
    {
      "epoch": 0.9439942373491806,
      "grad_norm": 0.37870684266090393,
      "learning_rate": 2.060784785748676e-05,
      "loss": 1.1337,
      "step": 2621
    },
    {
      "epoch": 0.9443544030253916,
      "grad_norm": 0.36785370111465454,
      "learning_rate": 2.0604236880115554e-05,
      "loss": 1.2435,
      "step": 2622
    },
    {
      "epoch": 0.9447145687016028,
      "grad_norm": 0.3802197575569153,
      "learning_rate": 2.0600625902744343e-05,
      "loss": 1.4046,
      "step": 2623
    },
    {
      "epoch": 0.9450747343778138,
      "grad_norm": 0.37555989623069763,
      "learning_rate": 2.0597014925373135e-05,
      "loss": 1.2804,
      "step": 2624
    },
    {
      "epoch": 0.9454349000540249,
      "grad_norm": 0.3931879699230194,
      "learning_rate": 2.0593403948001924e-05,
      "loss": 1.3255,
      "step": 2625
    },
    {
      "epoch": 0.9457950657302359,
      "grad_norm": 0.3800300359725952,
      "learning_rate": 2.0589792970630716e-05,
      "loss": 1.2763,
      "step": 2626
    },
    {
      "epoch": 0.946155231406447,
      "grad_norm": 0.3928414583206177,
      "learning_rate": 2.0586181993259512e-05,
      "loss": 1.3861,
      "step": 2627
    },
    {
      "epoch": 0.946515397082658,
      "grad_norm": 0.39820772409439087,
      "learning_rate": 2.05825710158883e-05,
      "loss": 1.2384,
      "step": 2628
    },
    {
      "epoch": 0.9468755627588691,
      "grad_norm": 0.38594022393226624,
      "learning_rate": 2.0578960038517093e-05,
      "loss": 1.2552,
      "step": 2629
    },
    {
      "epoch": 0.9472357284350801,
      "grad_norm": 0.3950919806957245,
      "learning_rate": 2.0575349061145885e-05,
      "loss": 1.2901,
      "step": 2630
    },
    {
      "epoch": 0.9475958941112912,
      "grad_norm": 0.4018951952457428,
      "learning_rate": 2.0571738083774674e-05,
      "loss": 1.2502,
      "step": 2631
    },
    {
      "epoch": 0.9479560597875023,
      "grad_norm": 0.3669896423816681,
      "learning_rate": 2.0568127106403467e-05,
      "loss": 1.1805,
      "step": 2632
    },
    {
      "epoch": 0.9483162254637133,
      "grad_norm": 0.3889355957508087,
      "learning_rate": 2.0564516129032256e-05,
      "loss": 1.1747,
      "step": 2633
    },
    {
      "epoch": 0.9486763911399244,
      "grad_norm": 0.4032306969165802,
      "learning_rate": 2.056090515166105e-05,
      "loss": 1.2411,
      "step": 2634
    },
    {
      "epoch": 0.9490365568161354,
      "grad_norm": 0.3811410963535309,
      "learning_rate": 2.0557294174289844e-05,
      "loss": 1.1927,
      "step": 2635
    },
    {
      "epoch": 0.9493967224923465,
      "grad_norm": 0.3859271705150604,
      "learning_rate": 2.0553683196918632e-05,
      "loss": 1.2834,
      "step": 2636
    },
    {
      "epoch": 0.9497568881685575,
      "grad_norm": 0.4014046788215637,
      "learning_rate": 2.0550072219547425e-05,
      "loss": 1.4675,
      "step": 2637
    },
    {
      "epoch": 0.9501170538447686,
      "grad_norm": 0.37677496671676636,
      "learning_rate": 2.0546461242176217e-05,
      "loss": 1.1985,
      "step": 2638
    },
    {
      "epoch": 0.9504772195209796,
      "grad_norm": 0.395209938287735,
      "learning_rate": 2.0542850264805006e-05,
      "loss": 1.2604,
      "step": 2639
    },
    {
      "epoch": 0.9508373851971907,
      "grad_norm": 0.39105114340782166,
      "learning_rate": 2.05392392874338e-05,
      "loss": 1.2918,
      "step": 2640
    },
    {
      "epoch": 0.9511975508734017,
      "grad_norm": 0.38209250569343567,
      "learning_rate": 2.053562831006259e-05,
      "loss": 1.379,
      "step": 2641
    },
    {
      "epoch": 0.9515577165496129,
      "grad_norm": 0.38026759028434753,
      "learning_rate": 2.0532017332691383e-05,
      "loss": 1.2269,
      "step": 2642
    },
    {
      "epoch": 0.9519178822258239,
      "grad_norm": 0.36440399289131165,
      "learning_rate": 2.0528406355320175e-05,
      "loss": 1.2354,
      "step": 2643
    },
    {
      "epoch": 0.952278047902035,
      "grad_norm": 0.3721330761909485,
      "learning_rate": 2.0524795377948964e-05,
      "loss": 1.3166,
      "step": 2644
    },
    {
      "epoch": 0.952638213578246,
      "grad_norm": 0.37391552329063416,
      "learning_rate": 2.0521184400577756e-05,
      "loss": 1.2434,
      "step": 2645
    },
    {
      "epoch": 0.9529983792544571,
      "grad_norm": 0.3637266755104065,
      "learning_rate": 2.051757342320655e-05,
      "loss": 1.1915,
      "step": 2646
    },
    {
      "epoch": 0.9533585449306681,
      "grad_norm": 0.4081508219242096,
      "learning_rate": 2.051396244583534e-05,
      "loss": 1.4128,
      "step": 2647
    },
    {
      "epoch": 0.9537187106068792,
      "grad_norm": 0.3617408275604248,
      "learning_rate": 2.0510351468464133e-05,
      "loss": 1.1325,
      "step": 2648
    },
    {
      "epoch": 0.9540788762830902,
      "grad_norm": 0.35911011695861816,
      "learning_rate": 2.0506740491092922e-05,
      "loss": 1.2891,
      "step": 2649
    },
    {
      "epoch": 0.9544390419593013,
      "grad_norm": 0.38457751274108887,
      "learning_rate": 2.0503129513721714e-05,
      "loss": 1.2522,
      "step": 2650
    },
    {
      "epoch": 0.9547992076355123,
      "grad_norm": 0.3835318386554718,
      "learning_rate": 2.0499518536350507e-05,
      "loss": 1.2971,
      "step": 2651
    },
    {
      "epoch": 0.9551593733117234,
      "grad_norm": 0.3860698640346527,
      "learning_rate": 2.0495907558979296e-05,
      "loss": 1.322,
      "step": 2652
    },
    {
      "epoch": 0.9555195389879344,
      "grad_norm": 0.3821415305137634,
      "learning_rate": 2.0492296581608088e-05,
      "loss": 1.2957,
      "step": 2653
    },
    {
      "epoch": 0.9558797046641455,
      "grad_norm": 0.38019803166389465,
      "learning_rate": 2.048868560423688e-05,
      "loss": 1.306,
      "step": 2654
    },
    {
      "epoch": 0.9562398703403565,
      "grad_norm": 0.38060399889945984,
      "learning_rate": 2.0485074626865673e-05,
      "loss": 1.3397,
      "step": 2655
    },
    {
      "epoch": 0.9566000360165676,
      "grad_norm": 0.40427836775779724,
      "learning_rate": 2.0481463649494465e-05,
      "loss": 1.3198,
      "step": 2656
    },
    {
      "epoch": 0.9569602016927787,
      "grad_norm": 0.3832300305366516,
      "learning_rate": 2.0477852672123254e-05,
      "loss": 1.2458,
      "step": 2657
    },
    {
      "epoch": 0.9573203673689897,
      "grad_norm": 0.36165332794189453,
      "learning_rate": 2.0474241694752046e-05,
      "loss": 1.1947,
      "step": 2658
    },
    {
      "epoch": 0.9576805330452008,
      "grad_norm": 0.3966743052005768,
      "learning_rate": 2.047063071738084e-05,
      "loss": 1.2402,
      "step": 2659
    },
    {
      "epoch": 0.9580406987214118,
      "grad_norm": 0.3957368731498718,
      "learning_rate": 2.0467019740009627e-05,
      "loss": 1.3116,
      "step": 2660
    },
    {
      "epoch": 0.958400864397623,
      "grad_norm": 0.41923704743385315,
      "learning_rate": 2.0463408762638423e-05,
      "loss": 1.2795,
      "step": 2661
    },
    {
      "epoch": 0.958761030073834,
      "grad_norm": 0.3820511996746063,
      "learning_rate": 2.0459797785267212e-05,
      "loss": 1.2545,
      "step": 2662
    },
    {
      "epoch": 0.9591211957500451,
      "grad_norm": 0.3818442225456238,
      "learning_rate": 2.0456186807896004e-05,
      "loss": 1.3052,
      "step": 2663
    },
    {
      "epoch": 0.9594813614262561,
      "grad_norm": 0.3904953896999359,
      "learning_rate": 2.0452575830524796e-05,
      "loss": 1.3595,
      "step": 2664
    },
    {
      "epoch": 0.9598415271024672,
      "grad_norm": 0.3724026381969452,
      "learning_rate": 2.0448964853153585e-05,
      "loss": 1.2897,
      "step": 2665
    },
    {
      "epoch": 0.9602016927786782,
      "grad_norm": 0.38543999195098877,
      "learning_rate": 2.0445353875782378e-05,
      "loss": 1.2908,
      "step": 2666
    },
    {
      "epoch": 0.9605618584548893,
      "grad_norm": 0.4016118049621582,
      "learning_rate": 2.0441742898411173e-05,
      "loss": 1.3865,
      "step": 2667
    },
    {
      "epoch": 0.9609220241311003,
      "grad_norm": 0.3804369866847992,
      "learning_rate": 2.0438131921039962e-05,
      "loss": 1.2883,
      "step": 2668
    },
    {
      "epoch": 0.9612821898073114,
      "grad_norm": 0.3727756440639496,
      "learning_rate": 2.0434520943668755e-05,
      "loss": 1.1955,
      "step": 2669
    },
    {
      "epoch": 0.9616423554835224,
      "grad_norm": 0.41097745299339294,
      "learning_rate": 2.0430909966297543e-05,
      "loss": 1.4399,
      "step": 2670
    },
    {
      "epoch": 0.9620025211597335,
      "grad_norm": 0.3677124083042145,
      "learning_rate": 2.0427298988926336e-05,
      "loss": 1.1677,
      "step": 2671
    },
    {
      "epoch": 0.9623626868359445,
      "grad_norm": 0.38786596059799194,
      "learning_rate": 2.0423688011555128e-05,
      "loss": 1.3364,
      "step": 2672
    },
    {
      "epoch": 0.9627228525121556,
      "grad_norm": 0.3936602771282196,
      "learning_rate": 2.0420077034183917e-05,
      "loss": 1.2274,
      "step": 2673
    },
    {
      "epoch": 0.9630830181883666,
      "grad_norm": 0.3937337398529053,
      "learning_rate": 2.0416466056812713e-05,
      "loss": 1.4378,
      "step": 2674
    },
    {
      "epoch": 0.9634431838645777,
      "grad_norm": 0.38759806752204895,
      "learning_rate": 2.0412855079441505e-05,
      "loss": 1.3434,
      "step": 2675
    },
    {
      "epoch": 0.9638033495407887,
      "grad_norm": 0.390593558549881,
      "learning_rate": 2.0409244102070294e-05,
      "loss": 1.4068,
      "step": 2676
    },
    {
      "epoch": 0.9641635152169998,
      "grad_norm": 0.3839862644672394,
      "learning_rate": 2.0405633124699086e-05,
      "loss": 1.2767,
      "step": 2677
    },
    {
      "epoch": 0.9645236808932108,
      "grad_norm": 0.36903226375579834,
      "learning_rate": 2.0402022147327875e-05,
      "loss": 1.2038,
      "step": 2678
    },
    {
      "epoch": 0.9648838465694219,
      "grad_norm": 0.4024197459220886,
      "learning_rate": 2.0398411169956667e-05,
      "loss": 1.3082,
      "step": 2679
    },
    {
      "epoch": 0.9652440122456329,
      "grad_norm": 0.3998744785785675,
      "learning_rate": 2.039480019258546e-05,
      "loss": 1.2737,
      "step": 2680
    },
    {
      "epoch": 0.965604177921844,
      "grad_norm": 0.3627786338329315,
      "learning_rate": 2.0391189215214252e-05,
      "loss": 1.2262,
      "step": 2681
    },
    {
      "epoch": 0.965964343598055,
      "grad_norm": 0.38511765003204346,
      "learning_rate": 2.0387578237843044e-05,
      "loss": 1.1844,
      "step": 2682
    },
    {
      "epoch": 0.9663245092742662,
      "grad_norm": 0.37928855419158936,
      "learning_rate": 2.0383967260471837e-05,
      "loss": 1.2501,
      "step": 2683
    },
    {
      "epoch": 0.9666846749504773,
      "grad_norm": 0.36726099252700806,
      "learning_rate": 2.0380356283100625e-05,
      "loss": 1.2074,
      "step": 2684
    },
    {
      "epoch": 0.9670448406266883,
      "grad_norm": 0.4179527461528778,
      "learning_rate": 2.0376745305729418e-05,
      "loss": 1.3676,
      "step": 2685
    },
    {
      "epoch": 0.9674050063028994,
      "grad_norm": 0.37730324268341064,
      "learning_rate": 2.0373134328358207e-05,
      "loss": 1.3424,
      "step": 2686
    },
    {
      "epoch": 0.9677651719791104,
      "grad_norm": 0.390508234500885,
      "learning_rate": 2.0369523350987e-05,
      "loss": 1.4159,
      "step": 2687
    },
    {
      "epoch": 0.9681253376553215,
      "grad_norm": 0.3820831775665283,
      "learning_rate": 2.0365912373615795e-05,
      "loss": 1.2527,
      "step": 2688
    },
    {
      "epoch": 0.9684855033315325,
      "grad_norm": 0.3787468373775482,
      "learning_rate": 2.0362301396244584e-05,
      "loss": 1.2483,
      "step": 2689
    },
    {
      "epoch": 0.9688456690077436,
      "grad_norm": 0.3914903998374939,
      "learning_rate": 2.0358690418873376e-05,
      "loss": 1.2135,
      "step": 2690
    },
    {
      "epoch": 0.9692058346839546,
      "grad_norm": 0.36781343817710876,
      "learning_rate": 2.0355079441502168e-05,
      "loss": 1.3954,
      "step": 2691
    },
    {
      "epoch": 0.9695660003601657,
      "grad_norm": 0.37761354446411133,
      "learning_rate": 2.0351468464130957e-05,
      "loss": 1.1767,
      "step": 2692
    },
    {
      "epoch": 0.9699261660363767,
      "grad_norm": 0.3780261278152466,
      "learning_rate": 2.034785748675975e-05,
      "loss": 1.206,
      "step": 2693
    },
    {
      "epoch": 0.9702863317125878,
      "grad_norm": 0.4035324454307556,
      "learning_rate": 2.034424650938854e-05,
      "loss": 1.3496,
      "step": 2694
    },
    {
      "epoch": 0.9706464973887988,
      "grad_norm": 0.38148629665374756,
      "learning_rate": 2.0340635532017334e-05,
      "loss": 1.3264,
      "step": 2695
    },
    {
      "epoch": 0.9710066630650099,
      "grad_norm": 0.38648632168769836,
      "learning_rate": 2.0337024554646126e-05,
      "loss": 1.2737,
      "step": 2696
    },
    {
      "epoch": 0.9713668287412209,
      "grad_norm": 0.3913154602050781,
      "learning_rate": 2.0333413577274915e-05,
      "loss": 1.3408,
      "step": 2697
    },
    {
      "epoch": 0.971726994417432,
      "grad_norm": 0.40304499864578247,
      "learning_rate": 2.0329802599903707e-05,
      "loss": 1.2371,
      "step": 2698
    },
    {
      "epoch": 0.972087160093643,
      "grad_norm": 0.3985019624233246,
      "learning_rate": 2.03261916225325e-05,
      "loss": 1.3764,
      "step": 2699
    },
    {
      "epoch": 0.9724473257698542,
      "grad_norm": 0.372294545173645,
      "learning_rate": 2.032258064516129e-05,
      "loss": 1.2747,
      "step": 2700
    },
    {
      "epoch": 0.9728074914460652,
      "grad_norm": 0.36734074354171753,
      "learning_rate": 2.0318969667790084e-05,
      "loss": 1.2809,
      "step": 2701
    },
    {
      "epoch": 0.9731676571222763,
      "grad_norm": 0.3831854462623596,
      "learning_rate": 2.0315358690418873e-05,
      "loss": 1.2423,
      "step": 2702
    },
    {
      "epoch": 0.9735278227984873,
      "grad_norm": 0.381134033203125,
      "learning_rate": 2.0311747713047666e-05,
      "loss": 1.2793,
      "step": 2703
    },
    {
      "epoch": 0.9738879884746984,
      "grad_norm": 0.38024795055389404,
      "learning_rate": 2.0308136735676458e-05,
      "loss": 1.254,
      "step": 2704
    },
    {
      "epoch": 0.9742481541509094,
      "grad_norm": 0.3896627724170685,
      "learning_rate": 2.0304525758305247e-05,
      "loss": 1.2654,
      "step": 2705
    },
    {
      "epoch": 0.9746083198271205,
      "grad_norm": 0.403138130903244,
      "learning_rate": 2.030091478093404e-05,
      "loss": 1.3896,
      "step": 2706
    },
    {
      "epoch": 0.9749684855033315,
      "grad_norm": 0.4030994474887848,
      "learning_rate": 2.029730380356283e-05,
      "loss": 1.504,
      "step": 2707
    },
    {
      "epoch": 0.9753286511795426,
      "grad_norm": 0.38724952936172485,
      "learning_rate": 2.0293692826191624e-05,
      "loss": 1.26,
      "step": 2708
    },
    {
      "epoch": 0.9756888168557536,
      "grad_norm": 0.38834255933761597,
      "learning_rate": 2.0290081848820416e-05,
      "loss": 1.4157,
      "step": 2709
    },
    {
      "epoch": 0.9760489825319647,
      "grad_norm": 0.3971594274044037,
      "learning_rate": 2.0286470871449205e-05,
      "loss": 1.311,
      "step": 2710
    },
    {
      "epoch": 0.9764091482081758,
      "grad_norm": 0.3837801218032837,
      "learning_rate": 2.0282859894077997e-05,
      "loss": 1.2953,
      "step": 2711
    },
    {
      "epoch": 0.9767693138843868,
      "grad_norm": 0.394432008266449,
      "learning_rate": 2.027924891670679e-05,
      "loss": 1.2339,
      "step": 2712
    },
    {
      "epoch": 0.9771294795605979,
      "grad_norm": 0.3932536840438843,
      "learning_rate": 2.027563793933558e-05,
      "loss": 1.3781,
      "step": 2713
    },
    {
      "epoch": 0.9774896452368089,
      "grad_norm": 0.3627764880657196,
      "learning_rate": 2.027202696196437e-05,
      "loss": 1.2175,
      "step": 2714
    },
    {
      "epoch": 0.97784981091302,
      "grad_norm": 0.38588249683380127,
      "learning_rate": 2.0268415984593166e-05,
      "loss": 1.249,
      "step": 2715
    },
    {
      "epoch": 0.978209976589231,
      "grad_norm": 0.3778935372829437,
      "learning_rate": 2.0264805007221955e-05,
      "loss": 1.2572,
      "step": 2716
    },
    {
      "epoch": 0.9785701422654421,
      "grad_norm": 0.43989044427871704,
      "learning_rate": 2.0261194029850748e-05,
      "loss": 1.3085,
      "step": 2717
    },
    {
      "epoch": 0.9789303079416531,
      "grad_norm": 0.3703484833240509,
      "learning_rate": 2.0257583052479536e-05,
      "loss": 1.3123,
      "step": 2718
    },
    {
      "epoch": 0.9792904736178643,
      "grad_norm": 0.39026010036468506,
      "learning_rate": 2.025397207510833e-05,
      "loss": 1.3138,
      "step": 2719
    },
    {
      "epoch": 0.9796506392940753,
      "grad_norm": 0.3935999572277069,
      "learning_rate": 2.025036109773712e-05,
      "loss": 1.3174,
      "step": 2720
    },
    {
      "epoch": 0.9800108049702864,
      "grad_norm": 0.40814489126205444,
      "learning_rate": 2.0246750120365913e-05,
      "loss": 1.1369,
      "step": 2721
    },
    {
      "epoch": 0.9803709706464974,
      "grad_norm": 0.3910667896270752,
      "learning_rate": 2.0243139142994706e-05,
      "loss": 1.2862,
      "step": 2722
    },
    {
      "epoch": 0.9807311363227085,
      "grad_norm": 0.37329450249671936,
      "learning_rate": 2.0239528165623498e-05,
      "loss": 1.2899,
      "step": 2723
    },
    {
      "epoch": 0.9810913019989195,
      "grad_norm": 0.3876937925815582,
      "learning_rate": 2.0235917188252287e-05,
      "loss": 1.5286,
      "step": 2724
    },
    {
      "epoch": 0.9814514676751306,
      "grad_norm": 0.3900670111179352,
      "learning_rate": 2.023230621088108e-05,
      "loss": 1.3072,
      "step": 2725
    },
    {
      "epoch": 0.9818116333513416,
      "grad_norm": 0.38223522901535034,
      "learning_rate": 2.0228695233509868e-05,
      "loss": 1.2185,
      "step": 2726
    },
    {
      "epoch": 0.9821717990275527,
      "grad_norm": 0.39047491550445557,
      "learning_rate": 2.022508425613866e-05,
      "loss": 1.4163,
      "step": 2727
    },
    {
      "epoch": 0.9825319647037637,
      "grad_norm": 0.39574310183525085,
      "learning_rate": 2.0221473278767456e-05,
      "loss": 1.3832,
      "step": 2728
    },
    {
      "epoch": 0.9828921303799748,
      "grad_norm": 0.4003901183605194,
      "learning_rate": 2.0217862301396245e-05,
      "loss": 1.2754,
      "step": 2729
    },
    {
      "epoch": 0.9832522960561858,
      "grad_norm": 0.3679186999797821,
      "learning_rate": 2.0214251324025037e-05,
      "loss": 1.1891,
      "step": 2730
    },
    {
      "epoch": 0.9836124617323969,
      "grad_norm": 0.3864094018936157,
      "learning_rate": 2.021064034665383e-05,
      "loss": 1.2806,
      "step": 2731
    },
    {
      "epoch": 0.9839726274086079,
      "grad_norm": 0.3741125166416168,
      "learning_rate": 2.020702936928262e-05,
      "loss": 1.2386,
      "step": 2732
    },
    {
      "epoch": 0.984332793084819,
      "grad_norm": 0.38201606273651123,
      "learning_rate": 2.020341839191141e-05,
      "loss": 1.379,
      "step": 2733
    },
    {
      "epoch": 0.98469295876103,
      "grad_norm": 0.38399842381477356,
      "learning_rate": 2.01998074145402e-05,
      "loss": 1.2689,
      "step": 2734
    },
    {
      "epoch": 0.9850531244372411,
      "grad_norm": 0.3799585700035095,
      "learning_rate": 2.0196196437168995e-05,
      "loss": 1.3466,
      "step": 2735
    },
    {
      "epoch": 0.9854132901134521,
      "grad_norm": 0.3824881911277771,
      "learning_rate": 2.0192585459797788e-05,
      "loss": 1.3118,
      "step": 2736
    },
    {
      "epoch": 0.9857734557896632,
      "grad_norm": 0.39733996987342834,
      "learning_rate": 2.0188974482426577e-05,
      "loss": 1.2033,
      "step": 2737
    },
    {
      "epoch": 0.9861336214658744,
      "grad_norm": 0.402209997177124,
      "learning_rate": 2.018536350505537e-05,
      "loss": 1.2394,
      "step": 2738
    },
    {
      "epoch": 0.9864937871420854,
      "grad_norm": 0.3699072301387787,
      "learning_rate": 2.018175252768416e-05,
      "loss": 1.2841,
      "step": 2739
    },
    {
      "epoch": 0.9868539528182965,
      "grad_norm": 0.37226665019989014,
      "learning_rate": 2.017814155031295e-05,
      "loss": 1.223,
      "step": 2740
    },
    {
      "epoch": 0.9872141184945075,
      "grad_norm": 0.38122016191482544,
      "learning_rate": 2.0174530572941742e-05,
      "loss": 1.3733,
      "step": 2741
    },
    {
      "epoch": 0.9875742841707186,
      "grad_norm": 0.3926073908805847,
      "learning_rate": 2.0170919595570535e-05,
      "loss": 1.2838,
      "step": 2742
    },
    {
      "epoch": 0.9879344498469296,
      "grad_norm": 0.39415091276168823,
      "learning_rate": 2.0167308618199327e-05,
      "loss": 1.3154,
      "step": 2743
    },
    {
      "epoch": 0.9882946155231407,
      "grad_norm": 0.397296667098999,
      "learning_rate": 2.016369764082812e-05,
      "loss": 1.2396,
      "step": 2744
    },
    {
      "epoch": 0.9886547811993517,
      "grad_norm": 0.4131995439529419,
      "learning_rate": 2.0160086663456908e-05,
      "loss": 1.3841,
      "step": 2745
    },
    {
      "epoch": 0.9890149468755628,
      "grad_norm": 0.3941696286201477,
      "learning_rate": 2.01564756860857e-05,
      "loss": 1.2082,
      "step": 2746
    },
    {
      "epoch": 0.9893751125517738,
      "grad_norm": 0.3846384584903717,
      "learning_rate": 2.0152864708714493e-05,
      "loss": 1.2432,
      "step": 2747
    },
    {
      "epoch": 0.9897352782279849,
      "grad_norm": 0.38913699984550476,
      "learning_rate": 2.0149253731343285e-05,
      "loss": 1.1892,
      "step": 2748
    },
    {
      "epoch": 0.9900954439041959,
      "grad_norm": 0.4025668501853943,
      "learning_rate": 2.0145642753972077e-05,
      "loss": 1.2739,
      "step": 2749
    },
    {
      "epoch": 0.990455609580407,
      "grad_norm": 0.379577100276947,
      "learning_rate": 2.0142031776600866e-05,
      "loss": 1.2623,
      "step": 2750
    },
    {
      "epoch": 0.990815775256618,
      "grad_norm": 0.39076316356658936,
      "learning_rate": 2.013842079922966e-05,
      "loss": 1.2213,
      "step": 2751
    },
    {
      "epoch": 0.9911759409328291,
      "grad_norm": 0.3630039095878601,
      "learning_rate": 2.013480982185845e-05,
      "loss": 1.208,
      "step": 2752
    },
    {
      "epoch": 0.9915361066090401,
      "grad_norm": 0.3858738839626312,
      "learning_rate": 2.013119884448724e-05,
      "loss": 1.2929,
      "step": 2753
    },
    {
      "epoch": 0.9918962722852512,
      "grad_norm": 0.3869766294956207,
      "learning_rate": 2.0127587867116032e-05,
      "loss": 1.2742,
      "step": 2754
    },
    {
      "epoch": 0.9922564379614622,
      "grad_norm": 0.4197361171245575,
      "learning_rate": 2.0123976889744828e-05,
      "loss": 1.2841,
      "step": 2755
    },
    {
      "epoch": 0.9926166036376733,
      "grad_norm": 0.3911530673503876,
      "learning_rate": 2.0120365912373617e-05,
      "loss": 1.2727,
      "step": 2756
    },
    {
      "epoch": 0.9929767693138843,
      "grad_norm": 0.38404619693756104,
      "learning_rate": 2.011675493500241e-05,
      "loss": 1.3013,
      "step": 2757
    },
    {
      "epoch": 0.9933369349900955,
      "grad_norm": 0.3841279149055481,
      "learning_rate": 2.0113143957631198e-05,
      "loss": 1.2914,
      "step": 2758
    },
    {
      "epoch": 0.9936971006663065,
      "grad_norm": 0.38470882177352905,
      "learning_rate": 2.010953298025999e-05,
      "loss": 1.1521,
      "step": 2759
    },
    {
      "epoch": 0.9940572663425176,
      "grad_norm": 0.38525843620300293,
      "learning_rate": 2.0105922002888782e-05,
      "loss": 1.2149,
      "step": 2760
    },
    {
      "epoch": 0.9944174320187286,
      "grad_norm": 0.3768479526042938,
      "learning_rate": 2.010231102551757e-05,
      "loss": 1.36,
      "step": 2761
    },
    {
      "epoch": 0.9947775976949397,
      "grad_norm": 0.3849254250526428,
      "learning_rate": 2.0098700048146367e-05,
      "loss": 1.143,
      "step": 2762
    },
    {
      "epoch": 0.9951377633711507,
      "grad_norm": 0.4143063426017761,
      "learning_rate": 2.009508907077516e-05,
      "loss": 1.2495,
      "step": 2763
    },
    {
      "epoch": 0.9954979290473618,
      "grad_norm": 0.3836750388145447,
      "learning_rate": 2.0091478093403948e-05,
      "loss": 1.3133,
      "step": 2764
    },
    {
      "epoch": 0.9958580947235729,
      "grad_norm": 0.4135315716266632,
      "learning_rate": 2.008786711603274e-05,
      "loss": 1.4562,
      "step": 2765
    },
    {
      "epoch": 0.9962182603997839,
      "grad_norm": 0.4023009240627289,
      "learning_rate": 2.008425613866153e-05,
      "loss": 1.4608,
      "step": 2766
    },
    {
      "epoch": 0.996578426075995,
      "grad_norm": 0.4016973376274109,
      "learning_rate": 2.0080645161290322e-05,
      "loss": 1.4691,
      "step": 2767
    },
    {
      "epoch": 0.996938591752206,
      "grad_norm": 0.39237523078918457,
      "learning_rate": 2.0077034183919117e-05,
      "loss": 1.2243,
      "step": 2768
    },
    {
      "epoch": 0.9972987574284171,
      "grad_norm": 0.38550132513046265,
      "learning_rate": 2.0073423206547906e-05,
      "loss": 1.2275,
      "step": 2769
    },
    {
      "epoch": 0.9976589231046281,
      "grad_norm": 0.3832562565803528,
      "learning_rate": 2.00698122291767e-05,
      "loss": 1.3751,
      "step": 2770
    },
    {
      "epoch": 0.9980190887808392,
      "grad_norm": 0.3636656701564789,
      "learning_rate": 2.006620125180549e-05,
      "loss": 1.327,
      "step": 2771
    },
    {
      "epoch": 0.9983792544570502,
      "grad_norm": 0.39302730560302734,
      "learning_rate": 2.006259027443428e-05,
      "loss": 1.286,
      "step": 2772
    },
    {
      "epoch": 0.9987394201332613,
      "grad_norm": 0.4048524498939514,
      "learning_rate": 2.0058979297063072e-05,
      "loss": 1.3715,
      "step": 2773
    },
    {
      "epoch": 0.9990995858094723,
      "grad_norm": 0.37734055519104004,
      "learning_rate": 2.005536831969186e-05,
      "loss": 1.3634,
      "step": 2774
    },
    {
      "epoch": 0.9994597514856834,
      "grad_norm": 0.3764287829399109,
      "learning_rate": 2.0051757342320657e-05,
      "loss": 1.3517,
      "step": 2775
    },
    {
      "epoch": 0.9998199171618944,
      "grad_norm": 0.4155363440513611,
      "learning_rate": 2.004814636494945e-05,
      "loss": 1.3002,
      "step": 2776
    },
    {
      "epoch": 1.0001800828381056,
      "grad_norm": 0.6838367581367493,
      "learning_rate": 2.0044535387578238e-05,
      "loss": 1.8728,
      "step": 2777
    },
    {
      "epoch": 1.0005402485143167,
      "grad_norm": 0.40272256731987,
      "learning_rate": 2.004092441020703e-05,
      "loss": 1.3238,
      "step": 2778
    },
    {
      "epoch": 1.0009004141905276,
      "grad_norm": 0.36594608426094055,
      "learning_rate": 2.0037313432835822e-05,
      "loss": 1.3645,
      "step": 2779
    },
    {
      "epoch": 1.0012605798667387,
      "grad_norm": 0.3827003538608551,
      "learning_rate": 2.003370245546461e-05,
      "loss": 1.2469,
      "step": 2780
    },
    {
      "epoch": 1.0016207455429498,
      "grad_norm": 0.37922003865242004,
      "learning_rate": 2.0030091478093404e-05,
      "loss": 1.2017,
      "step": 2781
    },
    {
      "epoch": 1.0019809112191609,
      "grad_norm": 0.3792226314544678,
      "learning_rate": 2.0026480500722196e-05,
      "loss": 1.2911,
      "step": 2782
    },
    {
      "epoch": 1.0023410768953718,
      "grad_norm": 0.3837646245956421,
      "learning_rate": 2.0022869523350988e-05,
      "loss": 1.2683,
      "step": 2783
    },
    {
      "epoch": 1.0027012425715829,
      "grad_norm": 0.41065990924835205,
      "learning_rate": 2.001925854597978e-05,
      "loss": 1.2428,
      "step": 2784
    },
    {
      "epoch": 1.003061408247794,
      "grad_norm": 0.37961578369140625,
      "learning_rate": 2.001564756860857e-05,
      "loss": 1.2807,
      "step": 2785
    },
    {
      "epoch": 1.003421573924005,
      "grad_norm": 0.38643312454223633,
      "learning_rate": 2.0012036591237362e-05,
      "loss": 1.2021,
      "step": 2786
    },
    {
      "epoch": 1.003781739600216,
      "grad_norm": 0.3778946101665497,
      "learning_rate": 2.0008425613866154e-05,
      "loss": 1.1333,
      "step": 2787
    },
    {
      "epoch": 1.004141905276427,
      "grad_norm": 0.38895559310913086,
      "learning_rate": 2.0004814636494943e-05,
      "loss": 1.3313,
      "step": 2788
    },
    {
      "epoch": 1.0045020709526382,
      "grad_norm": 0.37442776560783386,
      "learning_rate": 2.000120365912374e-05,
      "loss": 1.1841,
      "step": 2789
    },
    {
      "epoch": 1.0048622366288493,
      "grad_norm": 0.40877822041511536,
      "learning_rate": 1.9997592681752528e-05,
      "loss": 1.2634,
      "step": 2790
    },
    {
      "epoch": 1.0052224023050604,
      "grad_norm": 0.3833976984024048,
      "learning_rate": 1.999398170438132e-05,
      "loss": 1.2313,
      "step": 2791
    },
    {
      "epoch": 1.0055825679812713,
      "grad_norm": 0.3932660222053528,
      "learning_rate": 1.9990370727010112e-05,
      "loss": 1.2547,
      "step": 2792
    },
    {
      "epoch": 1.0059427336574824,
      "grad_norm": 0.3817754089832306,
      "learning_rate": 1.99867597496389e-05,
      "loss": 1.2875,
      "step": 2793
    },
    {
      "epoch": 1.0063028993336935,
      "grad_norm": 0.39240995049476624,
      "learning_rate": 1.9983148772267693e-05,
      "loss": 1.2606,
      "step": 2794
    },
    {
      "epoch": 1.0066630650099047,
      "grad_norm": 0.395894318819046,
      "learning_rate": 1.997953779489649e-05,
      "loss": 1.3757,
      "step": 2795
    },
    {
      "epoch": 1.0070232306861155,
      "grad_norm": 0.38380923867225647,
      "learning_rate": 1.9975926817525278e-05,
      "loss": 1.3782,
      "step": 2796
    },
    {
      "epoch": 1.0073833963623267,
      "grad_norm": 0.39262887835502625,
      "learning_rate": 1.997231584015407e-05,
      "loss": 1.1949,
      "step": 2797
    },
    {
      "epoch": 1.0077435620385378,
      "grad_norm": 0.3778687119483948,
      "learning_rate": 1.996870486278286e-05,
      "loss": 1.3552,
      "step": 2798
    },
    {
      "epoch": 1.0081037277147489,
      "grad_norm": 0.39468270540237427,
      "learning_rate": 1.996509388541165e-05,
      "loss": 1.4603,
      "step": 2799
    },
    {
      "epoch": 1.0084638933909598,
      "grad_norm": 0.3666561245918274,
      "learning_rate": 1.9961482908040444e-05,
      "loss": 1.3054,
      "step": 2800
    },
    {
      "epoch": 1.0088240590671709,
      "grad_norm": 0.39465680718421936,
      "learning_rate": 1.9957871930669233e-05,
      "loss": 1.3527,
      "step": 2801
    },
    {
      "epoch": 1.009184224743382,
      "grad_norm": 0.3839889168739319,
      "learning_rate": 1.995426095329803e-05,
      "loss": 1.1709,
      "step": 2802
    },
    {
      "epoch": 1.009544390419593,
      "grad_norm": 0.3746482729911804,
      "learning_rate": 1.995064997592682e-05,
      "loss": 1.2685,
      "step": 2803
    },
    {
      "epoch": 1.009904556095804,
      "grad_norm": 0.3978496789932251,
      "learning_rate": 1.994703899855561e-05,
      "loss": 1.3877,
      "step": 2804
    },
    {
      "epoch": 1.010264721772015,
      "grad_norm": 0.3930933475494385,
      "learning_rate": 1.9943428021184402e-05,
      "loss": 1.2983,
      "step": 2805
    },
    {
      "epoch": 1.0106248874482262,
      "grad_norm": 0.3875958323478699,
      "learning_rate": 1.993981704381319e-05,
      "loss": 1.1573,
      "step": 2806
    },
    {
      "epoch": 1.0109850531244373,
      "grad_norm": 0.3781072199344635,
      "learning_rate": 1.9936206066441983e-05,
      "loss": 1.2446,
      "step": 2807
    },
    {
      "epoch": 1.0113452188006482,
      "grad_norm": 0.38569024205207825,
      "learning_rate": 1.9932595089070775e-05,
      "loss": 1.2323,
      "step": 2808
    },
    {
      "epoch": 1.0117053844768593,
      "grad_norm": 0.3651498258113861,
      "learning_rate": 1.9928984111699568e-05,
      "loss": 1.2106,
      "step": 2809
    },
    {
      "epoch": 1.0120655501530704,
      "grad_norm": 0.38406580686569214,
      "learning_rate": 1.992537313432836e-05,
      "loss": 1.3251,
      "step": 2810
    },
    {
      "epoch": 1.0124257158292815,
      "grad_norm": 0.39175838232040405,
      "learning_rate": 1.9921762156957152e-05,
      "loss": 1.2498,
      "step": 2811
    },
    {
      "epoch": 1.0127858815054924,
      "grad_norm": 0.40370824933052063,
      "learning_rate": 1.991815117958594e-05,
      "loss": 1.1823,
      "step": 2812
    },
    {
      "epoch": 1.0131460471817035,
      "grad_norm": 0.4032423496246338,
      "learning_rate": 1.9914540202214733e-05,
      "loss": 1.2965,
      "step": 2813
    },
    {
      "epoch": 1.0135062128579146,
      "grad_norm": 0.3891929090023041,
      "learning_rate": 1.9910929224843522e-05,
      "loss": 1.2663,
      "step": 2814
    },
    {
      "epoch": 1.0138663785341258,
      "grad_norm": 0.3843700587749481,
      "learning_rate": 1.9907318247472315e-05,
      "loss": 1.3634,
      "step": 2815
    },
    {
      "epoch": 1.0142265442103366,
      "grad_norm": 0.4133453667163849,
      "learning_rate": 1.990370727010111e-05,
      "loss": 1.3335,
      "step": 2816
    },
    {
      "epoch": 1.0145867098865478,
      "grad_norm": 0.39811214804649353,
      "learning_rate": 1.99000962927299e-05,
      "loss": 1.2794,
      "step": 2817
    },
    {
      "epoch": 1.0149468755627589,
      "grad_norm": 0.3961954712867737,
      "learning_rate": 1.989648531535869e-05,
      "loss": 1.3152,
      "step": 2818
    },
    {
      "epoch": 1.01530704123897,
      "grad_norm": 0.3870745897293091,
      "learning_rate": 1.9892874337987484e-05,
      "loss": 1.2538,
      "step": 2819
    },
    {
      "epoch": 1.015667206915181,
      "grad_norm": 0.394775927066803,
      "learning_rate": 1.9889263360616273e-05,
      "loss": 1.3037,
      "step": 2820
    },
    {
      "epoch": 1.016027372591392,
      "grad_norm": 0.39488792419433594,
      "learning_rate": 1.9885652383245065e-05,
      "loss": 1.3652,
      "step": 2821
    },
    {
      "epoch": 1.016387538267603,
      "grad_norm": 0.39092928171157837,
      "learning_rate": 1.9882041405873857e-05,
      "loss": 1.2701,
      "step": 2822
    },
    {
      "epoch": 1.0167477039438142,
      "grad_norm": 0.3905947506427765,
      "learning_rate": 1.987843042850265e-05,
      "loss": 1.2461,
      "step": 2823
    },
    {
      "epoch": 1.0171078696200253,
      "grad_norm": 0.4029141068458557,
      "learning_rate": 1.9874819451131442e-05,
      "loss": 1.2232,
      "step": 2824
    },
    {
      "epoch": 1.0174680352962362,
      "grad_norm": 0.4061114490032196,
      "learning_rate": 1.987120847376023e-05,
      "loss": 1.224,
      "step": 2825
    },
    {
      "epoch": 1.0178282009724473,
      "grad_norm": 0.3967576324939728,
      "learning_rate": 1.9867597496389023e-05,
      "loss": 1.2806,
      "step": 2826
    },
    {
      "epoch": 1.0181883666486584,
      "grad_norm": 0.41565442085266113,
      "learning_rate": 1.9863986519017815e-05,
      "loss": 1.426,
      "step": 2827
    },
    {
      "epoch": 1.0185485323248695,
      "grad_norm": 0.39921480417251587,
      "learning_rate": 1.9860375541646604e-05,
      "loss": 1.3934,
      "step": 2828
    },
    {
      "epoch": 1.0189086980010804,
      "grad_norm": 0.3824213445186615,
      "learning_rate": 1.98567645642754e-05,
      "loss": 1.272,
      "step": 2829
    },
    {
      "epoch": 1.0192688636772915,
      "grad_norm": 0.3978298008441925,
      "learning_rate": 1.985315358690419e-05,
      "loss": 1.2836,
      "step": 2830
    },
    {
      "epoch": 1.0196290293535026,
      "grad_norm": 0.39671090245246887,
      "learning_rate": 1.984954260953298e-05,
      "loss": 1.3698,
      "step": 2831
    },
    {
      "epoch": 1.0199891950297137,
      "grad_norm": 0.39482423663139343,
      "learning_rate": 1.9845931632161774e-05,
      "loss": 1.2615,
      "step": 2832
    },
    {
      "epoch": 1.0203493607059246,
      "grad_norm": 0.3912050127983093,
      "learning_rate": 1.9842320654790562e-05,
      "loss": 1.2664,
      "step": 2833
    },
    {
      "epoch": 1.0207095263821357,
      "grad_norm": 0.4015830159187317,
      "learning_rate": 1.9838709677419355e-05,
      "loss": 1.2488,
      "step": 2834
    },
    {
      "epoch": 1.0210696920583469,
      "grad_norm": 0.38927125930786133,
      "learning_rate": 1.9835098700048147e-05,
      "loss": 1.226,
      "step": 2835
    },
    {
      "epoch": 1.021429857734558,
      "grad_norm": 0.37608760595321655,
      "learning_rate": 1.983148772267694e-05,
      "loss": 1.2409,
      "step": 2836
    },
    {
      "epoch": 1.0217900234107689,
      "grad_norm": 0.38409194350242615,
      "learning_rate": 1.982787674530573e-05,
      "loss": 1.3398,
      "step": 2837
    },
    {
      "epoch": 1.02215018908698,
      "grad_norm": 0.3938625156879425,
      "learning_rate": 1.982426576793452e-05,
      "loss": 1.3148,
      "step": 2838
    },
    {
      "epoch": 1.022510354763191,
      "grad_norm": 0.4010345935821533,
      "learning_rate": 1.9820654790563313e-05,
      "loss": 1.276,
      "step": 2839
    },
    {
      "epoch": 1.0228705204394022,
      "grad_norm": 0.40128231048583984,
      "learning_rate": 1.9817043813192105e-05,
      "loss": 1.2823,
      "step": 2840
    },
    {
      "epoch": 1.023230686115613,
      "grad_norm": 0.39755409955978394,
      "learning_rate": 1.9813432835820894e-05,
      "loss": 1.2166,
      "step": 2841
    },
    {
      "epoch": 1.0235908517918242,
      "grad_norm": 0.3854319453239441,
      "learning_rate": 1.9809821858449686e-05,
      "loss": 1.2303,
      "step": 2842
    },
    {
      "epoch": 1.0239510174680353,
      "grad_norm": 0.38805222511291504,
      "learning_rate": 1.980621088107848e-05,
      "loss": 1.2323,
      "step": 2843
    },
    {
      "epoch": 1.0243111831442464,
      "grad_norm": 0.4071084260940552,
      "learning_rate": 1.980259990370727e-05,
      "loss": 1.2858,
      "step": 2844
    },
    {
      "epoch": 1.0246713488204575,
      "grad_norm": 0.37942686676979065,
      "learning_rate": 1.9798988926336063e-05,
      "loss": 1.249,
      "step": 2845
    },
    {
      "epoch": 1.0250315144966684,
      "grad_norm": 0.40675005316734314,
      "learning_rate": 1.9795377948964852e-05,
      "loss": 1.2975,
      "step": 2846
    },
    {
      "epoch": 1.0253916801728795,
      "grad_norm": 0.394178181886673,
      "learning_rate": 1.9791766971593644e-05,
      "loss": 1.2747,
      "step": 2847
    },
    {
      "epoch": 1.0257518458490906,
      "grad_norm": 0.38808223605155945,
      "learning_rate": 1.9788155994222437e-05,
      "loss": 1.2012,
      "step": 2848
    },
    {
      "epoch": 1.0261120115253017,
      "grad_norm": 0.4182855486869812,
      "learning_rate": 1.978454501685123e-05,
      "loss": 1.3319,
      "step": 2849
    },
    {
      "epoch": 1.0264721772015126,
      "grad_norm": 0.3997219502925873,
      "learning_rate": 1.978093403948002e-05,
      "loss": 1.396,
      "step": 2850
    },
    {
      "epoch": 1.0268323428777237,
      "grad_norm": 0.3884633481502533,
      "learning_rate": 1.977732306210881e-05,
      "loss": 1.2918,
      "step": 2851
    },
    {
      "epoch": 1.0271925085539348,
      "grad_norm": 0.3785545229911804,
      "learning_rate": 1.9773712084737603e-05,
      "loss": 1.1321,
      "step": 2852
    },
    {
      "epoch": 1.027552674230146,
      "grad_norm": 0.3754700720310211,
      "learning_rate": 1.9770101107366395e-05,
      "loss": 1.3681,
      "step": 2853
    },
    {
      "epoch": 1.0279128399063568,
      "grad_norm": 0.39409586787223816,
      "learning_rate": 1.9766490129995184e-05,
      "loss": 1.2809,
      "step": 2854
    },
    {
      "epoch": 1.028273005582568,
      "grad_norm": 0.4010557532310486,
      "learning_rate": 1.9762879152623976e-05,
      "loss": 1.3339,
      "step": 2855
    },
    {
      "epoch": 1.028633171258779,
      "grad_norm": 0.3809567987918854,
      "learning_rate": 1.9759268175252772e-05,
      "loss": 1.1684,
      "step": 2856
    },
    {
      "epoch": 1.0289933369349902,
      "grad_norm": 0.3754987120628357,
      "learning_rate": 1.975565719788156e-05,
      "loss": 1.2561,
      "step": 2857
    },
    {
      "epoch": 1.029353502611201,
      "grad_norm": 0.3795848786830902,
      "learning_rate": 1.9752046220510353e-05,
      "loss": 1.1204,
      "step": 2858
    },
    {
      "epoch": 1.0297136682874122,
      "grad_norm": 0.39605388045310974,
      "learning_rate": 1.9748435243139142e-05,
      "loss": 1.1597,
      "step": 2859
    },
    {
      "epoch": 1.0300738339636233,
      "grad_norm": 0.4082220792770386,
      "learning_rate": 1.9744824265767934e-05,
      "loss": 1.2902,
      "step": 2860
    },
    {
      "epoch": 1.0304339996398344,
      "grad_norm": 0.3983895778656006,
      "learning_rate": 1.9741213288396726e-05,
      "loss": 1.1432,
      "step": 2861
    },
    {
      "epoch": 1.0307941653160453,
      "grad_norm": 0.386945515871048,
      "learning_rate": 1.9737602311025515e-05,
      "loss": 1.1842,
      "step": 2862
    },
    {
      "epoch": 1.0311543309922564,
      "grad_norm": 0.39078018069267273,
      "learning_rate": 1.973399133365431e-05,
      "loss": 1.3561,
      "step": 2863
    },
    {
      "epoch": 1.0315144966684675,
      "grad_norm": 0.38725709915161133,
      "learning_rate": 1.9730380356283103e-05,
      "loss": 1.2077,
      "step": 2864
    },
    {
      "epoch": 1.0318746623446786,
      "grad_norm": 0.3882463574409485,
      "learning_rate": 1.9726769378911892e-05,
      "loss": 1.2244,
      "step": 2865
    },
    {
      "epoch": 1.0322348280208895,
      "grad_norm": 0.4092943072319031,
      "learning_rate": 1.9723158401540685e-05,
      "loss": 1.4106,
      "step": 2866
    },
    {
      "epoch": 1.0325949936971006,
      "grad_norm": 0.3772593140602112,
      "learning_rate": 1.9719547424169473e-05,
      "loss": 1.316,
      "step": 2867
    },
    {
      "epoch": 1.0329551593733117,
      "grad_norm": 0.3952041268348694,
      "learning_rate": 1.9715936446798266e-05,
      "loss": 1.3545,
      "step": 2868
    },
    {
      "epoch": 1.0333153250495228,
      "grad_norm": 0.39421528577804565,
      "learning_rate": 1.9712325469427058e-05,
      "loss": 1.3003,
      "step": 2869
    },
    {
      "epoch": 1.033675490725734,
      "grad_norm": 0.39363783597946167,
      "learning_rate": 1.970871449205585e-05,
      "loss": 1.2357,
      "step": 2870
    },
    {
      "epoch": 1.0340356564019448,
      "grad_norm": 0.3979281485080719,
      "learning_rate": 1.9705103514684643e-05,
      "loss": 1.188,
      "step": 2871
    },
    {
      "epoch": 1.034395822078156,
      "grad_norm": 0.382390558719635,
      "learning_rate": 1.9701492537313435e-05,
      "loss": 1.2981,
      "step": 2872
    },
    {
      "epoch": 1.034755987754367,
      "grad_norm": 0.3847047984600067,
      "learning_rate": 1.9697881559942224e-05,
      "loss": 1.3067,
      "step": 2873
    },
    {
      "epoch": 1.0351161534305782,
      "grad_norm": 0.3867216110229492,
      "learning_rate": 1.9694270582571016e-05,
      "loss": 1.234,
      "step": 2874
    },
    {
      "epoch": 1.035476319106789,
      "grad_norm": 0.38184624910354614,
      "learning_rate": 1.9690659605199805e-05,
      "loss": 1.2847,
      "step": 2875
    },
    {
      "epoch": 1.0358364847830002,
      "grad_norm": 0.400992751121521,
      "learning_rate": 1.96870486278286e-05,
      "loss": 1.2244,
      "step": 2876
    },
    {
      "epoch": 1.0361966504592113,
      "grad_norm": 0.3896300196647644,
      "learning_rate": 1.9683437650457393e-05,
      "loss": 1.2225,
      "step": 2877
    },
    {
      "epoch": 1.0365568161354224,
      "grad_norm": 0.40690553188323975,
      "learning_rate": 1.9679826673086182e-05,
      "loss": 1.2641,
      "step": 2878
    },
    {
      "epoch": 1.0369169818116333,
      "grad_norm": 0.38643404841423035,
      "learning_rate": 1.9676215695714974e-05,
      "loss": 1.2208,
      "step": 2879
    },
    {
      "epoch": 1.0372771474878444,
      "grad_norm": 0.40450555086135864,
      "learning_rate": 1.9672604718343766e-05,
      "loss": 1.3393,
      "step": 2880
    },
    {
      "epoch": 1.0376373131640555,
      "grad_norm": 0.4148807227611542,
      "learning_rate": 1.9668993740972555e-05,
      "loss": 1.2806,
      "step": 2881
    },
    {
      "epoch": 1.0379974788402666,
      "grad_norm": 0.3990171253681183,
      "learning_rate": 1.9665382763601348e-05,
      "loss": 1.203,
      "step": 2882
    },
    {
      "epoch": 1.0383576445164775,
      "grad_norm": 0.4029892385005951,
      "learning_rate": 1.966177178623014e-05,
      "loss": 1.3742,
      "step": 2883
    },
    {
      "epoch": 1.0387178101926886,
      "grad_norm": 0.40650519728660583,
      "learning_rate": 1.9658160808858932e-05,
      "loss": 1.4227,
      "step": 2884
    },
    {
      "epoch": 1.0390779758688997,
      "grad_norm": 0.40352174639701843,
      "learning_rate": 1.9654549831487725e-05,
      "loss": 1.2148,
      "step": 2885
    },
    {
      "epoch": 1.0394381415451108,
      "grad_norm": 0.3956746757030487,
      "learning_rate": 1.9650938854116514e-05,
      "loss": 1.1989,
      "step": 2886
    },
    {
      "epoch": 1.0397983072213217,
      "grad_norm": 0.3688105046749115,
      "learning_rate": 1.9647327876745306e-05,
      "loss": 1.2207,
      "step": 2887
    },
    {
      "epoch": 1.0401584728975328,
      "grad_norm": 0.39422234892845154,
      "learning_rate": 1.9643716899374098e-05,
      "loss": 1.47,
      "step": 2888
    },
    {
      "epoch": 1.040518638573744,
      "grad_norm": 0.3850403130054474,
      "learning_rate": 1.9640105922002887e-05,
      "loss": 1.2853,
      "step": 2889
    },
    {
      "epoch": 1.040878804249955,
      "grad_norm": 0.374928742647171,
      "learning_rate": 1.9636494944631683e-05,
      "loss": 1.216,
      "step": 2890
    },
    {
      "epoch": 1.041238969926166,
      "grad_norm": 0.40202003717422485,
      "learning_rate": 1.963288396726047e-05,
      "loss": 1.3225,
      "step": 2891
    },
    {
      "epoch": 1.041599135602377,
      "grad_norm": 0.3869979679584503,
      "learning_rate": 1.9629272989889264e-05,
      "loss": 1.2188,
      "step": 2892
    },
    {
      "epoch": 1.0419593012785882,
      "grad_norm": 0.3890027105808258,
      "learning_rate": 1.9625662012518056e-05,
      "loss": 1.2892,
      "step": 2893
    },
    {
      "epoch": 1.0423194669547993,
      "grad_norm": 0.4059169292449951,
      "learning_rate": 1.9622051035146845e-05,
      "loss": 1.2045,
      "step": 2894
    },
    {
      "epoch": 1.0426796326310104,
      "grad_norm": 0.3847602903842926,
      "learning_rate": 1.9618440057775637e-05,
      "loss": 1.2294,
      "step": 2895
    },
    {
      "epoch": 1.0430397983072213,
      "grad_norm": 0.3759072721004486,
      "learning_rate": 1.961482908040443e-05,
      "loss": 1.1587,
      "step": 2896
    },
    {
      "epoch": 1.0433999639834324,
      "grad_norm": 0.3995635509490967,
      "learning_rate": 1.9611218103033222e-05,
      "loss": 1.2881,
      "step": 2897
    },
    {
      "epoch": 1.0437601296596435,
      "grad_norm": 0.40365538001060486,
      "learning_rate": 1.9607607125662014e-05,
      "loss": 1.3324,
      "step": 2898
    },
    {
      "epoch": 1.0441202953358546,
      "grad_norm": 0.3974197208881378,
      "learning_rate": 1.9603996148290803e-05,
      "loss": 1.1735,
      "step": 2899
    },
    {
      "epoch": 1.0444804610120655,
      "grad_norm": 0.4111781120300293,
      "learning_rate": 1.9600385170919595e-05,
      "loss": 1.289,
      "step": 2900
    },
    {
      "epoch": 1.0448406266882766,
      "grad_norm": 0.4053533673286438,
      "learning_rate": 1.9596774193548388e-05,
      "loss": 1.2989,
      "step": 2901
    },
    {
      "epoch": 1.0452007923644877,
      "grad_norm": 0.3953162729740143,
      "learning_rate": 1.9593163216177177e-05,
      "loss": 1.3263,
      "step": 2902
    },
    {
      "epoch": 1.0455609580406988,
      "grad_norm": 0.36893177032470703,
      "learning_rate": 1.9589552238805972e-05,
      "loss": 1.0551,
      "step": 2903
    },
    {
      "epoch": 1.0459211237169097,
      "grad_norm": 0.386390745639801,
      "learning_rate": 1.9585941261434765e-05,
      "loss": 1.2666,
      "step": 2904
    },
    {
      "epoch": 1.0462812893931208,
      "grad_norm": 0.38651639223098755,
      "learning_rate": 1.9582330284063554e-05,
      "loss": 1.408,
      "step": 2905
    },
    {
      "epoch": 1.046641455069332,
      "grad_norm": 0.3735201954841614,
      "learning_rate": 1.9578719306692346e-05,
      "loss": 1.2765,
      "step": 2906
    },
    {
      "epoch": 1.047001620745543,
      "grad_norm": 0.41222670674324036,
      "learning_rate": 1.9575108329321135e-05,
      "loss": 1.3509,
      "step": 2907
    },
    {
      "epoch": 1.047361786421754,
      "grad_norm": 0.37837132811546326,
      "learning_rate": 1.9571497351949927e-05,
      "loss": 1.2645,
      "step": 2908
    },
    {
      "epoch": 1.047721952097965,
      "grad_norm": 0.38756540417671204,
      "learning_rate": 1.956788637457872e-05,
      "loss": 1.2259,
      "step": 2909
    },
    {
      "epoch": 1.0480821177741761,
      "grad_norm": 0.3863261938095093,
      "learning_rate": 1.956427539720751e-05,
      "loss": 1.1959,
      "step": 2910
    },
    {
      "epoch": 1.0484422834503873,
      "grad_norm": 0.4086131453514099,
      "learning_rate": 1.9560664419836304e-05,
      "loss": 1.3358,
      "step": 2911
    },
    {
      "epoch": 1.0488024491265981,
      "grad_norm": 0.3904857635498047,
      "learning_rate": 1.9557053442465096e-05,
      "loss": 1.2551,
      "step": 2912
    },
    {
      "epoch": 1.0491626148028093,
      "grad_norm": 0.3985527753829956,
      "learning_rate": 1.9553442465093885e-05,
      "loss": 1.2309,
      "step": 2913
    },
    {
      "epoch": 1.0495227804790204,
      "grad_norm": 0.38637369871139526,
      "learning_rate": 1.9549831487722677e-05,
      "loss": 1.1969,
      "step": 2914
    },
    {
      "epoch": 1.0498829461552315,
      "grad_norm": 0.38492825627326965,
      "learning_rate": 1.9546220510351466e-05,
      "loss": 1.2828,
      "step": 2915
    },
    {
      "epoch": 1.0502431118314424,
      "grad_norm": 0.3808346390724182,
      "learning_rate": 1.954260953298026e-05,
      "loss": 1.2837,
      "step": 2916
    },
    {
      "epoch": 1.0506032775076535,
      "grad_norm": 0.41086044907569885,
      "learning_rate": 1.9538998555609054e-05,
      "loss": 1.3084,
      "step": 2917
    },
    {
      "epoch": 1.0509634431838646,
      "grad_norm": 0.39788907766342163,
      "learning_rate": 1.9535387578237843e-05,
      "loss": 1.2118,
      "step": 2918
    },
    {
      "epoch": 1.0513236088600757,
      "grad_norm": 0.39209437370300293,
      "learning_rate": 1.9531776600866636e-05,
      "loss": 1.2244,
      "step": 2919
    },
    {
      "epoch": 1.0516837745362866,
      "grad_norm": 0.3858189880847931,
      "learning_rate": 1.9528165623495428e-05,
      "loss": 1.2196,
      "step": 2920
    },
    {
      "epoch": 1.0520439402124977,
      "grad_norm": 0.38775506615638733,
      "learning_rate": 1.9524554646124217e-05,
      "loss": 1.2106,
      "step": 2921
    },
    {
      "epoch": 1.0524041058887088,
      "grad_norm": 0.3709622919559479,
      "learning_rate": 1.952094366875301e-05,
      "loss": 1.2283,
      "step": 2922
    },
    {
      "epoch": 1.05276427156492,
      "grad_norm": 0.4032459259033203,
      "learning_rate": 1.9517332691381798e-05,
      "loss": 1.2475,
      "step": 2923
    },
    {
      "epoch": 1.0531244372411308,
      "grad_norm": 0.39044883847236633,
      "learning_rate": 1.9513721714010594e-05,
      "loss": 1.2764,
      "step": 2924
    },
    {
      "epoch": 1.053484602917342,
      "grad_norm": 0.3913171887397766,
      "learning_rate": 1.9510110736639386e-05,
      "loss": 1.3298,
      "step": 2925
    },
    {
      "epoch": 1.053844768593553,
      "grad_norm": 0.39828306436538696,
      "learning_rate": 1.9506499759268175e-05,
      "loss": 1.2253,
      "step": 2926
    },
    {
      "epoch": 1.0542049342697641,
      "grad_norm": 0.385102242231369,
      "learning_rate": 1.9502888781896967e-05,
      "loss": 1.237,
      "step": 2927
    },
    {
      "epoch": 1.0545650999459752,
      "grad_norm": 0.4149267077445984,
      "learning_rate": 1.949927780452576e-05,
      "loss": 1.3899,
      "step": 2928
    },
    {
      "epoch": 1.0549252656221861,
      "grad_norm": 0.3928584158420563,
      "learning_rate": 1.949566682715455e-05,
      "loss": 1.3013,
      "step": 2929
    },
    {
      "epoch": 1.0552854312983972,
      "grad_norm": 0.4050414264202118,
      "learning_rate": 1.9492055849783344e-05,
      "loss": 1.202,
      "step": 2930
    },
    {
      "epoch": 1.0556455969746084,
      "grad_norm": 0.43005913496017456,
      "learning_rate": 1.9488444872412133e-05,
      "loss": 1.4085,
      "step": 2931
    },
    {
      "epoch": 1.0560057626508195,
      "grad_norm": 0.39411067962646484,
      "learning_rate": 1.9484833895040925e-05,
      "loss": 1.1785,
      "step": 2932
    },
    {
      "epoch": 1.0563659283270304,
      "grad_norm": 0.3832551836967468,
      "learning_rate": 1.9481222917669718e-05,
      "loss": 1.2379,
      "step": 2933
    },
    {
      "epoch": 1.0567260940032415,
      "grad_norm": 0.38899344205856323,
      "learning_rate": 1.9477611940298506e-05,
      "loss": 1.3023,
      "step": 2934
    },
    {
      "epoch": 1.0570862596794526,
      "grad_norm": 0.416562557220459,
      "learning_rate": 1.94740009629273e-05,
      "loss": 1.3846,
      "step": 2935
    },
    {
      "epoch": 1.0574464253556637,
      "grad_norm": 0.40725722908973694,
      "learning_rate": 1.947038998555609e-05,
      "loss": 1.3031,
      "step": 2936
    },
    {
      "epoch": 1.0578065910318746,
      "grad_norm": 0.4005177319049835,
      "learning_rate": 1.9466779008184883e-05,
      "loss": 1.3458,
      "step": 2937
    },
    {
      "epoch": 1.0581667567080857,
      "grad_norm": 0.39573395252227783,
      "learning_rate": 1.9463168030813676e-05,
      "loss": 1.2266,
      "step": 2938
    },
    {
      "epoch": 1.0585269223842968,
      "grad_norm": 0.3921428918838501,
      "learning_rate": 1.9459557053442465e-05,
      "loss": 1.2967,
      "step": 2939
    },
    {
      "epoch": 1.058887088060508,
      "grad_norm": 0.3878862261772156,
      "learning_rate": 1.9455946076071257e-05,
      "loss": 1.2439,
      "step": 2940
    },
    {
      "epoch": 1.0592472537367188,
      "grad_norm": 0.4171253740787506,
      "learning_rate": 1.945233509870005e-05,
      "loss": 1.3587,
      "step": 2941
    },
    {
      "epoch": 1.05960741941293,
      "grad_norm": 0.39712056517601013,
      "learning_rate": 1.9448724121328838e-05,
      "loss": 1.2687,
      "step": 2942
    },
    {
      "epoch": 1.059967585089141,
      "grad_norm": 0.3959915041923523,
      "learning_rate": 1.944511314395763e-05,
      "loss": 1.3797,
      "step": 2943
    },
    {
      "epoch": 1.0603277507653521,
      "grad_norm": 0.4122113883495331,
      "learning_rate": 1.9441502166586426e-05,
      "loss": 1.2797,
      "step": 2944
    },
    {
      "epoch": 1.060687916441563,
      "grad_norm": 0.40771374106407166,
      "learning_rate": 1.9437891189215215e-05,
      "loss": 1.3241,
      "step": 2945
    },
    {
      "epoch": 1.0610480821177741,
      "grad_norm": 0.404962420463562,
      "learning_rate": 1.9434280211844007e-05,
      "loss": 1.3151,
      "step": 2946
    },
    {
      "epoch": 1.0614082477939852,
      "grad_norm": 0.4007575511932373,
      "learning_rate": 1.9430669234472796e-05,
      "loss": 1.3314,
      "step": 2947
    },
    {
      "epoch": 1.0617684134701963,
      "grad_norm": 0.3854708671569824,
      "learning_rate": 1.942705825710159e-05,
      "loss": 1.1973,
      "step": 2948
    },
    {
      "epoch": 1.0621285791464072,
      "grad_norm": 0.4191112220287323,
      "learning_rate": 1.942344727973038e-05,
      "loss": 1.2717,
      "step": 2949
    },
    {
      "epoch": 1.0624887448226183,
      "grad_norm": 0.3942343592643738,
      "learning_rate": 1.941983630235917e-05,
      "loss": 1.128,
      "step": 2950
    },
    {
      "epoch": 1.0628489104988295,
      "grad_norm": 0.3939410150051117,
      "learning_rate": 1.9416225324987965e-05,
      "loss": 1.3838,
      "step": 2951
    },
    {
      "epoch": 1.0632090761750406,
      "grad_norm": 0.4120117127895355,
      "learning_rate": 1.9412614347616758e-05,
      "loss": 1.3084,
      "step": 2952
    },
    {
      "epoch": 1.0635692418512517,
      "grad_norm": 0.38915303349494934,
      "learning_rate": 1.9409003370245547e-05,
      "loss": 1.2569,
      "step": 2953
    },
    {
      "epoch": 1.0639294075274626,
      "grad_norm": 0.4203300178050995,
      "learning_rate": 1.940539239287434e-05,
      "loss": 1.3261,
      "step": 2954
    },
    {
      "epoch": 1.0642895732036737,
      "grad_norm": 0.40078938007354736,
      "learning_rate": 1.9401781415503128e-05,
      "loss": 1.2627,
      "step": 2955
    },
    {
      "epoch": 1.0646497388798848,
      "grad_norm": 0.38199102878570557,
      "learning_rate": 1.939817043813192e-05,
      "loss": 1.3408,
      "step": 2956
    },
    {
      "epoch": 1.065009904556096,
      "grad_norm": 0.3914855420589447,
      "learning_rate": 1.9394559460760716e-05,
      "loss": 1.2459,
      "step": 2957
    },
    {
      "epoch": 1.0653700702323068,
      "grad_norm": 0.41055694222450256,
      "learning_rate": 1.9390948483389505e-05,
      "loss": 1.2338,
      "step": 2958
    },
    {
      "epoch": 1.065730235908518,
      "grad_norm": 0.3971982002258301,
      "learning_rate": 1.9387337506018297e-05,
      "loss": 1.2865,
      "step": 2959
    },
    {
      "epoch": 1.066090401584729,
      "grad_norm": 0.40443310141563416,
      "learning_rate": 1.938372652864709e-05,
      "loss": 1.0966,
      "step": 2960
    },
    {
      "epoch": 1.0664505672609401,
      "grad_norm": 0.4096064865589142,
      "learning_rate": 1.9380115551275878e-05,
      "loss": 1.254,
      "step": 2961
    },
    {
      "epoch": 1.066810732937151,
      "grad_norm": 0.39670121669769287,
      "learning_rate": 1.937650457390467e-05,
      "loss": 1.3378,
      "step": 2962
    },
    {
      "epoch": 1.0671708986133621,
      "grad_norm": 0.3983646333217621,
      "learning_rate": 1.937289359653346e-05,
      "loss": 1.234,
      "step": 2963
    },
    {
      "epoch": 1.0675310642895732,
      "grad_norm": 0.3911215662956238,
      "learning_rate": 1.9369282619162255e-05,
      "loss": 1.2867,
      "step": 2964
    },
    {
      "epoch": 1.0678912299657843,
      "grad_norm": 0.3892754018306732,
      "learning_rate": 1.9365671641791047e-05,
      "loss": 1.2832,
      "step": 2965
    },
    {
      "epoch": 1.0682513956419952,
      "grad_norm": 0.407723605632782,
      "learning_rate": 1.9362060664419836e-05,
      "loss": 1.3763,
      "step": 2966
    },
    {
      "epoch": 1.0686115613182063,
      "grad_norm": 0.40166592597961426,
      "learning_rate": 1.935844968704863e-05,
      "loss": 1.2621,
      "step": 2967
    },
    {
      "epoch": 1.0689717269944174,
      "grad_norm": 0.40363809466362,
      "learning_rate": 1.935483870967742e-05,
      "loss": 1.3645,
      "step": 2968
    },
    {
      "epoch": 1.0693318926706286,
      "grad_norm": 0.38581013679504395,
      "learning_rate": 1.935122773230621e-05,
      "loss": 1.2005,
      "step": 2969
    },
    {
      "epoch": 1.0696920583468394,
      "grad_norm": 0.40922290086746216,
      "learning_rate": 1.9347616754935002e-05,
      "loss": 1.3726,
      "step": 2970
    },
    {
      "epoch": 1.0700522240230506,
      "grad_norm": 0.4098672568798065,
      "learning_rate": 1.9344005777563794e-05,
      "loss": 1.3112,
      "step": 2971
    },
    {
      "epoch": 1.0704123896992617,
      "grad_norm": 0.3917589485645294,
      "learning_rate": 1.9340394800192587e-05,
      "loss": 1.395,
      "step": 2972
    },
    {
      "epoch": 1.0707725553754728,
      "grad_norm": 0.3930133283138275,
      "learning_rate": 1.933678382282138e-05,
      "loss": 1.2456,
      "step": 2973
    },
    {
      "epoch": 1.0711327210516837,
      "grad_norm": 0.4159395694732666,
      "learning_rate": 1.9333172845450168e-05,
      "loss": 1.2962,
      "step": 2974
    },
    {
      "epoch": 1.0714928867278948,
      "grad_norm": 0.3876253664493561,
      "learning_rate": 1.932956186807896e-05,
      "loss": 1.2345,
      "step": 2975
    },
    {
      "epoch": 1.0718530524041059,
      "grad_norm": 0.40769848227500916,
      "learning_rate": 1.9325950890707752e-05,
      "loss": 1.3436,
      "step": 2976
    },
    {
      "epoch": 1.072213218080317,
      "grad_norm": 0.4134751260280609,
      "learning_rate": 1.932233991333654e-05,
      "loss": 1.3209,
      "step": 2977
    },
    {
      "epoch": 1.072573383756528,
      "grad_norm": 0.41483256220817566,
      "learning_rate": 1.9318728935965337e-05,
      "loss": 1.2437,
      "step": 2978
    },
    {
      "epoch": 1.072933549432739,
      "grad_norm": 0.40925222635269165,
      "learning_rate": 1.9315117958594126e-05,
      "loss": 1.166,
      "step": 2979
    },
    {
      "epoch": 1.07329371510895,
      "grad_norm": 0.40840816497802734,
      "learning_rate": 1.9311506981222918e-05,
      "loss": 1.2126,
      "step": 2980
    },
    {
      "epoch": 1.0736538807851612,
      "grad_norm": 0.3915880620479584,
      "learning_rate": 1.930789600385171e-05,
      "loss": 1.2397,
      "step": 2981
    },
    {
      "epoch": 1.0740140464613723,
      "grad_norm": 0.41025301814079285,
      "learning_rate": 1.93042850264805e-05,
      "loss": 1.2236,
      "step": 2982
    },
    {
      "epoch": 1.0743742121375832,
      "grad_norm": 0.3939979076385498,
      "learning_rate": 1.9300674049109292e-05,
      "loss": 1.2255,
      "step": 2983
    },
    {
      "epoch": 1.0747343778137943,
      "grad_norm": 0.4169248044490814,
      "learning_rate": 1.9297063071738087e-05,
      "loss": 1.3231,
      "step": 2984
    },
    {
      "epoch": 1.0750945434900054,
      "grad_norm": 0.402485728263855,
      "learning_rate": 1.9293452094366876e-05,
      "loss": 1.3542,
      "step": 2985
    },
    {
      "epoch": 1.0754547091662165,
      "grad_norm": 0.393370121717453,
      "learning_rate": 1.928984111699567e-05,
      "loss": 1.3377,
      "step": 2986
    },
    {
      "epoch": 1.0758148748424274,
      "grad_norm": 0.40467387437820435,
      "learning_rate": 1.9286230139624458e-05,
      "loss": 1.3447,
      "step": 2987
    },
    {
      "epoch": 1.0761750405186385,
      "grad_norm": 0.3871511220932007,
      "learning_rate": 1.928261916225325e-05,
      "loss": 1.1766,
      "step": 2988
    },
    {
      "epoch": 1.0765352061948497,
      "grad_norm": 0.3929021954536438,
      "learning_rate": 1.9279008184882042e-05,
      "loss": 1.2479,
      "step": 2989
    },
    {
      "epoch": 1.0768953718710608,
      "grad_norm": 0.3807896077632904,
      "learning_rate": 1.927539720751083e-05,
      "loss": 1.2118,
      "step": 2990
    },
    {
      "epoch": 1.0772555375472717,
      "grad_norm": 0.4223559498786926,
      "learning_rate": 1.9271786230139627e-05,
      "loss": 1.5286,
      "step": 2991
    },
    {
      "epoch": 1.0776157032234828,
      "grad_norm": 0.41226258873939514,
      "learning_rate": 1.926817525276842e-05,
      "loss": 1.2106,
      "step": 2992
    },
    {
      "epoch": 1.0779758688996939,
      "grad_norm": 0.4086678922176361,
      "learning_rate": 1.9264564275397208e-05,
      "loss": 1.3218,
      "step": 2993
    },
    {
      "epoch": 1.078336034575905,
      "grad_norm": 0.42567673325538635,
      "learning_rate": 1.9260953298026e-05,
      "loss": 1.3989,
      "step": 2994
    },
    {
      "epoch": 1.0786962002521159,
      "grad_norm": 0.4134601950645447,
      "learning_rate": 1.925734232065479e-05,
      "loss": 1.196,
      "step": 2995
    },
    {
      "epoch": 1.079056365928327,
      "grad_norm": 0.40922537446022034,
      "learning_rate": 1.925373134328358e-05,
      "loss": 1.219,
      "step": 2996
    },
    {
      "epoch": 1.079416531604538,
      "grad_norm": 0.4058077037334442,
      "learning_rate": 1.9250120365912374e-05,
      "loss": 1.3165,
      "step": 2997
    },
    {
      "epoch": 1.0797766972807492,
      "grad_norm": 0.40399283170700073,
      "learning_rate": 1.9246509388541166e-05,
      "loss": 1.3476,
      "step": 2998
    },
    {
      "epoch": 1.08013686295696,
      "grad_norm": 0.4022843837738037,
      "learning_rate": 1.9242898411169958e-05,
      "loss": 1.2369,
      "step": 2999
    },
    {
      "epoch": 1.0804970286331712,
      "grad_norm": 0.38950973749160767,
      "learning_rate": 1.923928743379875e-05,
      "loss": 1.322,
      "step": 3000
    }
  ],
  "logging_steps": 1,
  "max_steps": 8328,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.37606228237484e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
